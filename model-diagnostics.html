<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>APS 240: Data Analysis and Statistics with R</title>
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
  <meta name="description" content="Course book for Data Analysis and Statistics with R (APS 240) in the Department of Animal and Plant Sciences, University of Sheffield">
  <meta name="generator" content="bookdown 0.1.5 and GitBook 2.6.7">

  <meta property="og:title" content="APS 240: Data Analysis and Statistics with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Course book for Data Analysis and Statistics with R (APS 240) in the Department of Animal and Plant Sciences, University of Sheffield" />
  <meta name="github-repo" content="dzchilds/stats-for-bio" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="APS 240: Data Analysis and Statistics with R" />
  
  <meta name="twitter:description" content="Course book for Data Analysis and Statistics with R (APS 240) in the Department of Animal and Plant Sciences, University of Sheffield" />
  

<meta name="author" content="Dylan Z. Childs">

<meta name="date" content="2016-10-27">

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="regression-diagnostics.html">
<link rel="next" href="correlation.html">

<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<style type="text/css">

.warning-box {
  border: 3px solid #e60000;
  margin:  25px;
  padding: 10px 10px 5px 10px;
  border-radius: 6px 6px 6px 6px;
}

.advanced-box {
  border: 3px solid #268bd2;
  margin:  25px;
  padding: 10px 10px 5px 10px;
  border-radius: 6px 6px 6px 6px;
}

.do-something {
  border: 3px solid #803e00;
  margin:  25px;
  padding: 10px 10px 5px 10px;
  border-radius: 6px 6px 6px 6px;
}

</style>


<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>


  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">APS 240</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Course information and overview</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#why-data-analysis"><i class="fa fa-check"></i><b>1.1</b> Why do a data analysis course?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.2</b> Course overview</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#aims"><i class="fa fa-check"></i><b>1.2.1</b> Aims</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#objectives"><i class="fa fa-check"></i><b>1.2.2</b> Objectives</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#assumed-background"><i class="fa fa-check"></i><b>1.2.3</b> Assumed background</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#methods"><i class="fa fa-check"></i><b>1.2.4</b> Methods</a></li>
<li class="chapter" data-level="1.2.5" data-path="index.html"><a href="index.html#non-assessed-material"><i class="fa fa-check"></i><b>1.2.5</b> Non-assessed material</a></li>
<li class="chapter" data-level="1.2.6" data-path="index.html"><a href="index.html#what-is-required-of-you"><i class="fa fa-check"></i><b>1.2.6</b> What is required of you?</a></li>
<li class="chapter" data-level="1.2.7" data-path="index.html"><a href="index.html#assessment"><i class="fa fa-check"></i><b>1.2.7</b> Assessment</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#how-to-use-the-teaching-material"><i class="fa fa-check"></i><b>1.3</b> How to use the teaching material</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#printed-material"><i class="fa fa-check"></i><b>1.3.1</b> The online course book</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#printed-material"><i class="fa fa-check"></i><b>1.3.2</b> Printed material</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#how-to-make-best-use-of-it"><i class="fa fa-check"></i><b>1.3.3</b> How to make best use of the teaching material</a></li>
<li class="chapter" data-level="1.3.4" data-path="index.html"><a href="index.html#conventions"><i class="fa fa-check"></i><b>1.3.4</b> Conventions used in the course material</a></li>
<li class="chapter" data-level="1.3.5" data-path="index.html"><a href="index.html#feedback"><i class="fa fa-check"></i><b>1.3.5</b> Feedback</a></li>
<li class="chapter" data-level="1.3.6" data-path="index.html"><a href="index.html#help"><i class="fa fa-check"></i><b>1.3.6</b> Help sessions</a></li>
<li class="chapter" data-level="1.3.7" data-path="index.html"><a href="index.html#overall"><i class="fa fa-check"></i><b>1.3.7</b> Overall…</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#health-and-safety"><i class="fa fa-check"></i><b>1.4</b> Health and safety using display screen equipment</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="expected-learning-outcomes.html"><a href="expected-learning-outcomes.html"><i class="fa fa-check"></i><b>2</b> Expected learning outcomes</a><ul>
<li class="chapter" data-level="2.1" data-path="expected-learning-outcomes.html"><a href="expected-learning-outcomes.html#statistical-concepts-i"><i class="fa fa-check"></i><b>2.1</b> Statistical Concepts (I)</a></li>
<li class="chapter" data-level="2.2" data-path="expected-learning-outcomes.html"><a href="expected-learning-outcomes.html#statistical-concepts-ii"><i class="fa fa-check"></i><b>2.2</b> Statistical Concepts (II)</a></li>
<li class="chapter" data-level="2.3" data-path="expected-learning-outcomes.html"><a href="expected-learning-outcomes.html#simple-parametric-statistics-t-tests"><i class="fa fa-check"></i><b>2.3</b> Simple parametric statistics (<em>t</em>-tests)</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="programming-prerequisites.html"><a href="programming-prerequisites.html"><i class="fa fa-check"></i><b>3</b> Programming prerequisites</a><ul>
<li class="chapter" data-level="3.1" data-path="programming-prerequisites.html"><a href="programming-prerequisites.html#starting-an-r-session-in-rstudio"><i class="fa fa-check"></i><b>3.1</b> Starting an R session in RStudio</a></li>
<li class="chapter" data-level="3.2" data-path="programming-prerequisites.html"><a href="programming-prerequisites.html#using-packages"><i class="fa fa-check"></i><b>3.2</b> Using packages</a></li>
<li class="chapter" data-level="3.3" data-path="programming-prerequisites.html"><a href="programming-prerequisites.html#reading-data-into-r"><i class="fa fa-check"></i><b>3.3</b> Reading data into R</a></li>
<li class="chapter" data-level="3.4" data-path="programming-prerequisites.html"><a href="programming-prerequisites.html#data-frames"><i class="fa fa-check"></i><b>3.4</b> Data frames</a></li>
<li class="chapter" data-level="3.5" data-path="programming-prerequisites.html"><a href="programming-prerequisites.html#package-functions"><i class="fa fa-check"></i><b>3.5</b> Package functions</a></li>
</ul></li>
<li class="part"><span><b>Statistical Concepts (I)</b></span></li>
<li class="chapter" data-level="4" data-path="the-scientific-process.html"><a href="the-scientific-process.html"><i class="fa fa-check"></i><b>4</b> The scientific process</a><ul>
<li class="chapter" data-level="4.1" data-path="the-scientific-process.html"><a href="the-scientific-process.html#stages-scientific-process"><i class="fa fa-check"></i><b>4.1</b> Stages in the scientific process</a><ul>
<li class="chapter" data-level="4.1.1" data-path="the-scientific-process.html"><a href="the-scientific-process.html#stages-observations"><i class="fa fa-check"></i><b>4.1.1</b> Observations</a></li>
<li class="chapter" data-level="4.1.2" data-path="the-scientific-process.html"><a href="the-scientific-process.html#stages-questions"><i class="fa fa-check"></i><b>4.1.2</b> Questions</a></li>
<li class="chapter" data-level="4.1.3" data-path="the-scientific-process.html"><a href="the-scientific-process.html#stages-hypotheses"><i class="fa fa-check"></i><b>4.1.3</b> Hypotheses</a></li>
<li class="chapter" data-level="4.1.4" data-path="the-scientific-process.html"><a href="the-scientific-process.html#stages-predictions"><i class="fa fa-check"></i><b>4.1.4</b> Predictions</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="the-scientific-process.html"><a href="the-scientific-process.html#hypothesis-testing"><i class="fa fa-check"></i><b>4.2</b> Hypothesis testing</a></li>
<li class="chapter" data-level="4.3" data-path="the-scientific-process.html"><a href="the-scientific-process.html#are-we-sure"><i class="fa fa-check"></i><b>4.3</b> Don’t we ever know anything for sure?</a></li>
<li class="chapter" data-level="4.4" data-path="the-scientific-process.html"><a href="the-scientific-process.html#further-reading"><i class="fa fa-check"></i><b>4.4</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-and-variables.html"><a href="data-and-variables.html"><i class="fa fa-check"></i><b>5</b> Data and variables</a><ul>
<li class="chapter" data-level="5.1" data-path="data-and-variables.html"><a href="data-and-variables.html#observations-on-material-and-obvious-things"><i class="fa fa-check"></i><b>5.1</b> “Observations on material and obvious things”</a></li>
<li class="chapter" data-level="5.2" data-path="data-and-variables.html"><a href="data-and-variables.html#var-types"><i class="fa fa-check"></i><b>5.2</b> Revision: Types of variable</a><ul>
<li class="chapter" data-level="5.2.1" data-path="data-and-variables.html"><a href="data-and-variables.html#nominal-categorical-variables"><i class="fa fa-check"></i><b>5.2.1</b> Nominal (categorical) variables</a></li>
<li class="chapter" data-level="5.2.2" data-path="data-and-variables.html"><a href="data-and-variables.html#ordinal-categorical-data"><i class="fa fa-check"></i><b>5.2.2</b> Ordinal (categorical) data</a></li>
<li class="chapter" data-level="5.2.3" data-path="data-and-variables.html"><a href="data-and-variables.html#interval-scale-numeric-variables"><i class="fa fa-check"></i><b>5.2.3</b> Interval scale (numeric) variables</a></li>
<li class="chapter" data-level="5.2.4" data-path="data-and-variables.html"><a href="data-and-variables.html#ratio-scale-numeric-variables"><i class="fa fa-check"></i><b>5.2.4</b> Ratio scale (numeric) variables</a></li>
<li class="chapter" data-level="5.2.5" data-path="data-and-variables.html"><a href="data-and-variables.html#which-is-best"><i class="fa fa-check"></i><b>5.2.5</b> Which is best?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="data-and-variables.html"><a href="data-and-variables.html#accuracy-precision"><i class="fa fa-check"></i><b>5.3</b> Accuracy and precision</a><ul>
<li class="chapter" data-level="5.3.1" data-path="data-and-variables.html"><a href="data-and-variables.html#what-do-they-mean"><i class="fa fa-check"></i><b>5.3.1</b> What do they mean?</a></li>
<li class="chapter" data-level="5.3.2" data-path="data-and-variables.html"><a href="data-and-variables.html#implied-precision---significant-figures"><i class="fa fa-check"></i><b>5.3.2</b> Implied precision - significant figures</a></li>
<li class="chapter" data-level="5.3.3" data-path="data-and-variables.html"><a href="data-and-variables.html#how-precise-should-measurements-be"><i class="fa fa-check"></i><b>5.3.3</b> How precise should measurements be?</a></li>
<li class="chapter" data-level="5.3.4" data-path="data-and-variables.html"><a href="data-and-variables.html#error-bias-and-prejudice"><i class="fa fa-check"></i><b>5.3.4</b> Error, bias and prejudice</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="learning-from-data.html"><a href="learning-from-data.html"><i class="fa fa-check"></i><b>6</b> Learning from data</a><ul>
<li class="chapter" data-level="6.1" data-path="learning-from-data.html"><a href="learning-from-data.html#populations"><i class="fa fa-check"></i><b>6.1</b> Populations</a></li>
<li class="chapter" data-level="6.2" data-path="learning-from-data.html"><a href="learning-from-data.html#learning-about-populations"><i class="fa fa-check"></i><b>6.2</b> Learning about populations</a></li>
<li class="chapter" data-level="6.3" data-path="learning-from-data.html"><a href="learning-from-data.html#morph-example"><i class="fa fa-check"></i><b>6.3</b> A simple example</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="sampling-error.html"><a href="sampling-error.html"><i class="fa fa-check"></i><b>7</b> Sampling error</a><ul>
<li class="chapter" data-level="7.1" data-path="sampling-error.html"><a href="sampling-error.html#sampling-error-1"><i class="fa fa-check"></i><b>7.1</b> Sampling error</a></li>
<li class="chapter" data-level="7.2" data-path="sampling-error.html"><a href="sampling-error.html#sampling-distributions"><i class="fa fa-check"></i><b>7.2</b> Sampling distributions</a></li>
<li class="chapter" data-level="7.3" data-path="sampling-error.html"><a href="sampling-error.html#the-effect-of-sample-size"><i class="fa fa-check"></i><b>7.3</b> The effect of sample size</a></li>
<li class="chapter" data-level="7.4" data-path="sampling-error.html"><a href="sampling-error.html#the-standard-error"><i class="fa fa-check"></i><b>7.4</b> The standard error</a></li>
<li class="chapter" data-level="7.5" data-path="sampling-error.html"><a href="sampling-error.html#what-is-the-point-of-all-this"><i class="fa fa-check"></i><b>7.5</b> What is the point of all this!?</a></li>
</ul></li>
<li class="part"><span><b>Statistical Concepts (II)</b></span></li>
<li class="chapter" data-level="8" data-path="statistical-significance-and-p-values.html"><a href="statistical-significance-and-p-values.html"><i class="fa fa-check"></i><b>8</b> Statistical significance and <em>p</em>-values</a><ul>
<li class="chapter" data-level="8.1" data-path="statistical-significance-and-p-values.html"><a href="statistical-significance-and-p-values.html#bootstrap"><i class="fa fa-check"></i><b>8.1</b> Estimating a sampling distribution</a><ul>
<li class="chapter" data-level="8.1.1" data-path="statistical-significance-and-p-values.html"><a href="statistical-significance-and-p-values.html#bootstrap-overview"><i class="fa fa-check"></i><b>8.1.1</b> Overview of bootstrapping</a></li>
<li class="chapter" data-level="8.1.2" data-path="statistical-significance-and-p-values.html"><a href="statistical-significance-and-p-values.html#doing-it-for-real"><i class="fa fa-check"></i><b>8.1.2</b> Doing it for real</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="statistical-significance-and-p-values.html"><a href="statistical-significance-and-p-values.html#statistical-significance"><i class="fa fa-check"></i><b>8.2</b> Statistical significance</a></li>
<li class="chapter" data-level="8.3" data-path="statistical-significance-and-p-values.html"><a href="statistical-significance-and-p-values.html#concluding-remarks"><i class="fa fa-check"></i><b>8.3</b> Concluding remarks</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="comparing-populations.html"><a href="comparing-populations.html"><i class="fa fa-check"></i><b>9</b> Comparing populations</a><ul>
<li class="chapter" data-level="9.1" data-path="comparing-populations.html"><a href="comparing-populations.html#making-comparisons"><i class="fa fa-check"></i><b>9.1</b> Making comparisons</a></li>
<li class="chapter" data-level="9.2" data-path="comparing-populations.html"><a href="comparing-populations.html#morph-weights-eg"><i class="fa fa-check"></i><b>9.2</b> A new example</a></li>
<li class="chapter" data-level="9.3" data-path="comparing-populations.html"><a href="comparing-populations.html#evaluating-differences-between-population-means"><i class="fa fa-check"></i><b>9.3</b> Evaluating differences between population means</a></li>
<li class="chapter" data-level="9.4" data-path="comparing-populations.html"><a href="comparing-populations.html#a-permutation-test"><i class="fa fa-check"></i><b>9.4</b> A permutation test</a></li>
<li class="chapter" data-level="9.5" data-path="comparing-populations.html"><a href="comparing-populations.html#what-have-we-learned"><i class="fa fa-check"></i><b>9.5</b> What have we learned?</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html"><i class="fa fa-check"></i><b>10</b> Hypotheses and <em>p</em>-values</a><ul>
<li class="chapter" data-level="10.1" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html#a-few-words-about-the-null-hypothesis"><i class="fa fa-check"></i><b>10.1</b> A few words about the null hypothesis</a><ul>
<li class="chapter" data-level="10.1.1" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html#hypotheses-and-null-hypotheses"><i class="fa fa-check"></i><b>10.1.1</b> Hypotheses and null hypotheses</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html#interpreting-and-reporting-p-values"><i class="fa fa-check"></i><b>10.2</b> Interpreting and reporting <em>p</em>-values</a><ul>
<li class="chapter" data-level="10.2.1" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html#what-if-p-is-close-to-0.05"><i class="fa fa-check"></i><b>10.2.1</b> What if <em>p</em> is close to 0.05?</a></li>
<li class="chapter" data-level="10.2.2" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html#presenting-p-values"><i class="fa fa-check"></i><b>10.2.2</b> Presenting <em>p</em>-values</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="hypotheses-and-p-values.html"><a href="hypotheses-and-p-values.html#biological-vs.statistical-significance"><i class="fa fa-check"></i><b>10.3</b> Biological vs. statistical significance</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="parametric-statistics.html"><a href="parametric-statistics.html"><i class="fa fa-check"></i><b>11</b> Parametric statistics</a><ul>
<li class="chapter" data-level="11.1" data-path="parametric-statistics.html"><a href="parametric-statistics.html#introduction"><i class="fa fa-check"></i><b>11.1</b> Introduction</a></li>
<li class="chapter" data-level="11.2" data-path="parametric-statistics.html"><a href="parametric-statistics.html#math-models"><i class="fa fa-check"></i><b>11.2</b> Mathematical models</a></li>
<li class="chapter" data-level="11.3" data-path="parametric-statistics.html"><a href="parametric-statistics.html#parametric-stats"><i class="fa fa-check"></i><b>11.3</b> The normal distribution</a><ul>
<li class="chapter" data-level="11.3.1" data-path="parametric-statistics.html"><a href="parametric-statistics.html#standard-error-of-the-mean"><i class="fa fa-check"></i><b>11.3.1</b> Standard error of the mean</a></li>
<li class="chapter" data-level="11.3.2" data-path="parametric-statistics.html"><a href="parametric-statistics.html#the-t-distribution"><i class="fa fa-check"></i><b>11.3.2</b> The <em>t</em> distribution</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Simple Parametric Statistics</b></span></li>
<li class="chapter" data-level="12" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html"><i class="fa fa-check"></i><b>12</b> One sample <em>t</em>-tests</a><ul>
<li class="chapter" data-level="12.1" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#when-do-we-use-one-sample-t-test"><i class="fa fa-check"></i><b>12.1</b> When do we use one-sample <em>t</em>-test?</a></li>
<li class="chapter" data-level="12.2" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#how-does-the-one-sample-t-test-work"><i class="fa fa-check"></i><b>12.2</b> How does the one-sample <em>t</em>-test work?</a><ul>
<li class="chapter" data-level="12.2.1" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#assumptions-of-the-one-sample-t-test"><i class="fa fa-check"></i><b>12.2.1</b> Assumptions of the one-sample <em>t</em>-test</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#carrying-out-a-one-sample-t-test-in-r"><i class="fa fa-check"></i><b>12.3</b> Carrying out a one-sample <em>t</em>-test in R</a><ul>
<li class="chapter" data-level="12.3.1" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#visualising-the-data-and-checking-the-assumptions"><i class="fa fa-check"></i><b>12.3.1</b> Visualising the data and checking the assumptions</a></li>
<li class="chapter" data-level="12.3.2" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#carrying-out-the-test"><i class="fa fa-check"></i><b>12.3.2</b> Carrying out the test</a></li>
<li class="chapter" data-level="12.3.3" data-path="one-sample-t-tests.html"><a href="one-sample-t-tests.html#summarising-the-result"><i class="fa fa-check"></i><b>12.3.3</b> Summarising the result</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html"><i class="fa fa-check"></i><b>13</b> Two-sample <em>t</em>-test</a><ul>
<li class="chapter" data-level="13.1" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#when-do-we-use-a-two-sample-t-test"><i class="fa fa-check"></i><b>13.1</b> When do we use a two-sample <em>t</em>-test?</a></li>
<li class="chapter" data-level="13.2" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#how-does-the-two-sample-t-test-work"><i class="fa fa-check"></i><b>13.2</b> How does the two-sample <em>t</em>-test work?</a><ul>
<li class="chapter" data-level="13.2.1" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#assumptions-of-the-two-sample-t-test"><i class="fa fa-check"></i><b>13.2.1</b> Assumptions of the two-sample <em>t</em>-test</a></li>
<li class="chapter" data-level="13.2.2" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#what-about-the-equal-variance-assumption"><i class="fa fa-check"></i><b>13.2.2</b> What about the <em>equal variance</em> assumption?</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#carrying-out-a-two-sample-t-test-in-r"><i class="fa fa-check"></i><b>13.3</b> Carrying out a two-sample <em>t</em>-test in R</a><ul>
<li class="chapter" data-level="13.3.1" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#visualising-the-data-and-checking-the-assumptions-1"><i class="fa fa-check"></i><b>13.3.1</b> Visualising the data and checking the assumptions</a></li>
<li class="chapter" data-level="13.3.2" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#carrying-out-the-test-1"><i class="fa fa-check"></i><b>13.3.2</b> Carrying out the test</a></li>
<li class="chapter" data-level="13.3.3" data-path="two-sample-t-test.html"><a href="two-sample-t-test.html#summarising-the-result-1"><i class="fa fa-check"></i><b>13.3.3</b> Summarising the result</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html"><i class="fa fa-check"></i><b>14</b> Paired-sample <em>t</em>-tests</a><ul>
<li class="chapter" data-level="14.1" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html#when-do-we-use-a-paired-sample-t-test"><i class="fa fa-check"></i><b>14.1</b> When do we use a paired-sample <em>t</em>-test?</a></li>
<li class="chapter" data-level="14.2" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html#why-do-we-use-a-paired-sample-design"><i class="fa fa-check"></i><b>14.2</b> Why do we use a paired-sample design?</a></li>
<li class="chapter" data-level="14.3" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html#how-do-you-carry-out-a-t-test-on-paired-samples"><i class="fa fa-check"></i><b>14.3</b> How do you carry out a <em>t</em>-test on paired-samples?</a><ul>
<li class="chapter" data-level="14.3.1" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html#assumptions-of-the-one-sample-t-test-1"><i class="fa fa-check"></i><b>14.3.1</b> Assumptions of the one-sample <em>t</em>-test</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html#carrying-out-a-paired-sample-t-test-in-r"><i class="fa fa-check"></i><b>14.4</b> Carrying out a paired-sample <em>t</em>-test in R</a><ul>
<li class="chapter" data-level="14.4.1" data-path="paired-sample-t-tests.html"><a href="paired-sample-t-tests.html#using-the-paired-true-argument"><i class="fa fa-check"></i><b>14.4.1</b> Using the <code>paired = TRUE</code> argument</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="one-tailed-vs-two-tailed-tests.html"><a href="one-tailed-vs-two-tailed-tests.html"><i class="fa fa-check"></i><b>15</b> One-tailed vs. two-tailed tests</a><ul>
<li class="chapter" data-level="15.1" data-path="one-tailed-vs-two-tailed-tests.html"><a href="one-tailed-vs-two-tailed-tests.html#introduction-1"><i class="fa fa-check"></i><b>15.1</b> Introduction</a></li>
<li class="chapter" data-level="15.2" data-path="one-tailed-vs-two-tailed-tests.html"><a href="one-tailed-vs-two-tailed-tests.html#an-example-of-a-one-tailed-hypothesis"><i class="fa fa-check"></i><b>15.2</b> An example of a one-tailed hypothesis</a></li>
<li class="chapter" data-level="15.3" data-path="one-tailed-vs-two-tailed-tests.html"><a href="one-tailed-vs-two-tailed-tests.html#so-how-do-we-perform-a-one-tailed-t-test"><i class="fa fa-check"></i><b>15.3</b> So how do we perform a one-tailed <em>t</em>-test?</a><ul>
<li class="chapter" data-level="15.3.1" data-path="one-tailed-vs-two-tailed-tests.html"><a href="one-tailed-vs-two-tailed-tests.html#carrying-out-one-tailed-t-tests-in-r"><i class="fa fa-check"></i><b>15.3.1</b> Carrying out one-tailed <em>t</em>-tests in R</a></li>
<li class="chapter" data-level="15.3.2" data-path="one-tailed-vs-two-tailed-tests.html"><a href="one-tailed-vs-two-tailed-tests.html#when-to-use-and-not-to-use-one-tailed-t-tests"><i class="fa fa-check"></i><b>15.3.2</b> When to use, and not to use, one-tailed <em>t</em>-tests</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Categorical Data</b></span></li>
<li class="chapter" data-level="16" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html"><i class="fa fa-check"></i><b>16</b> Working with frequencies</a><ul>
<li class="chapter" data-level="16.1" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html#introduction-2"><i class="fa fa-check"></i><b>16.1</b> Introduction</a></li>
<li class="chapter" data-level="16.2" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html#a-new-kind-of-distribution"><i class="fa fa-check"></i><b>16.2</b> A new kind of distribution</a></li>
<li class="chapter" data-level="16.3" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html#types-of-test"><i class="fa fa-check"></i><b>16.3</b> Types of test</a><ul>
<li class="chapter" data-level="16.3.1" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html#chi2-goodness-of-fit-test"><i class="fa fa-check"></i><b>16.3.1</b> <span class="math inline">\(\chi^{2}\)</span> goodness of fit test</a></li>
<li class="chapter" data-level="16.3.2" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html#chi2-contingency-table-test"><i class="fa fa-check"></i><b>16.3.2</b> <span class="math inline">\(\chi^{2}\)</span> contingency table test</a></li>
<li class="chapter" data-level="16.3.3" data-path="working-with-frequencies.html"><a href="working-with-frequencies.html#the-assumptions-and-requirements-of-chi2-tests"><i class="fa fa-check"></i><b>16.3.3</b> The assumptions and requirements of <span class="math inline">\(\chi^{2}\)</span> tests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="17" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html"><i class="fa fa-check"></i><b>17</b> Goodness of fit tests</a><ul>
<li class="chapter" data-level="17.1" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#when-do-we-use-a-chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>17.1</b> When do we use a chi-square goodness of fit test?</a></li>
<li class="chapter" data-level="17.2" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#how-does-the-chi-square-goodness-of-fit-test-work"><i class="fa fa-check"></i><b>17.2</b> How does the chi-square goodness of fit test work?</a><ul>
<li class="chapter" data-level="17.2.1" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#assumptions-of-the-chi-square-goodness-of-fit-test"><i class="fa fa-check"></i><b>17.2.1</b> Assumptions of the chi-square goodness of fit test</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#carrying-out-a-chi-square-goodness-of-fit-test-in-r"><i class="fa fa-check"></i><b>17.3</b> Carrying out a chi-square goodness of fit test in R</a><ul>
<li class="chapter" data-level="17.3.1" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#summarising-the-result-2"><i class="fa fa-check"></i><b>17.3.1</b> Summarising the result</a></li>
<li class="chapter" data-level="17.3.2" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#a-bit-more-about-goodness-of-fit-tests-in-r"><i class="fa fa-check"></i><b>17.3.2</b> A bit more about goodness of fit tests in R</a></li>
<li class="chapter" data-level="17.3.3" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#doing-it-the-long-way"><i class="fa fa-check"></i><b>17.3.3</b> Doing it the long way…</a></li>
</ul></li>
<li class="chapter" data-level="17.4" data-path="goodness-of-fit-tests.html"><a href="goodness-of-fit-tests.html#determining-appropriate-expected-values"><i class="fa fa-check"></i><b>17.4</b> Determining appropriate expected values</a></li>
</ul></li>
<li class="chapter" data-level="18" data-path="contingency-tables.html"><a href="contingency-tables.html"><i class="fa fa-check"></i><b>18</b> Contingency tables</a><ul>
<li class="chapter" data-level="18.1" data-path="contingency-tables.html"><a href="contingency-tables.html#when-do-we-use-a-chi-square-contingency-table-test"><i class="fa fa-check"></i><b>18.1</b> When do we use a chi-square contingency table test?</a></li>
<li class="chapter" data-level="18.2" data-path="contingency-tables.html"><a href="contingency-tables.html#how-does-the-chi-square-contingency-table-test-work"><i class="fa fa-check"></i><b>18.2</b>  How does the chi-square contingency table test work?</a><ul>
<li class="chapter" data-level="18.2.1" data-path="contingency-tables.html"><a href="contingency-tables.html#assumptions-of-the-chi-square-contingency-table-test"><i class="fa fa-check"></i><b>18.2.1</b> Assumptions of the chi-square contingency table test</a></li>
</ul></li>
<li class="chapter" data-level="18.3" data-path="contingency-tables.html"><a href="contingency-tables.html#carrying-out-a-chi-square-contingency-table-test-in-r"><i class="fa fa-check"></i><b>18.3</b> Carrying out a chi-square contingency table test in R</a><ul>
<li class="chapter" data-level="18.3.1" data-path="contingency-tables.html"><a href="contingency-tables.html#step-1.-getting-the-data-into-the-correct-format"><i class="fa fa-check"></i><b>18.3.1</b> Step 1. Getting the data into the correct format</a></li>
<li class="chapter" data-level="18.3.2" data-path="contingency-tables.html"><a href="contingency-tables.html#step-2.-doing-the-test"><i class="fa fa-check"></i><b>18.3.2</b> Step 2. Doing the test</a></li>
<li class="chapter" data-level="18.3.3" data-path="contingency-tables.html"><a href="contingency-tables.html#summarising-the-result-3"><i class="fa fa-check"></i><b>18.3.3</b> Summarising the result</a></li>
</ul></li>
<li class="chapter" data-level="18.4" data-path="contingency-tables.html"><a href="contingency-tables.html#working-with-larger-tables"><i class="fa fa-check"></i><b>18.4</b> Working with larger tables</a></li>
</ul></li>
<li class="part"><span><b>Associations and Relationships</b></span></li>
<li class="chapter" data-level="19" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html"><i class="fa fa-check"></i><b>19</b> Relationships and regression</a><ul>
<li class="chapter" data-level="19.1" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#introduction-3"><i class="fa fa-check"></i><b>19.1</b> Introduction</a></li>
<li class="chapter" data-level="19.2" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#what-does-linear-regression-do"><i class="fa fa-check"></i><b>19.2</b> What does linear regression do?</a></li>
<li class="chapter" data-level="19.3" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#how-does-simple-linear-regression-work"><i class="fa fa-check"></i><b>19.3</b> How does simple linear regression work?</a><ul>
<li class="chapter" data-level="19.3.1" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#finding-the-best-fit-line"><i class="fa fa-check"></i><b>19.3.1</b> Finding the best fit line</a></li>
</ul></li>
<li class="chapter" data-level="19.4" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#what-do-you-get-out-of-a-regression"><i class="fa fa-check"></i><b>19.4</b> What do you get out of a regression?</a><ul>
<li class="chapter" data-level="19.4.1" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#interpreting-a-regression"><i class="fa fa-check"></i><b>19.4.1</b> Interpreting a regression</a></li>
<li class="chapter" data-level="19.4.2" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#evaluating-hypotheses-inference"><i class="fa fa-check"></i><b>19.4.2</b> Evaluating hypotheses (‘inference’)</a></li>
</ul></li>
<li class="chapter" data-level="19.5" data-path="relationships-and-regression.html"><a href="relationships-and-regression.html#the-assumptions-of-regression"><i class="fa fa-check"></i><b>19.5</b> The assumptions of regression</a></li>
</ul></li>
<li class="chapter" data-level="20" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html"><i class="fa fa-check"></i><b>20</b> Simple regression in R</a><ul>
<li class="chapter" data-level="20.1" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#regression-in-R"><i class="fa fa-check"></i><b>20.1</b> Carrying out a simple linear regression in R</a></li>
<li class="chapter" data-level="20.2" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#first-steps"><i class="fa fa-check"></i><b>20.2</b> First steps</a><ul>
<li class="chapter" data-level="20.2.1" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#visualising-the-data"><i class="fa fa-check"></i><b>20.2.1</b> Visualising the data</a></li>
<li class="chapter" data-level="20.2.2" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#checking-the-assumptions"><i class="fa fa-check"></i><b>20.2.2</b> Checking the assumptions</a></li>
</ul></li>
<li class="chapter" data-level="20.3" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#model-fitting-and-significance-tests"><i class="fa fa-check"></i><b>20.3</b> Model fitting and significance tests</a></li>
<li class="chapter" data-level="20.4" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#present-results"><i class="fa fa-check"></i><b>20.4</b> Presenting results</a><ul>
<li class="chapter" data-level="20.4.1" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#plotting-the-fitted-line-and-the-data"><i class="fa fa-check"></i><b>20.4.1</b> Plotting the fitted line and the data</a></li>
</ul></li>
<li class="chapter" data-level="20.5" data-path="simple-regression-in-r.html"><a href="simple-regression-in-r.html#causation"><i class="fa fa-check"></i><b>20.5</b> What about causation?</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="regression-diagnostics.html"><a href="regression-diagnostics.html"><i class="fa fa-check"></i><b>21</b> Regression diagnostics</a></li>
<li class="chapter" data-level="22" data-path="model-diagnostics.html"><a href="model-diagnostics.html"><i class="fa fa-check"></i><b>22</b> Model diagnostics</a><ul>
<li class="chapter" data-level="22.1" data-path="parametric-statistics.html"><a href="parametric-statistics.html#introduction"><i class="fa fa-check"></i><b>22.1</b> Introduction</a><ul>
<li class="chapter" data-level="22.1.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#understanding-data"><i class="fa fa-check"></i><b>22.1.1</b> Understanding data</a></li>
<li class="chapter" data-level="22.1.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#checking-assumptions"><i class="fa fa-check"></i><b>22.1.2</b> Checking assumptions</a></li>
</ul></li>
<li class="chapter" data-level="22.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#regres-diagnose"><i class="fa fa-check"></i><b>22.2</b> Diagnostics for simple linear regression</a><ul>
<li class="chapter" data-level="22.2.1" data-path="model-diagnostics.html"><a href="model-diagnostics.html#a-simple-linear-regression-example"><i class="fa fa-check"></i><b>22.2.1</b> A simple linear regression example</a></li>
<li class="chapter" data-level="22.2.2" data-path="model-diagnostics.html"><a href="model-diagnostics.html#simple-checks"><i class="fa fa-check"></i><b>22.2.2</b> Simple checks</a></li>
<li class="chapter" data-level="22.2.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#fitted-values"><i class="fa fa-check"></i><b>22.2.3</b> Fitted values</a></li>
<li class="chapter" data-level="22.2.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#checking-the-linearity-assumption"><i class="fa fa-check"></i><b>22.2.4</b> Checking the linearity assumption</a></li>
<li class="chapter" data-level="22.2.5" data-path="model-diagnostics.html"><a href="model-diagnostics.html#checking-the-normality-assumption"><i class="fa fa-check"></i><b>22.2.5</b> Checking the normality assumption</a></li>
<li class="chapter" data-level="22.2.6" data-path="model-diagnostics.html"><a href="model-diagnostics.html#checking-the-constant-variance-assumption"><i class="fa fa-check"></i><b>22.2.6</b> Checking the constant variance assumption</a></li>
</ul></li>
<li class="chapter" data-level="22.3" data-path="model-diagnostics.html"><a href="model-diagnostics.html#regression-diagnostics-the-easy-way"><i class="fa fa-check"></i><b>22.3</b> Regression diagnostics the easy way</a></li>
<li class="chapter" data-level="22.4" data-path="model-diagnostics.html"><a href="model-diagnostics.html#other-diagnose"><i class="fa fa-check"></i><b>22.4</b> Diagnostics for other kinds of models</a></li>
</ul></li>
<li class="chapter" data-level="23" data-path="correlation.html"><a href="correlation.html"><i class="fa fa-check"></i><b>23</b> Correlation</a><ul>
<li class="chapter" data-level="23.1" data-path="correlation.html"><a href="correlation.html#introduction-4"><i class="fa fa-check"></i><b>23.1</b> Introduction</a></li>
<li class="chapter" data-level="23.2" data-path="correlation.html"><a href="correlation.html#correlation-and-regression"><i class="fa fa-check"></i><b>23.2</b> Correlation and regression</a></li>
<li class="chapter" data-level="23.3" data-path="correlation.html"><a href="correlation.html#pearsons-product-moment-correlation-coefficient"><i class="fa fa-check"></i><b>23.3</b> Pearson’s product-moment correlation coefficient</a><ul>
<li class="chapter" data-level="23.3.1" data-path="correlation.html"><a href="correlation.html#pearsons-product-moment-correlation-coefficient-in-r"><i class="fa fa-check"></i><b>23.3.1</b> Pearson’s product-moment correlation coefficient in R</a></li>
<li class="chapter" data-level="23.3.2" data-path="correlation.html"><a href="correlation.html#reporting-the-result"><i class="fa fa-check"></i><b>23.3.2</b> Reporting the result</a></li>
</ul></li>
<li class="chapter" data-level="23.4" data-path="correlation.html"><a href="correlation.html#spearmans-rank-correlation"><i class="fa fa-check"></i><b>23.4</b> Spearman’s rank correlation</a><ul>
<li class="chapter" data-level="23.4.1" data-path="correlation.html"><a href="correlation.html#spearmans-rank-correlation-coefficient-in-r"><i class="fa fa-check"></i><b>23.4.1</b> Spearman’s rank correlation coefficient in R</a></li>
<li class="chapter" data-level="23.4.2" data-path="correlation.html"><a href="correlation.html#reporting-the-result-1"><i class="fa fa-check"></i><b>23.4.2</b> Reporting the result</a></li>
</ul></li>
</ul></li>
<li class="part"><span><b>Comparing Means</b></span></li>
<li class="part"><span><b>Experimental Design</b></span></li>
<li class="part"><span><b>Fixing Problems</b></span></li>
<li class="appendix"><span><b>Supplementary Material</b></span></li>
<li class="chapter" data-level="A" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>A</b> Exercises</a><ul>
<li class="chapter" data-level="A.1" data-path="exercises.html"><a href="exercises.html#week-2"><i class="fa fa-check"></i><b>A.1</b> Week 2</a><ul>
<li class="chapter" data-level="A.1.1" data-path="exercises.html"><a href="exercises.html#what-kind-of-variable-is-it"><i class="fa fa-check"></i><b>A.1.1</b> What kind of variable is it?</a></li>
<li class="chapter" data-level="A.1.2" data-path="exercises.html"><a href="exercises.html#definitions"><i class="fa fa-check"></i><b>A.1.2</b> Definitions</a></li>
<li class="chapter" data-level="A.1.3" data-path="exercises.html"><a href="exercises.html#what-form-do-sampling-distributions-take"><i class="fa fa-check"></i><b>A.1.3</b> What form do sampling distributions take?</a></li>
<li class="chapter" data-level="A.1.4" data-path="exercises.html"><a href="exercises.html#how-does-sample-size-influence-the-standard-error"><i class="fa fa-check"></i><b>A.1.4</b> How does sample size influence the standard error?</a></li>
</ul></li>
<li class="chapter" data-level="A.2" data-path="exercises.html"><a href="exercises.html#week-3"><i class="fa fa-check"></i><b>A.2</b> Week 3</a><ul>
<li class="chapter" data-level="A.2.1" data-path="exercises.html"><a href="exercises.html#sample-size-and-statistical-power"><i class="fa fa-check"></i><b>A.2.1</b> Sample size and statistical power</a></li>
<li class="chapter" data-level="A.2.2" data-path="exercises.html"><a href="exercises.html#a-bit-more-about-statistical-power"><i class="fa fa-check"></i><b>A.2.2</b> A bit more about statistical power</a></li>
</ul></li>
<li class="chapter" data-level="A.3" data-path="exercises.html"><a href="exercises.html#week-4"><i class="fa fa-check"></i><b>A.3</b> Week 4</a><ul>
<li class="chapter" data-level="A.3.1" data-path="exercises.html"><a href="exercises.html#eagle-owls-and-norway-rats"><i class="fa fa-check"></i><b>A.3.1</b> Eagle owls and Norway rats</a></li>
<li class="chapter" data-level="A.3.2" data-path="exercises.html"><a href="exercises.html#the-power-of-pairing"><i class="fa fa-check"></i><b>A.3.2</b> The power of pairing</a></li>
<li class="chapter" data-level="A.3.3" data-path="exercises.html"><a href="exercises.html#fungal-infection-in-french-beans"><i class="fa fa-check"></i><b>A.3.3</b> Fungal infection in French beans</a></li>
<li class="chapter" data-level="A.3.4" data-path="exercises.html"><a href="exercises.html#sheep-grass-and-nature-reserves"><i class="fa fa-check"></i><b>A.3.4</b> Sheep, grass and nature reserves</a></li>
</ul></li>
<li class="chapter" data-level="A.4" data-path="exercises.html"><a href="exercises.html#week-5"><i class="fa fa-check"></i><b>A.4</b> Week 5</a><ul>
<li class="chapter" data-level="A.4.1" data-path="exercises.html"><a href="exercises.html#oviposition-behaviour-in-bean-weevils"><i class="fa fa-check"></i><b>A.4.1</b> Oviposition behaviour in bean weevils</a></li>
<li class="chapter" data-level="A.4.2" data-path="exercises.html"><a href="exercises.html#determining-expected-values"><i class="fa fa-check"></i><b>A.4.2</b> Determining expected values</a></li>
<li class="chapter" data-level="A.4.3" data-path="exercises.html"><a href="exercises.html#sex-and-eye-colour"><i class="fa fa-check"></i><b>A.4.3</b> Sex and eye colour</a></li>
<li class="chapter" data-level="A.4.4" data-path="exercises.html"><a href="exercises.html#eagle-owls-and-prey-choice"><i class="fa fa-check"></i><b>A.4.4</b> Eagle owls and prey choice</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">APS 240: Data Analysis and Statistics with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="model-diagnostics" class="section level1">
<h1><span class="header-section-number">Chapter 22</span> Model diagnostics</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">22.1</span> Introduction</h2>
<p>We usually have an analysis in mind when we design an experiment or observational data collection protocol. It may be tempting to jump straight into this analysis without carefully examining the data first. This is never a good idea. In the few chapters we have repeatedly emphasised that careful data analysis always begins with inspection of the data. Visualising a dataset helps us to understand the data and and evaluate whether or not the assumptions of a statistical tool are likely to be violated.</p>
<div id="understanding-data" class="section level3">
<h3><span class="header-section-number">22.1.1</span> Understanding data</h3>
<p>We’ve been using ‘well-behaved’ data sets in this book so far, which tends to give the impression that visual inspections of the data are not all that necessary. Here’s an example of why it matters. Imagine we are interested in quantifying the relationship between two variables, called <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. We might be tempted to carry out a linear regression analysis without first inspecting these data to get straight to ‘the answer’: the coefficients of the linear regression model. This could be very misleading. Take a look at these four scatter plots:</p>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-167-1.png" width="672" /></p>
<p>These four artificial data sets were constructed by the statistician Francis Anscombe. The means and variances of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are nearly identical in all four data sets, and what’s more, the intercepts and slopes of the best fit regression lines are almost identical (the intercept and slope are 3.00 and 0.500, respectively). The nature of the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is quite obviously different among the four cases:</p>
<ol style="list-style-type: decimal">
<li><p>“Case 1” two linearly related, normally distributed variables. This is the kind of data we often hope for in a statistical analysis.</p></li>
<li><p>“Case 2” shows two variables that are not normally distributed, but there is a perfect non-linear relationship between the tw.</p></li>
<li><p>“Case 3” shows an example the variables are perfectly linearly associated for all but one observation which ruins the perfect relationship.</p></li>
<li><p>“Case 4” shows an example where a single outlier generates an apparent relationship where the two variables are otherwise unrelated.</p></li>
</ol>
<p>Each of these plots tells a different story about the relationship between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>, yet the linear regression model says the same thing is happening in each case. These are obviously somewhat pathological examples, but they clearly illustrate the kinds of issues that can, and do, arise with real data. There is a real risk we will apply an inappropriate analysis if we fail to detect these kinds of problems.</p>
</div>
<div id="checking-assumptions" class="section level3">
<h3><span class="header-section-number">22.1.2</span> Checking assumptions</h3>
<p>Every statistical model makes certain assumptions about the data<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a>. Even if a dataset doesn’t exhibit the very obvious problems seen in the Anscombe examples, we still need to assess whether the assumptions of the statistical model we want to use are likely to be valid. We always check these assumptions each time we introduced a new model or test.</p>
<p>For example, when working with a linear regression models, we started with a scatter plot of the dependent variable vs. the independent variable. This allowed us to assess whether the two variables are linearly related, and to see whether the variability of the residuals in constant. We then plotted the residuals from the fitted regresssion model to evaluate the normality assumption.</p>
<p>These kinds of <em>ad hoc</em> approaches are useful, but they can be often be difficult to apply at times. For example, if we only have a small sample it is difficult to evaluate the normality assumption of a regression using a dot plot or histogram. Similarly, a scatter plot may hide subtle non-linearities in the relationship between two variables.</p>
<p>These limitations can be addressed by moving away from plots of raw data and residuals. It is much better to rely on a set of graphical tools called <strong>regression diagnostics</strong>. Instead of using just the raw data, regression diagnostics use properties of the fitted model to understand how well the model fits the data and evaluate the model assumptions. This chapter is about how to generate and interpret three basic regression diagnostic plots. We’ll learn about these tools in the contect of regression, though they can be used with any statistical model produced by the <code>lm</code> function.</p>
</div>
</div>
<div id="regres-diagnose" class="section level2">
<h2><span class="header-section-number">22.2</span> Diagnostics for simple linear regression</h2>
<p>Before examining some diagnostic plots for linear regression, we should review the underlying assumptions:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Independence.</strong> The residuals must be independent.</p></li>
<li><p><strong>Measurement scale.</strong> The dependent <span class="math inline">\(y\)</span> variable is measured on an interval or ratio scale.</p></li>
<li><p><strong>Measurement error.</strong> The values of the independent <span class="math inline">\(x\)</span> variable are determined with negligible error.</p></li>
<li><p><strong>Linearity</strong> The relationship between the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> variable is linear.</p></li>
<li><p><strong>Normality.</strong> The residuals are drawn from a normal distribution.</p></li>
<li><p><strong>Constant variance.</strong> The variance of the residuals is constant.</p></li>
</ol>
<p>Assumptions 1 (independence), 2 (measurement scale) and 3 (measurement error) are features of the experimental design and the data collection protocol. They generally can not be explicitly checked by looking at the data or by producing regression diagnostics. This leaves assumptions 4 (linearity), 5 (normality) and 6 (constant variance). There is a specific diagnostic plot for each of these.</p>
<p>We’ll work with a specific example to learn how these diagnostics work…</p>
<div class="do-something">
<p>
<strong>Walk through example</strong>
</p>
<p>
You should work through the example in the next few sections.
</p>
</div>
<div id="a-simple-linear-regression-example" class="section level3">
<h3><span class="header-section-number">22.2.1</span> A simple linear regression example</h3>
<p>A survey was carried out to establish whether the abundance of hedgerows in agricultural land had an effect on the abundance of grey partridge. From an area of agricultural land covering several farms, 40 plots were selected which had land uses as similar as possible, but differed in the density of hedgerows (km hedgerow per km<sup>2</sup>). Plots were selected to cover a wide range of hedgerow densities. The density of partridges was established by visiting all fields in a study plot once immediately after dawn and once just before dusk, when partridges are most likely to be seen. Counts of birds observed were made on each visit and the dawn and dusk data were averaged to give a value for partridge abundance for each study plot.</p>
<p>The data are stored in a CSV file PARTRIDG_BIGSTUDY.CSV (not PARTRIDG.CSV!). The density of hedgerows (km per km<sup>2</sup>) is in the <code>Hedgerow</code> variable and the density of partridges (no. per km) is in the <code>Partridge</code> variable. Read the data into R, calling it <code>partridge</code>.</p>
</div>
<div id="simple-checks" class="section level3">
<h3><span class="header-section-number">22.2.2</span> Simple checks</h3>
<p>We relied on simple plots of the raw data to evaluate the assumptions of regression in the last chapter. Let’s revise these evaluations first. We start by constructing a scatter plot of the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(partridge, <span class="kw">aes</span>(<span class="dt">x =</span> Hedgerow, <span class="dt">y =</span> Partridge)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Hedgerow density (km per km²)&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Partridge Count&quot;</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-171-1.png" width="672" /></p>
<div class="do-something">
<p>
Spend some time looking at the scatter plot. Think about the linearity, normality and constant variance assumptions of linear regression. Do you think these data satisfy all three of these assumptions? If not, how is (are) the assumption(s) broken?
</p>
</div>
<p>The linearity, normality and constant variance assumptions are features of: 1) the nature of the relationship between the dependent (<code>Partridge</code>) and independent (<code>Hedgrow</code>) variables, and 2) the scatter of the observations around this relationship. These are difficult to evaluate without at least having a sensible reference point: the line of best fit.</p>
<p>Let’s look at the data again, but this time include the line of best fit. We know how to do this—there are three steps. First, we have to fit the linear regression model to the data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">partridge_model &lt;-<span class="st"> </span><span class="kw">lm</span>(Partridge ~<span class="st"> </span>Hedgerow, <span class="dt">data =</span> partridge)</code></pre></div>
<p>In the second step, we use the <code>predict</code> function with the fitted model object (<code>partridge_model</code>) to generate some predicted values of the dependent variable, over a range of values of the independent variable:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">pred.data &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>(<span class="dt">Hedgerow =</span> <span class="dv">5</span>:<span class="dv">45</span>) %&gt;%<span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">Partridge =</span> <span class="kw">predict</span>(partridge_model, <span class="dt">newdata =</span> .))</code></pre></div>
<p>The final step uses <code>ggplot2</code> to plot the data and add the predictions:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(pred.data, <span class="kw">aes</span>(<span class="dt">x =</span> Hedgerow, <span class="dt">y =</span> Partridge)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="dt">colour =</span> <span class="st">&quot;steelblue&quot;</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data =</span> partridge) +<span class="st"> </span><span class="co"># remember: use the raw data here</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Hedgerow density (km per km²)&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Partridge Count&quot;</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-175-1.png" width="672" /> It is somewhat easier to evaluate the assumptions now that we have added the line of best fit.</p>
<div class="do-something">
<p>
Spend some time looking at the new scatter plot. Think again about the linearity, normality and constant variance assumptions of linear regression. Does including the line of best fit make it easier to assess these assumptions? If your answer is yes, which of the assumption(s) is (are) easier to evaluate now?
</p>
</div>
<p>The second kind of assessment we used is based on the distribution of the residuals from a fitted model. Each observation has one residual associated with it—this is the vertical distance between the relevant value of a partridge density observation and the fitted line. We use the <code>resid</code> function to extract the residuals from a fitted model object:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">resid</span>(partridge_model)</code></pre></div>
<pre><code>##      1      2      3      4      5      6      7      8      9     10 
##  13.30  14.42  17.51  19.74  10.02  10.63   5.44   3.41  -6.16   9.11 
##     11     12     13     14     15     16     17     18     19     20 
##   6.64  -5.06 -18.81 -11.39 -11.55  -0.91 -13.99   1.12  -1.40  -2.69 
##     21     22     23     24     25     26     27     28     29     30 
## -15.53 -19.65 -15.76 -10.53  -7.16 -10.75 -38.01  -9.51  -0.86 -25.91 
##     31     32     33     34     35     36     37     38     39     40 
##   7.38 -21.33  27.48  18.02 -44.07  44.00  27.74   9.51  51.15  -5.63</code></pre>
<p>Staring at a long list of residuals is not really all that useful. Instead, we need to summarise their distribution, using either a dot plot or a histogram. We have to place them into a data frame first to use <code>ggplot</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># step 1 -- place the residuals in a data frame</span>
plt_data &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">residuals =</span> <span class="kw">resid</span>(partridge_model))
<span class="co"># step 2 -- make a dot plot</span>
<span class="kw">ggplot</span>(plt_data, <span class="kw">aes</span>(<span class="dt">x =</span> residuals)) +<span class="st"> </span><span class="kw">geom_dotplot</span>(<span class="dt">binwidth =</span> <span class="dv">5</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-179-1.png" width="672" /></p>
<div class="do-something">
<p>
Which assumption of linear regression—linearity, normality or constant variance—does this histogram allow you to evaluate? Does it look like the assumption has been violated?
</p>
</div>
<p>That’s enough revision. Now it’s time to turn to the regression diagnostics.</p>
</div>
<div id="fitted-values" class="section level3">
<h3><span class="header-section-number">22.2.3</span> Fitted values</h3>
<p>In order to understand regression diagnostics we have to know what a <strong>fitted value</strong> is. The phrase ‘fitted value’ is just another expression for ‘predicted value’. Look at the plot below: <img src="stats-for-bio_files/figure-html/unnamed-chunk-181-1.png" width="672" /> This shows the raw data (black points), the line of best fit (blue line), the residuals (the vertical grey lines), and the fitted values (red points). We find the fitted values by drawing a vertical line from each observation to the line of best fit. The values of the dependent variable (<code>Partridge</code> in this case) at the point where these touch the line of best fit are the ‘fitted values’. This means the fitted values are just predictions from the statistical model, generated for each value of the independent variable. We can use the <code>fitted</code> function to extract these from a fitted model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">fitted</span>(partridge_model)</code></pre></div>
<pre><code>##     1     2     3     4     5     6     7     8     9    10    11    12 
## -11.6 -11.0  -8.4  -7.2  -4.9  -1.4   1.8   7.3   8.2  10.2  11.1  14.3 
##    13    14    15    16    17    18    19    20    21    22    23    24 
##  22.1  22.7  27.0  28.2  28.8  35.2  37.8  38.1  43.0  45.4  48.3  52.3 
##    25    26    27    28    29    30    31    32    33    34    35    36 
##  57.6  61.0  62.2  62.2  63.7  68.3  71.2  77.0  86.6  87.8  88.1  90.4 
##    37    38    39    40 
##  91.6  96.8  98.0 101.7</code></pre>
<div class="do-something">
<p>
Notice that some of the fitted values are below zero. Why do we see negative fitted values? This doesn’t make much sense biologically (negative partridges?). Do you think it is a problem?
</p>
</div>
</div>
<div id="checking-the-linearity-assumption" class="section level3">
<h3><span class="header-section-number">22.2.4</span> Checking the linearity assumption</h3>
<p>The linearity assumption states that the general relationship between the dependent and independent variable should look like a straight line. We can evaluate this assumption by constructing a <strong>residuals vs. fitted values</strong> plot. This is a two-step process. First use the <code>fitted</code> and <code>resid</code> functions to construct a data frame containing the fitted values and residuals from the model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plt_data &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>(<span class="dt">Fitted =</span> <span class="kw">fitted</span>(partridge_model), 
             <span class="dt">Resids =</span>  <span class="kw">resid</span>(partridge_model))</code></pre></div>
<p>We called the data frame <code>plt_data</code>. Once we have made this data frame, we use <code>ggplot2</code> to plot the residuals against the fitted values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(plt_data, <span class="kw">aes</span>(<span class="dt">x =</span> Fitted, <span class="dt">y =</span> Resids)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Residuals&quot;</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-186-1.png" width="672" /> What does this plot tell us? It indicates that the residuals tend to be positive at the largest and smallest fitted values, and that they are generally negative in the middle of the range. This U-shaped pattern is indicative of a problem with our model. It tells us that there is some kind of pattern in the association between the two variables that is not being accommodated by the linear regression model we fitted to the data. The U-shape indicates that the relationship is non-linear, and that it ‘curves upward’.</p>
<p>We can see where this pattern comes from when we look at the raw data and fitted model again: <img src="stats-for-bio_files/figure-html/unnamed-chunk-187-1.png" width="672" /> There is obviously some curvature in the relationship between partridge counts and hedgerow density, yet we fitted a straight line through the data. The U-shape in the residuals vs. fitted value plot comes from the fact that the relationship between the dependent and independent variables is ‘accelerating’ (also called ‘convex’).</p>
<p>What other kinds of patterns might we see residuals vs. fitted value plot? Two are particularly common: U-shapes (the one we just saw) and hump-shapes. Look at the two artificial data sets below… <img src="stats-for-bio_files/figure-html/unnamed-chunk-189-1.png" width="672" /> The data labelled ‘Accelerating’ is similar to the partridge data: it exhibits a curved, accelerating relationship between the dependent variable and the independent variable. The data labelled ‘Decelerating’ shows a different kind of relationship: there is a curved, decelerating relationship between the two variables. We can fit a linear model to each of these data sets, and then visualise the corresponding residuals vs. fitted value plots: <img src="stats-for-bio_files/figure-html/unnamed-chunk-190-1.png" width="672" /> Here we see the characteristic U-shape and hump-shape pattern we mentioned above. The U-shape occurs when there is an accelerating relationship between the dependent variable and the independent variable. The hump-shape occurs when there is an decelerating relationship between the dependent variable and the independent variable.</p>
<p>By this point something may be bothering you. This seems like a lot of extra work to evaluate an aspect of the model that we can assess by just plotting the raw data. This is true when we are working with a simple linear regression model. However, it can much harder to evaluate the linearity assumption when working with more complicated models where there is more than one independent variable<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a>. In these situations, a residuals vs. fitted values plot gives us a powerful way to evaluate whether or not the assumption of a linear relationship is reasonable.</p>
<p>That’s enough about the residuals vs. fitted values plot. Let’s move on to the normality evaluation…</p>
</div>
<div id="checking-the-normality-assumption" class="section level3">
<h3><span class="header-section-number">22.2.5</span> Checking the normality assumption</h3>
<p>How should we evaluate the normality assumption of linear regression? That is, how do we assess whether or not the residuals are drawn from a normal distribution? We know how to extract the residuals from a model and plot their distribution, but there is a more powerful graphical technique to available to us: the <strong>normal probability plot</strong>.</p>
<p>The normal probability plot is used to identify departures from normality. If we know what we are looking for, we can identify many different kinds of problems, but to keep life simple we will focus on the most common type of assessment: determining whether or not the distribution of residuals is excessively <strong>skewed</strong>. Remember the concept of distributional skew? A skewed distribution is just one that is not symmetric. For example, the first distribution below is skewed to the left (‘negative skew’), the second is skewed to the right (‘positive skew’), and the third is symmetric (‘zero skew’): <img src="stats-for-bio_files/figure-html/unnamed-chunk-191-1.png" width="672" /></p>
<p>The skewness in the first two distributions is easy to spot because they contain a lot of data and the skewness is quite pronounced. A normal probability plot allows us to pick up potential problems when we are not so lucky. The methodology underlying construction of a normal probability plot is quite technical, so we will only try to give a flavour of it here. Don’t worry if the next segment is confusing—interpreting a normal probability plot is much easier than making one.</p>
<p>We’ll work with the partridge example again. We start by extracting the residuals from the fitted model into a vector, using the <code>resids</code> function, and then standardise these by dividing them by their standard deviation:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_resids &lt;-<span class="st"> </span><span class="kw">resid</span>(partridge_model)
mod_resids &lt;-<span class="st"> </span>mod_resids /<span class="st"> </span><span class="kw">sd</span>(mod_resids)</code></pre></div>
<p>The standardisation step is not essential, but dividing the raw residuals by their standard deviation ensures that the standard deviation of the new residuals is equal to 1. Standardising the residuals like this makes it a little easier to compare more than one normal probability plot. We call these new residuals the ‘standardised residuals’.</p>
<p>The next step is to find the rank order of each residual. That is, we sort the data from lowest to highest, and find the position of each case in the sequence (this is its ‘rank’). The function <code>order</code> can does this:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">resid_order &lt;-<span class="st"> </span><span class="kw">order</span>(mod_resids)
resid_order</code></pre></div>
<pre><code>##  [1] 35 27 30 32 22 13 23 21 17 15 14 26 24 28 25  9 40 12 20 19 16 29 18
## [24]  8  7 11 31 10 38  5  6  1  2  3 34  4 33 37 36 39</code></pre>
<p>This tells us that the first residual is the 35th largest, the second is the 27th largest, the third is the 30th largest, and so on.</p>
<p>The last step is the tricky one. Once we have established the rank order of the residuals, we ask the following question: if the residuals really were drawn from a normal distribution, what is their most likely value, based on their rank? We can’t really explain how to do this without delving into the mathematics of distributions, so this will have to be a ‘trust us’ situation. As usual, R can do this for us, and we don’t even need the ranks—we just calculated them to help us explain what happens when we build a normal probability plot. The function we need is called <code>qqnorm</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">all_resids &lt;-<span class="st"> </span><span class="kw">qqnorm</span>(mod_resids, <span class="dt">plot.it =</span> <span class="ot">FALSE</span>)
all_resids &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(all_resids)</code></pre></div>
<p>The <code>qqnorm</code> doesn’t produce a data frame by default, so we had to covert the result using a function called <code>as.data.frame</code>. This extra little step isn’t really all that important.</p>
<p>The <code>all_resids</code> object is now a data frame with two variables: <code>x</code> contains the theoretical values of normally distributed residuals, based on the rank orders of the residuals from the model, and <code>y</code> contains the actual standardised residuals. Here are the first 10 values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">head</span>(all_resids, <span class="dv">10</span>)</code></pre></div>
<pre><code>##             x          y
## 1   0.7977768  0.6855052
## 2   0.8871466  0.7431596
## 3   0.9842350  0.9021176
## 4   1.2133396  1.0174265
## 5   0.6356570  0.5162945
## 6   0.7143674  0.5478778
## 7   0.2858409  0.2800927
## 8   0.2211187  0.1759340
## 9  -0.2858409 -0.3173156
## 10  0.4887764  0.4693592</code></pre>
<p>Finally, we can plot these against one another to make a normal probability plot:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(all_resids, <span class="kw">aes</span>(<span class="dt">x =</span> x, <span class="dt">y =</span> y)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span><span class="kw">geom_abline</span>(<span class="dt">intercept =</span> <span class="dv">0</span>, <span class="dt">slope =</span> <span class="dv">1</span>) +
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Theoretical Value&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Standardised Residual&quot;</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-196-1.png" width="672" /> We used <code>geom_abline(intercept = 0, slope = 1)</code> to add the one-to-one (1:1) line. We haven’t used this function before and we won’t need it again. The one-to-one line is just a line with a slope of 1 and an intercept of 0—if an <span class="math inline">\(x\)</span> value and <span class="math inline">\(y\)</span> value are equal their corresponding point will lie on this line.</p>
<p>Don’t worry too much if those calculations seem opaque. We said at the beginning of this section that it’s not important to understand how a normal probability plot is constructed. It is important to know how to interpret one. The important feature to look out for is the positioning of the points relative to the 1:1 line. If the residuals really are drawn from a normal distribution they should generally match the theoretical values, i.e. the points should lie on the 1:1 line.</p>
<p>In the partridge example that is exactly what we see. A couple of the more extreme values diverge a little, but this isn’t something to worry about. We never expect to see a perfect 1:1 relationship in these kinds of plots. The vast majority of the points are very close to the 1:1 line though, which provides strong evidence that the residuals probably are sampled from a normal distribution.</p>
<p>What does a normal probability plot look like when residuals are not consistent with the normality assumption? Deviations from a straight line suggest departures from normality. How do right skew (‘positive skew’) and left skew (‘negative skew’) manifest themselves in a normal probability plot? Here is the normal probability plot produced using data from the left-skewed distribution above: <img src="stats-for-bio_files/figure-html/unnamed-chunk-197-1.png" width="672" /> Rather than a straight line, we see a decelerating curved line. This is the signature of residuals that are non-normal, and left-skewed. We see the opposite sort of curvature when the residuals are right-skewed: <img src="stats-for-bio_files/figure-html/unnamed-chunk-198-1.png" width="672" /></p>
<p>You should always use normal probability plots to assess normality assumptions in your own analyses. They work with every kind of model fitted the <code>lm</code> function deals with. What is more, they also work reasonably well when we only have a few residuals to play with. Seven is probably the lowest number we might accept—with fewer points it becomes hard to distinguish between random noise and a real deviation from normality.</p>
<p>That’s enough discussion of normal probability plot. Let’s move on to the constant variance evaluation…</p>
</div>
<div id="checking-the-constant-variance-assumption" class="section level3">
<h3><span class="header-section-number">22.2.6</span> Checking the constant variance assumption</h3>
<p>How do we evaluate the constant variance assumption of a linear regression? That is, how do we assess whether or not the variability of the residuals is constant or not? This assumption can be evaluated in a similar way to the linearity assumption, by producing something called a ‘scale-location plot’. We construct this by plotting residuals against the fitted values, but instead of plotting raw residuals, we transform them first using the following ‘recipe’:</p>
<ol style="list-style-type: decimal">
<li><p>Standardise the residuals by dividing them by their standard deviation. Remember, this ensures the new residuals have a standard deviation of 1.</p></li>
<li><p>Find the absolute value of the residuals produced in step 1. If they are negative, make them positive, otherwise, leave them alone.</p></li>
<li><p>Take the square root of the residuals produced in step 2.</p></li>
</ol>
<p>These calculations are simple enough in R. We’ll demonstrate them using the partridge data set again:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># extract the residuals</span>
sqrt_abs_resids &lt;-<span class="st"> </span><span class="kw">resid</span>(partridge_model)
<span class="co"># step 1. standardise them</span>
sqrt_abs_resids &lt;-<span class="st"> </span>sqrt_abs_resids /<span class="st"> </span><span class="kw">sd</span>(sqrt_abs_resids)
<span class="co"># step 2. find their absolute value</span>
sqrt_abs_resids &lt;-<span class="st"> </span><span class="kw">abs</span>(sqrt_abs_resids)
<span class="co"># step 3. square root these</span>
sqrt_abs_resids &lt;-<span class="st"> </span><span class="kw">sqrt</span>(sqrt_abs_resids)</code></pre></div>
<p>Now we use the <code>fitted</code> function to extract the fitted values form the model and place these in a data frame with the transformed residuals:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">plt_data &lt;-<span class="st"> </span>
<span class="st">  </span><span class="kw">data.frame</span>(<span class="dt">Fitted =</span> <span class="kw">fitted</span>(partridge_model), <span class="dt">Resids =</span> sqrt_abs_resids)</code></pre></div>
<p>We called the data frame <code>plt_data</code>. Once we have made this data frame, we use <code>ggplot2</code> to plot the transformed residuals against the fitted values:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(plt_data, <span class="kw">aes</span>(<span class="dt">x =</span> Fitted, <span class="dt">y =</span> Resids)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>() +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;Fitted values&quot;</span>) +<span class="st"> </span><span class="kw">ylab</span>(<span class="st">&quot;Square root of absolute residuals&quot;</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-201-1.png" width="672" /> This is a scale-location plot. Why is this useful? We are looking to see if there is any kind of relationship between the transformed residuals and the fitted values, i.e., we want to assess whether or not the size of these new residuals increase or decrease as the fitted values get larger. If they do not—the relationship is essentially flat—then we can conclude that the variability in the residuals is constant. Otherwise, we have to conclude that the constant variance assumption is violated.</p>
<p>Although the pattern is not exactly clear cut, in this example there seems to be a bit of an upward trend with respect to the fitted values. This suggests that the variability (more formally, the ‘variance’) of the residuals increases with the fitted values. Larger partridge counts seem to be associated with more variability. This is a very common feature of count data.</p>
<div class="advanced-box">
<p>
<strong>Poor model fit complicates scale-location plots</strong>
</p>
<p>
It is worth reflecting on the ambiguity in this pattern. It is suggestive, but it certainly isn’t as clear as the U-shape in the residuals vs. fitted values plot used earlier. There is one potentially important reason for this ambiguity. The model we have used to describe the relationship between partridge counts and hedgerow density is not a very good model for these data. There is curvature in the relationship that we failed to take account of, and consequently, this lack of fit is impacting the scale-location plot. When a model does not fit the data well, the scale-location plot does not only describe the variability in the residuals. It also reflects the lack of fit. The take-home message is that it is a good idea to fix a lack of fit problem before trying to evaluate the constant variance assumption.
</p>
</div>
</div>
</div>
<div id="regression-diagnostics-the-easy-way" class="section level2">
<h2><span class="header-section-number">22.3</span> Regression diagnostics the easy way</h2>
<p>It turns out that we did not have to do all that work to construct the three diagnistic plots. R has a built in facility to plot these. It works by using a function called <code>plot</code> with the fitted model object. For example, to produce a residuals vs fitted values plot, we use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(partridge_model, <span class="dt">which =</span> <span class="dv">1</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-203-1.png" width="672" /> The first argument is the name of fitted model object. The second argument controls the output of the <code>plot</code> function: <code>which = 1</code> argument tells it to produce a residuals vs. fitted values plot. That’s it.</p>
<p>Once again, there is no need to laboriously construct these plots. R has a built in facility to construct them from a fitted model object, accessed with the <code>plot</code> function :</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(partridge_model, <span class="dt">which =</span> <span class="dv">2</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-204-1.png" width="672" /> Again, the first argument must be the fitted model object, but this time the second argument needs to be <code>which = 2</code>. This produces essentially the same kind of plot we just made, with one small difference (apart from the different <span class="math inline">\(x\)</span> axis label, which we can ignore). Rather than drawing a 1:1 line, the ‘plot’ function shows us a line of best fit. This just allows us to pick out the curvature a little more easily.</p>
<p>Once again, we do not have to manually construct this plot. We can produce a scale-location plot using the <code>plot</code> function by using the <code>which = 3</code>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(partridge_model, <span class="dt">which =</span> <span class="dv">3</span>)</code></pre></div>
<p><img src="stats-for-bio_files/figure-html/unnamed-chunk-205-1.png" width="672" /></p>
<p>Non-constant variance can be a problem because it affects the validity of <em>p</em>-values associated with a model. You should aim to use scale-location plots to assess the constant variance assumption in your own analyses, but keep in mind that a scale-location plot may also reflect non-linearity.</p>
</div>
<div id="other-diagnose" class="section level2">
<h2><span class="header-section-number">22.4</span> Diagnostics for other kinds of models</h2>
<p>We have focussed on simple linear regression in this session because regression diagnostics are most easily understood in the context of a regression model. However, the term ‘regression diagnostic’ is a bit of a misnomer. A more accurate term might be ‘linear model diagnostic’ but no one really uses this. Regression diagnostics can be used with many different kinds of models. In fact, the diagnostic plots we have introduced here can be applied to any model fitted by the <code>lm</code> function. This includes things like the ANOVA models we’ll study in later chapters.</p>
<div class="advanced-box">
<p>
<strong>Don’t panic if your dignostics aren’t perfect!</strong>
</p>
<p>
The good news about regression is that it is quite a robust technique. It will often give us reasonable answers even when the assumptions are not perfectly fulfilled. We should be aware of the assumptions but should not become too obsessed by them. If the violations are modest, it is often fine to proceed. We just need to interpret results with care. Of course, we have to know what constiutes a ‘modest’ violation. There are no hard and fast rules. The ability to make that judgement is something that comes with experience.
</p>
</div>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="38">
<li id="fn38"><p>Even so-called ‘non-parametric’ models have underpinning assumptions; these are just not as restrictive as their parametric counterpart<a href="model-diagnostics.html#fnref38">↩</a></p></li>
<li id="fn39"><p>This is the situatoin we face with multiple regression. A multiple regression is a type regression with more than one independent variable—we don’t study them in this course, but they are often used in biology<a href="model-diagnostics.html#fnref39">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression-diagnostics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="correlation.html" class="navigation navigation-next " aria-label="Next page""><i class="fa fa-angle-right"></i></a>

<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/dzchilds/stats-for-bio/edit/master/5_03_regression_diagnostics.Rmd",
"text": null
},
"download": null,
"toc": {
"collapse": "section"
},
"split_by": "section"
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
