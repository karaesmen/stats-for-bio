[
["index.html", "APS 240: Data Analysis and Statistics with R Chapter 1 Course information and overview 1.1 Why do a data analysis course? 1.2 Course overview 1.3 How to use the teaching material 1.4 Health and safety using display screen equipment", " APS 240: Data Analysis and Statistics with R Dylan Z. Childs 2016-11-22 Chapter 1 Course information and overview This is the online course book for the Data Analysis and Statistics with R (APS 240) module. You can view this book in any modern desktop browser, as well as on your phone or tablet device. The site is self-contained—it contains all the material you are expected to learn this year. Dylan Childs is the course co-coordinator. Please email him if you have have any general queries about the course. Andrew Beckerman is the second course instructor. The Teaching Assistants (‘TAs’) this year are Ross Booton, Matthew Hethcoat, Bethan Hindle, Tamora James, Felix Lim, and Simon Mills. 1.1 Why do a data analysis course? To do science yourself, or to understand the science other people do, you need some understanding of the principles of experimental design, data collection, data presentation and data analysis. That doesn’t mean becoming a maths wizard, or a computer genius. It means knowing how to take sensible decisions about designing studies and collecting data, and then being able to interpret those data correctly. Sometimes the methods required are extremely simple, sometimes more complex. You aren’t expected to get to grips with all of them, but what we hope to do in the course is to give you a good practical grasp of the core techniques that are widely used in biology and environmental sciences. You should then be equipped to use these techniques intelligently and, equally importantly, know when they are not appropriate, and when you need to seek help to find the correct way to design or analyse your study. You should, with some work on your part, acquire a set of skills which you will use at various stages throughout the remainder of your course, in practicals, field courses and in your project work. These same skills will almost certainly also be useful after your degree, whether doing biology, or something completely different. We live in a world that is increasingly flooded with data, and people who know how to make sense of this are in high demand. The R statistical programming environment underpins much of this endeavour, in both academic and commercial settings. Learning the basic principles of data analysis with R will only improve your employment prospects. 1.2 Course overview 1.2.1 Aims This course has two main, and equal, aims. The first is to provide a basic training in the use of statistical methods and software (R and RStudio) to analyse biological data. The second is to introduce some of the principles of experimental design, sampling, data interpretation, graphical presentation and scientific writing relevant to the biological and environmental sciences. 1.2.2 Objectives By the end of the course you should be familiar with the principles and use of a range of basic statistical techniques, be able to use the R programming language to carry out appropriate analyses of biological data, evaluate your statistical models, and make sensible interpretation of the results. You should be able to relate the ways in which data are collected (by different designs of sampling or experiment) to the types of statistical methods that can be used to analyse those data. In combination with the skills you developed in APS 135, you should be able to decide on appropriate ways of investigate data graphically, be able to produce good quality scientific figures, and incorporate these, along with statistical results, into a formal report. 1.2.3 Assumed background You are assumed to be familiar with the use of personal computers on the University network, and with the use of R for data input, manipulation and plotting introduced in APS 135. If you are unsure about these basic methods, then you will need to revise the material covered in the Level 1 IT practicals. The key skills you need are covered in the Programming prerequisites chapter. We will revise the most important topics in the first practical session. Environmental Sciences Students Don’t panic if you are one of the Environmental Sciences students joining us from Geography. We’ll provide targetted help at the beginning of the course to help you learn the fundamentals of R. 1.2.4 Methods The course runs over semester 1, in weeks 1-12. The first 10 weeks consists of a 2-hour IT practical each week, along with some additional preparatory practical work and reading. All components of the course are compulsory. The remaining 2 weeks are devoted to revision and an assessed data analysis project. Students come to APS 240 with very different experiences and competancies. Because of this unavoidable situation, the course is designed as a ‘self-teaching’ module to you flexibility in the rate at which you work through the material. This doesn’t mean no one is going to help you, but you are expected to take responsibility of your own learning. 1.2.4.1 Preparatory reading and practical work Each week starts with some preparatory reading and ‘walk-through’ practical work from the course book. We’ll let you know what you need to do each week. The book chapters generally come in one of two forms: Practical walk-through chapters. These are designed to introduce the practical aspects of different analysis techniques. These generally focus on the ‘when’ and ‘why’ we use a particular technique, as well how to actually do it in R. You are free (encouraged even) to work through these in groups. Concept chapters. These focus on ideas and concepts, rather then using R per se (though they may use R from time to time). They provide background information or more detailed discussion relating to the topics you are covering. These are an integral part of the course. Please don’t skip them! It is up to you to complete the preparatory work in your own time. However, we are not expecting you to understand everything the first time around. This point is so important it’s worth saying again—you are not expected to understand everything in the course book the first time you read it. Just do your best to understand it, taking careful notes of anything you’re struggling with. The TAs and staff will happily answer any questions you have during the timetabled sessions. Indeed, that is what they’re there to do. 1.2.4.2 Timetabled practicals The timetabled practicals take place in the APS IT rooms, either Perak IT labs or B56. In each of these sessions, you will work through a number of small exercises to help you consolidate what you’re been working on. You’ll be asked to note down your answers to some of the exercises. The correct answers will be available. TAs and staff are there to help if you get stuck, so don’t suffer in silence. You can (should!) also use this time to ask questions about concepts that are confusing you. You are welcome to use your own computer to complete your work. Keep in mind that the university computers are the only ‘officially supported’ platform. If you run into a problem using your own computer, the TAs and staff will try to help resolve these. Unfortunately, if these prove to be intractable, you will have to use the university computing facilities. It just isn’t fair on other students for teaching staff to spend valuable contact time trying to solve installation / setup problems. 1.2.5 Non-assessed material Although every topic is important, in the sense that it contains material that will help you become a better data analyst, we want to avoid creating too much of an assessment burden in this course. To this end, the material in a few of the chapters is be formally assessed. The Expected learning outcomes chapter tells you what you need to be able to know by the end of the course. If you’re the kind of person who wants to focus on the assessment, you should use that as your guide. As always, feel free ask an instructor (not a TA) for clarification if you’re not sure of what you need to be able to do. 1.2.6 What is required of you? A willingness to learn and to take responsibility for your learning! Data analysis is not the easiest subject in the world, but neither is it the most difficult. It’s worth making the effort. What you learn in this course will form the basis for much of what you do in field course, practical and project work that follows in later semesters. The minimum requirement for the course is that you: attend your designated practical session each week (please ensure that you arrive on time and register, or you will be recorded as absent), complete the preparatory practical work and reading, taking careful notes of anything that you don’t understand, be proactive about your lerning and ask the TAs and staff questions about things you’re struggling with, complete the exercises for each week before the next practical class, and check through your answers to questions using MOLE. How you work through the book is fairly flexible, but remember, the self-study preparatory work lays the foundatation for the material covered in the timetabled practicals. Take our word for it, you won’t get much out of the timetabled sessions if you skip the preparatory work. In each practical session you should aim to complete most, if not all, of the exercises for that practical. If you don’t manage to do this you should try to finish off the work in your own time before the next practical. However, if you have problems with any of the work, staff will help you during the practical sessions, even if it is not the topic designated for that session. So if you need to catch up, there will be opportunities to do so. A word of advice: Don’t let the flexibility of the course tempt you into letting a backlog of work build up. This will compromise your ability to do the assessed work when it is set and will make it difficult to revise for the exam. One last point: the university guidelines assume the total study time associated with a 10 credit module to be about 100 hours. There is an expectation that you will spend significant time outside the timetabled practical classes working on this module. You should aim to spend about 5 hours each week working through the chapters, completing the practical exercises, and finally, working on the assessed project. This leaves you about 40 hours to revise for the formal exam in the new year. 1.2.7 Assessment Assessment of the course will have two components. The first is a short data analysis project in weeks 11-12 of the course, the second is an open book exam in the winter exam period. ‘Open book’ means you will have access to this book during the exam, i.e. you don’t have to memorise everything in here, just understand it! Further details will be given as the course progresses. 1.3 How to use the teaching material 1.3.1 The online course book All of the teaching material will be made available through a single online course book (this website). The book is organised such that it forms a complete, stand-alone introduction to data analysis. You should bookmark this now if you haven’t already done so. There are a couple of very good reasons for delivering the course material this way: Practicality: Most exercises can be completed by building on the examples in the course material. Copying the relevant R code from the course website and pasting it into your script is much more efficient, and less error-prone, than copying by eye from a printed page. A website also allows us to cross-reference topics and link to the odd bit of outside reading. Permanence: Experience suggests that many of you will want to refer to the material in this course after you graduate. However, bits of paper are easy to lose, and because the R landscape is always changing, some elements of the course may be less relevant in a few years time. By putting everything on a website, we can ensure that you will always be able to access a familiar, but up-to-date data analysis course. 1.3.2 Printed material There is a small amount of printed material in this course: Cheat sheets: We will supply you with copies of the dplyr and ggplot2 cheat sheets produced by the people who build RStudio . It may help you to refer to these when you need use either the dplyr or ggplot2 packages in a practical. Assessment information: Although much of the assessment will be done on the computer, any information relating to the assessments will be produced in printed form. This will be handed out in week 10. 1.3.3 How to make best use of the teaching material DO: When working through an exercise, follow the instructions carefully, but also think about what you are doing. Work at your own pace; you are not being assessed on whether you can do an exercise in a particular time. Ask teaching staff for help in the practicals if there are things that you don’t follow, or when things don’t seem to come out the way they should—that’s what they’re there for! Collaborate! If you are not sure you understand something feel free to discuss it with a friend—more often than not this is exactly how scientists resolve and clarify problems in their experimental design and analysis. Be prepared to experiment with R to solve problems that you encounter. You can’t break your R or RStudio by generating errors. When you run into a problem, go back to the line of code that generated the first error and try making a change. Complete each week’s work before the next week’s session. You may be able to complete some sessions quite quickly, others may take more time and require more work on your own outside the timetabled periods. DON’T: Just copy what someone else tells you to do without understanding why you are doing it. You need to understand it for yourself (and you’ll be on your own in the exam). Skip practicals or preparatory work and get behind schedule — there is too much material to assimilate all at once when you get to the assessments. Like all skills data analysis is something you have practice. 1.3.4 Conventions used in the course material The teaching material, as far as possible, uses a standard set of conventions to differentiate between various sorts of information, action and instruction: 1.3.4.1 Text, instructions, and explanations Normal text, instructions, explanations etc. are written in the same type as this paragarph (obvious really), we will tend to use bold to highlight specific technical terms when they are first introduced. Italics are generally used for emphasis and with Latin names. When we want you to do something important or pay particular attentions—e.g., waksing you to write down an answer or giving you a set of instructions—we place the text inside a box like this one: Brown boxes Here is some important text telling you to do something or remember something important. Sometimes it just contains a warning that the next bit will be hard… Please don’t ignore these. At various points in the text you will also come across different coloured boxes that contain additional information: Blue boxes These aim to offer a not-too-technical discussion of how or why something works the way it does. These are things that it may be useful to know, or at least know about, but aren’t necessarily part of the main thread of a section. Red boxes These contain a warning or flag a common gotcha that may trip you up. They highlight potential pitfalls and show you how to avoid them. You will avoid a lot of future mistakes if pay close attention to these. We use block quotations to indicate an example of how a particular statistical result should be presented when you write it in a report: e.g. The mean lengths of male and female locusts differed significantly (t=4.04, df=15, p=0.001), with males being significantly larger. 1.3.4.2 R code, files and RStudio This typeface is used to distinguish R code within a sentence of text: e.g. “We use the summary function to obtain information about an object produced by the lm function.” A sequence of selections from an RStudio menu is indicated as follows: e.g. File ▶ New File ▶ R Script File names referred to in general text are given in upper case in the normal typeface: e.g. MYFILE.CSV. 1.3.5 Feedback There are a number of ways in which you can obtain feedback on how well you understand the course material 1.3.5.1 Self-assessment questions: At various points in the course material there are questions for you to answer. When you reach one of these, you should be in a position to answer the question —- so make a note of the answer! When you’ve completed the session, you can check your answers using the ‘self-test’ for that particular session on MOLE. You will see if you have the correct answer and in some cases you will also get some additional explanation as to why that answer is right (or wrong!). 1.3.5.2 Each other: Discussing what you are doing with someone as you go along, or working through a problem with someone else, can help clarify your understanding. Please bear in mind, however, that you learn little or nothing by simply copying information from someone else, and when it comes to the assessed project, it must be your own work. 1.3.5.3 Staff: In the practicals you will have opportunities to ask questions and discuss what you are doing with staff and teaching assistants. They are not just there to help you with the practical. You should use them to help you work through any problems you have with the course material, both conceptual and practical. There will also be an opportunity to have topics you raise discussed in later practicals. 1.3.6 Help sessions We will run an open ‘help’ session every Friday from 12-2.00pm, in the B56 IT Room in APS. An instructor will be on hand during this period to answer specific questions about the course material. This room holds about 40 students, so please only attend if you require one-to-one assistance, i.e, don’t just use this session to complete unfinished practicals (unless you are stuck of course). 1.3.7 Overall… We hope that the material is clear and easy to use, and that you find the course useful, or even enjoy it! In a text of this size, which is continually being improved and updated, errors do creep in; if you find something you think is wrong please tell us. If it’s not wrong we will be happy to explain why, and if it is then you will save yourself and others a lot of confusion. Similarly, if you have any comments or suggestions for improving the teaching materials please let us know. 1.4 Health and safety using display screen equipment Although using a computer may not seem like a particularly risky activity you should be aware that you can suffer ill effects if you work at a computer for long periods without observing a few sensible precautions. The standard guidelines are as follows: Make sure that your equipment is properly adjusted: ensure that your lower back is well supported by adjusting the seat back height adjust your chair seat height so that your forearms are level when using the keyboard make sure that the front edge of the keyboard is at least 8-10 cm away from the edge of the desk if you are using a mouse, have it far enough away from the edge of the desk so that your wrist is supported whilst you use it. If you can learn to use the mouse with either hand then this can help avoid strains Do not have your screen positioned in such a way that there is glare or reflections from the windows or room lights on the screen. Maintain good posture. Take regular breaks away from the computer. It is recommended that you take about 10 minutes break every hour. Most Departments will have a Display Screen Trainer or Advisor, who can offer specific advice if you are using a display screen for a substantial amount of time, or if you experience, or anticipate, specific problems. "],
["expected-learning-outcomes.html", "Chapter 2 Expected learning outcomes 2.1 Statistical Concepts (I) 2.2 Statistical Concepts (II) 2.3 Simple parametric statistics (t-tests) 2.4 Categorical Data (chi-square tests) 2.5 Associations and Relationships (regression) 2.6 Associations and Relationships (correlation) 2.7 Experimental Design and ANOVA (I) 2.8 Experimental Design and ANOVA (II)", " Chapter 2 Expected learning outcomes 2.1 Statistical Concepts (I) Given a description of some data, classify variables as numeric vs categorical, ratio vs. interval, and ordinal vs. nominal. Construct simple summaries of numeric variables in R using the mean, sd, var functions. Explain what sampling error is (in non-technical terms) and understand why it is necessary to quantify sampling error alongside point estimates. Recognise the difference between the distribution of a sample, and the sampling distribution of an estimate derived from that sample. Recognise the difference between the standard deviation (a property of a sample) and the standard error (a property of a sampling distribution). Given a description of an experimental setting, recognise… …the difference between a statistical population and a sample from the population. …the difference between a population parameter and a point estimate of the population parameter. 2.2 Statistical Concepts (II) Understand what is meant by the term ‘null hypothesis’ and, given a scenario, state the appropriate null hypothesis for the associated statistical test. Identify whether or not a result is ‘statistically significance’ by examining the p-value it produces. Calculate the standard error of a sample mean when the population distribution of a variable follows a normal distribution. You are not expected to be able to explain or use the the bootstrap or permutation test—this were introduced to help you learn the principles listed above. 2.3 Simple parametric statistics (t-tests) Understand that a one-sample t-test is used to assess whether a population mean is different from a particular reference value (often 0). Understand that a two-sample t-test is used to assess whether two population means are different from one another. Understand that a paired-sample t-test is used to assess whether the mean difference among paired cases is different from reference value (usually 0). Given an experimental scenario and question, choose the correct t-test to use to answer the question. State the assumptions of the one-sample, two-sample, and paired-sample t-tests, and explain how you might check them for a given problem using R. Carry out a one-sample, two-sample or paired-sample t-test using the t.test function, and be able to interpret the output produced by t.test. Write an informative and concise summary of the results from a one-sample, two-sample, or paired-sample t-test. 2.4 Categorical Data (chi-square tests) Recognise situations where you are studying one or more categorical variables and you need to compare the frequencies of each category (or combinations of categories) in some way. Given an experimental scenario and question… …decide on the appropriate type of analysis to use: goodness-of-fit, or contingency table. …state the null hypothesis of the required test. State the assumptions of the goodness-of-fit and contingency table tests. Carry out a goodness-of-fit test using the chisq.test function. Use the xtabs function to convert a set of counts of different combinations of categories to a table of cross-tabulated counts (when necessary). Carry out a contingency table test of association using the chisq.test function. Write an informative and concise summary of the results from goodness-of-fit and contingency table tests. 2.5 Associations and Relationships (regression) Understand what simple linear regression analysis does, and when to use it. State the assumptions of the simple linear regression model. Fit a simple linear regression using the lm function and interpret the output from… …anova to determine the significance of fit …summary to extract the variance explained (\\(R^{2}\\)) and the fitted coefficients (intercept and slope) Use the plot function with a fitted model object to… …construct a residuals vs. fitted values plot and evaluate the linearity assumption. …construct a normal probability plot and evaluate the normality assumption. …construct a scale-location plot and evaluate the constant variance assumption. (You are not expected to be able to produce these plots manually using functions like resid and fitted) Calculate predicted \\(y\\)-values from a fitted regression. Write an informative and concise summary of the results from a simple linear regression analysis. Your ability to summarise a regression analysis graphically—showing the data and the fitted line—will not be assessed. 2.6 Associations and Relationships (correlation) Distinguish between situations in which correlation or regression is the most appropriate technique to use. State the assumptions of parametric (Pearson) and nonparametric (Spearman rank) correlation techniques and determine which approach is most approapriate for a given scenario. Carry out the two types of correlation (Pearson and Spearman rank) using the cor.test function and interpret the results. Write an informative and concise summary of the results from a correlation analysis. 2.7 Experimental Design and ANOVA (I) Understand what one-way ANOVA does and when to use it. State the assumptions of one-way ANOVA model. Fit a one-way ANOVA using the lm function and interpret the global test of significance produced by the anova function. Use the plot function with a fitted model object to… …construct a normal probability plot and evaluate the normality assumption. …construct a scale-location plot and evaluate the constant variance assumption. Determine whether it is appropriate to carry out a multiple comparison test. Where appropriate, carry out a Tukey multiple comparison test using the TukeyHSD or HSD.test functions and interpret the results. Write an informative and concise summary of the results from a one-way ANOVA analysis. Your ability to summarise a one-way ANOVA graphically—showing the means and standard errrors—will not be assessed. 2.8 Experimental Design and ANOVA (II) Understand what two-way ANOVA does and when to use it. Understand what is meant by the terms ‘main effect’ and ‘interaction’ and evaluate the likley presence or absence of these effects using an interaction plot. State the assumptions of two-way ANOVA model. Fit a two-way ANOVA using the lm function and interpret the three global tests of significance produced by the anova function. Use the plot function with a fitted model object to… …construct a normal probability plot and evaluate the normality assumption. …construct a scale-location plot and evaluate the constant variance assumption. Identify the main effect and interaction terms for which it is appropriate to carry out a multiple comparisons test. Where appropriate, carry out Tukey multiple comparison test(s) using the TukeyHSD or HSD.test functions, and interpret the results. Write an informative and concise summary of the results from a two-way ANOVA analysis. Your ability to summarise a two-way ANOVA graphically—showing the means and standard errrors—will not be assessed. "],
["programming-prerequisites.html", "Chapter 3 Programming prerequisites 3.1 Starting an R session in RStudio 3.2 Using packages 3.3 Reading data into R 3.4 Data frames 3.5 Package functions", " Chapter 3 Programming prerequisites This chapter gives a quick overview of the prerequisite R skills needed for this course (we studied these last year). We will use these skills this year, so you may need to spend revising them if you feel that you’re a little rusty. Biology students The key R skills you need to have in place to work through this book will be revised in the first practical session. The lecture slides will be placed on MOLE after the practical. You can also access a version without the answers to exercises here. Environmental Science students If you are an Environmental Science student joining us from Geography the material in this section won’t make any sense to you at the moment. Don’t panic! We will help you catch up in the first few weeks of the course, and because you will be working in a smaller group, you will be able to access more one-to-one help. 3.1 Starting an R session in RStudio Here is a quick overview of the process you should go through every time you start a new R session: Open up RStudio and set your working directory. You should do this via the RStudio menu system: Session ▶ Set Working Directory ▶ Choose Working Directory…. Make sure that you choose a sensible location. This is where you will store your data and R scripts, so it needs to be somewhere you can find and access again each time you use R. If you want to keep life really simple, it is a good idea to use the same location in every practical, but you don’t have to do this. Open up a new R script using the RStudio menu system: File ▶ New File ▶ R Script. Don’t create any other kind of file. There are a couple of things that need appear at the start of every script. Add these to the top of your new script before you do anything else. You should always clear the workspace with rm, and load up any packages you plan to use: # clear the workspace so that we have a &#39;clean sheet&#39; rm(list = ls()) # load and attach the packages we want to use... # 1. &#39;dplyr&#39; for data manipulation library(dplyr) # 2. &#39;ggplot2&#39; for plotting library(ggplot2) Now run the preamble section of the script, i.e. highlight everything and hit Ctrl+Enter. If the library commands didn’t work it suggests that you have not previously installed the relevant package. Install the package (see below) and try rerunning the script. Once the preamble bit of the script is working you should save the script. Look at the label of the tab the script lives in. This will probably be called something like Untitled1, and it the label will be red. This is RStudio telling you that you have not saved the file yet. You are now ready to start developing your new script. 3.2 Using packages R packages extend the basic functionality of R so that you can do more with it. A package bundles together R code, data, and documentation in a way that is easy to use and share with other users. Last year we learned how to use some of the functions provided by the dplyr package (for data manipulation) and the ggplot2 package (for making plots). We are going to use the dplyr and ggplot2 packages again this year, so you need to understand R’s package system in order to access these. You can revise how to use the package system in the packages topic. It isn’t difficult to use, and we will obviously help you if you run into difficulties. Installing a package is done via the install.packages function, e.g. install.packages(&quot;dplyr&quot;) Loading and attaching the package a package happens via the library function, e.g. library(&quot;dplyr&quot;) The key point—which seems to cause endless confusion—is that installing a package, and then loading and attaching the package, are different activities. You only have to install it once onto your computer, but you have to load a package every time you want to use it in a new R session (i.e. every time you start up RStudio). 3.3 Reading data into R Last year we made extensive use of several data sets that reside inside various R packages. This was useful because it meant we could use the data without first reading it into R, meaning that we could concentrate on developing your R skills rather than fixing data input errors. We don’t have the luxury of doing this when we work with our own data, and so this year, we will adopt more realistic practices. Whenever you need to work with a data set, you will have to first download it (from MOLE), and then read it into R. Each data set is stored as a Comma Separated Value (‘CSV’) text file, and so you will need to use the read.csv function to read it in. You can revise how all of this works in the relevant section of the data frames topic. 3.4 Data frames When you read data into R using a function like read.csv, it places that data into a data frame. The data frame is the most important type of object in R. It is table-like object that collects together different variables, storing each of them as a named column. We can access the data inside a data frame by referring to particular columns and rows. You can revise how to work with data frames in the data frames topic. 3.5 Package functions We will use functions from the dplyr package from time-to-time to manipulate data, and we will use the ggplot2 package to make plots of our data and summarise statistical models. However, we will remind you which functions you need to use to solve a particular problem as the course unfolds, so there is no need to revise all of this material now. "],
["the-scientific-process.html", "Chapter 4 The scientific process 4.1 Stages in the scientific process 4.2 Hypothesis testing 4.3 Don’t we ever know anything for sure? 4.4 Further reading", " Chapter 4 The scientific process There is something fascinating about science. One gets such a wholesale return of conjecture out of a trifling investment of fact. Mark Twain To do science is to search for patterns, not simply to accumulate facts. Robert MacArthur 4.1 Stages in the scientific process Science is about asking, and answering, the right questions. Within this process a number of distinct stages usually occur: making observations, asking questions, formulating hypotheses, and testing predictions. Collectively these are the building blocks of what is known as the scientific method. Exactly how they fit together, and what the philosophical and practical limitations of different approaches are, have been the subject of much debate by philosophers of science. We are not going to tackle those issues here, fascinating though they are, but instead try to extract a general working framework for the process of a typical scientific investigation. 4.1.1 Observations Observation — information, or impression, about events or objects. In general the questions we ask are not generated by pure abstract thought, but are a result of observations about the natural world. These may take the form of direct observations we make ourselves, patterns that crop up in data collected for other purposes, in non-specific surveys, and the previous work, or accumulated information, of other people. So, while pottering around in a stream one day, you notice that the freshwater ‘shrimps’ (Gammarus) that abound in the stream seem to occur almost entirely under stones; you rarely seem to see them when you just watch a patch of open stream bed. Having made observations, it may be necessary to collect some more data to check that this phenomenon is not just a one-off event, or a false impression. Look under a few more stones, watch the same species another day, or in another place, check the literature for similar observations by others. Such observations of biological systems will lead almost automatically into asking questions. 4.1.2 Questions Question — what it is that you want to know; the scope of your investigation. e.g., Why does Gammarus spend most of its time under stones? You should try and make your question reasonably focused - the overall aim of your study is to answer this question. The question of why the tropics are more diverse than the temperate regions is a vast topic - so even though it is a valid (and fascinating) question, it may not be a good choice for a final year project or even a PhD! The next stage is to formulate an hypothesis. 4.1.3 Hypotheses Hypothesis — an explanation proposed to account for observed facts. In general, in biology, the important distinguishing feature of an hypothesis is that it specifies some biological process, or processes, which might account for the observations made. One question will often generate more than one hypothesis: Gammarus occur under stones because: they need to shelter from the current their food (leaf litter) gets trapped and accumulates under stones they are subject to predation by visually hunting fish and need to remain out of sight Formulating hypotheses requires more than just a restatement of the question - it usually embodies some mechanism (though in some cases this may not be fully understood) and it will often draw on additional information (e.g., the fact that Gammarus feed on dead leaves). 4.1.4 Predictions Prediction — what you would expect to see if the hypothesis was true. Hypotheses are about proposing explanations, but they might not be directly testable; that is they may not tell you what data to collect, or what pattern to expect in the data. To be able to test an hypothesis you need to make some predictions from that hypothesis. These will be determined both by what you expect to see and what it is possible, or practical, to measure. A prediction is not simply a rephrasing of the hypothesis - it should more or less give you a statement of the experiment to conduct or observation to make, and type of data to collect: Shelter hypothesis: a greater proportion of Gammarus should be found in the open in streams with slow flow, or in slower flowing areas of a stream. Food hypothesis: Gammarus should not aggregate under stones from which all leaf litter has been removed; Gammarus should aggregate on patches of leaf litter tethered in the open part of the stream bed. Predation hypothesis: Gammarus should aggregate under stones more in streams where fish are present than where they are not; Gammarus may spend less time under stones at night. Ideally you are looking for a prediction that is unique to the hypothesis it is based on - so if the prediction is true only one of the hypotheses could have been responsible, but this may not always be possible and some combination of predictions may need to be used. Additionally, several processes may be operating at the same time. This makes hypothesis testing harder still. It may be necessary to consider two or more hypotheses, and their corresponding predictions, in combination. For example, Gammarus may be under stones because it prefers the sheltered environment, but also because food accumulates there. In this case we might expect that Gammarus will show a weak aggregative response to shelter alone, or food alone, and a stronger one to them both together. 4.2 Hypothesis testing Once we have firmed up our questions, hypotheses, and predictions we can collect the data to evaluate our ideas. On the basis of these data we will either accept or reject the various hypotheses. The important thing to realise about the process of hypothesis testing is that, in science especially, hypotheses are either rejected, or not rejected, but an hypothesis can rarely, except in trivial cases, be proved. This seems like an odd state of affairs! True, but it does make sense. Since you cannot be sure that you have thought of all the possible hypotheses to explain an observation, finding evidence that supports the prediction from one of your hypotheses, does not guarantee that the hypothesis is the only one which could have produced the effect you find. On the other hand, if you find evidence that directly contradicts the prediction(s) from your hypothesis, you can be certain (assuming the prediction and data are not flawed) that the hypothesis cannot be true. An hypothesis which predicted that all conifers should be evergreen could be supported by numerous observations of different conifer species in forests around the world, but is conclusively refuted by the first larch tree we encounter. Having tested your hypothesis, by examining the evidence that its predictions are true, you may accept it as the best (so far) explanation of the observations, or you may reject it as an explanation, and turn to other hypotheses. The same procedure must then be repeated for these hypotheses. This basic cycle of proposing hypotheses and then seeking evidence potentially capable of falsifying them, is, in essence, the idealized model of the scientific process famously proposed by the philosopher of science Karl Popper (1902-1994). It is often termed falsificationism. 4.3 Don’t we ever know anything for sure? The method presented here provides a view of science as one in which we suggest hypotheses, then test them, trying to reject them by finding conclusive counter-evidence, then replacing them with new hypotheses. It all sounds a bit frustrating. In fact of course we do ‘accept’ hypotheses all the time — that is we fail to reject them over and over again. These hypotheses become more accepted and in some sense become regarded as ‘true’ if repeated attempts to test them all fail to provide good counter evidence. In other words, we have some ideas that are doing pretty well in terms of resisting falsification, and we use these as our best estimates of the truth, with the proviso that it is still possible a better idea will come along in due course. The simple process of falsification described above also presents a picture of scientists as wonderfully neutral, objective creatures, rationally proceeding through cycles of setting up hypotheses, testing them, rejecting them, cheerfully setting them aside and starting over again. Of course this is not a true reflection of the complex, often messy, business really involved in trying to figure out how the world works. Philosophers of science have argued long and hard about how far from this idealized process real science actually is. Various alternative philosophies suggest more ‘realistic’ processes, such as Thomas Kuhn’s view of science as periods of relative stasis, where people work within an accepted paradigm (a set of views about how things work) despite accumulating evidence that doesn’t always support the paradigm, until finally it is upset by a ‘revolution’ which rejects the entire paradigm, and proposes a new view. The philosopher Imre Lakatos proposed some resolution of these views, suggesting that scientific ideas were grouped together in ‘research programmes’ concerned with particular endeavours, and that within these there may be core ideas that are not challenged, but other related ideas which are being challenged and adjusted by falsification, and that together these make each research programme progress. Programmes that don’t progress should be abandoned in favour of those that do. Of course that is a very over-simplified sketch of some important ideas, which are well worth reading a bit about, but in practice these philosophical arguments are really more focused on how whole areas of science develop. When you are just thinking about constructing a simple study of one problem, then the basic falsification cycle is a pretty good approach to have in your mind. Even in its simple form, however, it is not immune from the effect of human fallibility (see below). The process laid out here is not a strict set of rules, but outlines an approach to scientific investigation which is widely considered to provide a rigorous and productive system. As with all such systems understanding the ‘normal’ process is a prerequisite for constructively breaking the rules. It’s hard to reject an hypothesis you love! An interesting aside on the process is given by Sutherland (1994) who suggests that, in everyday life at least, we are often very reluctant to deliberately seek evidence that might refute a hypothesis we have formulated, and often persist in holding on to the hypothesis even when the evidence is against us. This can be seen in experiments where people are presented with a sequence of numbers (say 2, 4, 6) and have to try and establish the general rule to which the set of numbers conforms. The subject decides on an initial rule and then can test this rule by suggesting other sets of numbers and being told whether or not those numbers conform to the rule. The initial guess at a rule is usually ‘even numbers in ascending order’, and the initial test suggestions are typically another set of even numbers ascending by two (e.g. 16, 18, 20). These, the subjects are told, conform to the rule. The next set of numbers suggested is then often another similar set (40, 42, 44) – which again conforms, and sometimes another similar guess will follow. However, the suggestion of further sets of even numbers ascending by two cannot test this (most simple rules that allow 2, 4, 6 will allow the other sequences too). A better first suggestion would be to change one of the components of the initially proposed rule e.g. even numbers to odd numbers: 3, 5, 7; or the increment: 2, 8, 14. If these conform we can reject a specific component of our initial rule. (The rule is, in fact, simply any ascending sequence of numbers.) 4.4 Further reading Barnard C, Gilbert F and McGregor P (1993) Asking questions in biology. Longman. Ladyman, J (2002) Understanding the philosophy of science. Routledge. Sutherland S (1994) Irrationality. Penguin. "],
["data-and-variables.html", "Chapter 5 Data and variables 5.1 “Observations on material and obvious things” 5.2 Revision: Types of variable 5.3 Accuracy and precision", " Chapter 5 Data and variables The truth is the science of nature has been already too long made only a work of the Brain and the Fancy. It is now high time that it should return to the plainness and soundness of Observations on material and obvious things. Robert Hooke (1665) The plural of anecdote is not data. Roger Brinner 5.1 “Observations on material and obvious things” As Hooke’s observation suggests, science cannot proceed on theory alone. The information we gather about a system both stimulates questions and ideas about it and, in turn, can also allow us to test these ideas. In fact the idea of measuring and counting things is so familiar to us that it is easy to start a project without giving much thought to something as apparently mundane as the nature, quantity and resolution of the data we intend to collect. It is worth considering, however, as features of the data in a study determine both the types of analyses that can be carried out, and the confidence we can have in any conclusions that are drawn. We will spend quite a lot of time considering the statistical tools that can help us extract information from data, but no statistical wizardry can extract information that isn’t there to begin with. So what is there to say about data? The first point to note is that, properly, the word data is the plural of datum (a single, often numerical, piece of information) …so we should say “the data are…” not “the data is…”. However, the use of the word in the singular is becoming widespread, and you will commonly hear it used in this way. Grammar Nazis don’t like this though, so it’s worth knowing what the “correct” subject-verb agreement looks like if you want to avoid incurring their wrath. The second point is that there are many different sorts of data. Examples include spatial maps of the occurrence of a particular species and environmental variables, DNA sequences or even the whole genomes of individuals, and networks of feeding relationships among species (i.e. food webs). These kinds of data can be very challenging to analyse correctly. Fortunately for us, we are concerned with relatively simple kinds of data in this course. When we collect data it is typically organised as a set of one or more related statistical variables. Remember, what we learned last year. Statisticians use the word ‘variable’ as a generic term to refer to any characteristic that can be measured or experimentally controlled on different items or objects. We tend to think of variables as numeric quantities, but there is nothing to stop us working with non-numeric variables. Collectively, a set of related variables are referred to as a data set (or just ‘the data’). Confused? Let’s look at a concrete example. Consider the spatial map example above. A minimal data set might comprise two variables containing the x and y position of sample locations, a third variable denoting the presence / absence of a species, and one or more additional variables containing information about the environmental factors we measured. Data and variables in R Remember what you learned last year about data frames and vectors? When using R, we typically store a data set in a data frame. Each column in the data frame is one of R’s vectors — numeric, character, etc. Remember the ‘tidy data’ concept from last year? If the data are tidy then the columns of the data frame should correspond to the statistical variables in our data, and each row corresponds to a single observation. This simple connection between abstract statistical concepts and the concrete objects in R is not coincidence—R was designed first and foremost to analyse data. 5.2 Revision: Types of variable Again, because we handle data of one sort or another so frequently, we often don’t stop and think about exactly what kind of data we are using. Most of the time that doesn’t cause too much of a problem. However, when you come to design your own studies, and analyse your own data, it can be very important to understand what sort of data you need, or have, as it can affect what information you can extract from it. Last year we learned that the variables that comprise a data set can be classified as being either numeric or categorical: categorical variables have values that describe a characteristic of an observation, like ‘what type’ or ‘which category’; numeric variables have values that describe a measurable quantity as a number, like ‘how many’ or ‘how much’. Categorical variables can be further characterised according to whether or not they have a natural order (nominal vs. ordinal variables), and numeric variables can be further characterised according to the type of scale they are meaured on (interval vs. ratio scales). Let’s review these classifications. 5.2.1 Nominal (categorical) variables Nominal variables arise where observations are recorded as categories which have no natural ordering relative to each other. For example: Marital status Sex Colour morph Single Male Red Married Female Yellow Widowed Black Divorced Data of this type are common in surveys where, for example, a record is made of the species found at each site. 5.2.2 Ordinal (categorical) data Ordinal variables occur where observations can be assigned some meaningful order, but where the exact numerical relationship between items in the order are not necessarily fixed, the same, or even known. For example If you are studying the behaviour of an animal when it meets another individual it may not be possible to obtain quantitative data about these interactions, but you can score the behaviours you see in order of aggressiveness: Behaviour Score initiates attack 3 aggressive display 2 ignores 1 retreats 0 Rank orderings are also ordinal data. For example the order in which runners finish a race (1st, 2nd, 3rd, etc.) is a rank ordering—it doesn’t tell us whether it was a close finish or not, but still conveys important information about the result. In both situations you can say something about the relationships between categories: in the first example, the larger the score the more aggressive the response; in the second example the greater the rank the slower the runner. However, you can’t say that the gap between the first runner and the second was the same as between the second and third (even though 2-1=3-2) and you can’t say that a score of 2 is twice as aggressive as a score of 1. How should you code different categories? We always have to define some kind of coding scheme to represent the different categories of a nominal/ordinal variables. It was once common practise to assign numbers to different categories (e.g. Female=1, Male=2) for handling data in computerised form. This method was sensible in the early days of computer-based data analysis because it allowed data to be stored efficiently—numbers take up less space in memory than words. However, this efficiency argument is much less relevant on a modern computer with many GB of memory. There are good reasons to avoid numeric coding schemes though: Numeric coding makes it harder to understand your raw data and to interpret the output of a statistical analysis of those data, because you have to remember which number is associated with each category. This is particularly problematic when a variable has many categories. Numeric codes are arbitrary. This means, for example, they should not be treated as numbers for mathematical operations (it is meaningless to say 2 [“male”] is larger than 1 [“female”]). R has a special way of representing categorical variables (called ‘factors’), so it assumes that any variable containing numeric values is meant to be treated as a number. So here’s the warning: always use words (e.g., ‘female’ vs. ‘male’), not numbers, to describe the different categories when you are preparing your data for analysis in R. You are much more likely to make a silly mistake if you don’t do this, as R will try to treat the offending categorical variable as a number. 5.2.3 Interval scale (numeric) variables Interval scale varaibles take values on a consistent numerical scale but where that scale starts at an arbitrary point. Temperature on the Celsius scale is a good example of interval data. You can say that 60\\(^{\\circ}\\)C is hotter than 50\\(^{\\circ}\\)C. You can also say that the difference in temperature between 60\\(^{\\circ}\\)C and 70\\(^{\\circ}\\)C is the same as that between -20\\(^{\\circ}\\)C and -10\\(^{\\circ}\\)C. However you cannot say that 60\\(^{\\circ}\\)C is twice as hot as 30\\(^{\\circ}\\)C because temperature on the Celsius scale has an artificial zero value (the freezing point of water). This point becomes obvious when you consider that temperature can equally well be measured on the Fahrenheit scale (where the freezing point of water is 32 degrees). There is a temperature scale which has a true zero: the Kelvin scale. Zero K is absolute zero, where a substance actually has no thermal energy whatsoever. So temperature in degrees K would not be interval data. You can add and subtract data measured on an interval scale but you cannot divide or multiply such data (and get a meaningful result). 5.2.4 Ratio scale (numeric) variables Ratio scale variables have a true zero and known and consistent mathematical relationship between any points on the measurement scale. Temperature measurements in degrees K are on a ratio scale, i.e. it makes sense to say that 60 K is twice as hot as 30 K. These are the variables we are most used to, because physical quantities are often measured on a ratio scale. For example, length, weight, or numbers of organisms are usually measured on a ratio scale. You can add, subtract, multiply and divide this sort of data and get meaningful results. Continuous or discontinuous? A common confusion with numeric data concerns whether the data are on continuous or discontinuous scales. Ratio data can be either. Many biological ratio data are discrete (i.e. only certain discrete values are possible in the original data), and therefore discontinuous. Count data are an obvious example, e.g. the number of eggs found in a nest, the number of plants recorded in a quadrat, or number of heartbeats counted in a minute. These can only comprise whole numbers, ‘in between’ values are not possible. However, the distinction between continuous and discontinuous data is often not clear cut – even ‘continuous’ variables such as weight are made discontinuous in reality by the fact that our measuring apparatus is of limited resolution (i.e. a balance may weigh to the nearest 0.01 g). So… just keep in mind that the fact that data look (or really are) discontinuous does not mean they are necessarily ordinal data. 5.2.5 Which is best? All types of data can be useful but it is important to be aware that not all types can be used with all statistical models. This is one very good reason for why it is worth having an idea of the statistical tools you intend to use when designing your study. In general, ratio data is the data type best suited for statistical analysis. But biological systems often cannot be readily represented as ratio data, or the work involved in collecting good ratio data may be vastly greater than the resources allow, or the question we are interested in may not demand ratio data to achieve a perfectly satisfactory answer. It is this last question that should really come first when thinking about a study. What sort of data do we need to answer the question we are interested in? If it is clear at the outset that data on a rank scale will not be sufficiently detailed to enable us to answer the question then we must either develop a better way of collecting the data, or abandon that approach altogether. If you know the data you are able to collect cannot address the question, then you would be better doing something else, so it is good to work that out in advance. And an obvious, but important point: you can always convert measurements taken on a ratio scale to an interval scale, but you cannot do the reverse. Similarly, you can convert interval scale data to ordinal data, but you cannot do the reverse. In general, it is a good idea to avoid such conversions if you can, as they inevitably result in a loss of information. 5.3 Accuracy and precision 5.3.1 What do they mean? The two terms accuracy and precision are used more or less synonymously in everyday speech, but in scientific investigation they have quite distinct meanings. Accuracy – how close a measurement is to the true value of whatever it is you are trying to measure. Precision – how repeatable a measure is, irrespective of whether it is close to the actual value. If you are measuring an insect’s weight on an old and poorly maintained balance, which measures to the nearest 0.1 g, you might weigh the same insect several times and each time get a different weight — the balance is not very precise, though some of the measurements might happen be quite close to the real weight. By contrast you could be using a new electronic balance, weighing to the nearest 0.01g, but which has been incorrectly zeroed so that it is 0.2 g out from the true weight. Repeated weighing here might yield results that are identical, but all incorrect (i.e. not the true value) — the balance is precise, but the results are inaccurate. The analogy often used is with shooting at a target: Figure 5.1: Accuracy and precision It is obviously important to know how accurate and how precise your data are. The ideal is the situation in the top left target in the diagram, but in many circumstances high precision is not possible and it is usually preferable to make measurements of whose accuracy you can be reasonably confident (bottom left), than more precise measurements, whose accuracy may be suspect (top right). Taking an average of the values for the bottom left target would produce a value pretty close to the centre; taking an average for the top right target wouldn’t help your accuracy at all (though the repeatability of the values might well give you spurious confidence in the data). It is also worth being aware that when you state results, you are making implicit statements of the precision of the measurement. 5.3.2 Implied precision - significant figures The number of significant figures you use suggests something about the precision of the result. A result quoted as 12.375 mm implies the measurement is more precise than one quoted as 12.4 mm. A value of 12.4 actually measured with the same precision as 12.735 should properly be written 12.400. When quoting results look at the original data to decide how many significant figures to use - generally the same number of significant figures will be appropriate. If you are working with discrete data these considerations do not apply in quite the same way, e.g. precision of measurement is not an issue in recording the number of eggs in a nest. You use 4 not 4.0, but since 4 eggs implies 4.0 eggs you would be correct to quote average clutch size from several nests as 4.3 eggs. However, even with discrete data, if numbers are large then obviously precision is an issue again … a figure of 300 000 ants in a nest is likely to imply a precision of plus or minus 50 000. A figure of 320987 ants implies a rather improbably precise measurement (nobody will believe you actually counted them all!). 5.3.3 How precise should measurements be? The appropriate precision to use when making measurements is largely common sense. It will depend on practicality (it may not be possible to weigh an elephant to the nearest 0.001g) and the use to which you wish to put the data (if you want to know whether the elephant will cause a 10 tonne bridge to collapse then the nearest tonne will be good enough, if you want to compare the mean sizes of male and female elephants then the nearest 100 kg may be sufficient, if you want to monitor the progress of a pregnant female elephant then the nearest 10 kg or less might be desirable). As a rough guide aim, where possible, for a scale where the number of measurement steps is between 30 and 300. So for example, in a study of the variation in shell thickness of dogwhelks on a 300 m transect up a shore, it would be adequate to measure the position of each sampling point on the transect to the nearest metre, but shell thickness will almost certainly need to be measured to the nearest 0.1 mm. 5.3.4 Error, bias and prejudice Error is present in almost all biological data, but not all error is equally problematic. Usually the worst form of error is bias. Bias is a systematic lack of accuracy, i.e. the data are not just inaccurate, but all tend to deviate from the true measurements in the same direction (situations B and D in the ‘target’ analogy above). Thus there is an important distinction in statistics between the situation where the measurements differ from the true value at random and those where they differ systematically. Measurements lacking some precision, such as the situation illustrated in C, may still yield a reasonable estimate of the true value if the mean of a number of values is taken. Avoiding bias in the collection of data is one of the most important skills in designing biological (or other) investigations. Some forms of bias are obvious, others more subtle and hard to spot. Some sources of bias in biology include: Non-random sampling. Many sampling techniques are selective, and may result in biased information. For example pitfall trapping of arthropods will favour collection of the very active species, which encounter traps most frequently. Studying escape responses of an organism in the lab may be biased since the process of catching organisms to use in the study may have selected for those whose escape response is poorest. Conditioning of biological material. Organisms kept under particular conditions, especially in a laboratory, for periods of time may become acclimatised to conditions unlike those they normally encounter, or if kept in a laboratory for many generations their characteristics may change through natural selection. Such organisms may give a biased impression of the behaviour of the organism in natural conditions. Interference by the process of investigation. Often the process of making a measurement itself distorts the characteristic being measured. For example it may be hard to measure the level of adrenalin in the blood of a small mammal, without affecting the adrenalin level in the process. Pitfall traps are often filled with a preservative, such as ethanol, but the ethanol attracts species of insect that normally feed on decaying fruit and use the fermentation products as a cue to find resources. Investigator bias. Measurements can be strongly influenced by conscious or unconscious prejudice on the part of the investigator. We rarely undertake studies without some initial idea of what we are expecting, or we form ideas about the patterns we think we are seeing as the study progresses. This can introduce bias. For example, rounding up ‘in between’ values in the samples you are expecting to have large values and rounding down where a smaller value is expected, or having another ‘random’ throw of a quadrat when it doesn’t land in a ‘typical’ bit of habitat. The ways in which biases, conscious and unconscious, can affect our investigations are many, often subtle, and sometimes serious. Sutherland (1994) gives an illuminating and sometimes frightening catalogue of the ways in which biases affect our perception of the world and the judgements we make about it. The message is that the results you get from your investigation must always be judged and interpreted with respect to the nature of the data that were used to derive them – if the data are suspect, then the results will be suspect too. "],
["learning-from-data.html", "Chapter 6 Learning from data 6.1 Populations 6.2 Learning about populations 6.3 A simple example", " Chapter 6 Learning from data Statistics is the science of learning from data, and of measuring, controlling, and communicating uncertainty; and it thereby provides the navigation essential for controlling the course of scientific and societal advances Davidian and Louis (2012) The particular flavour of statistics we use in this course is called ‘frequentist statistics’. It isn’t hugely important that you remember that phrase, or indeed, from a practical perspective, that you even know you’re using frequentist statistics. You can apply the tools by just learning a few basic ‘rules’. Nonetheless, if you’re the kind of person who likes to understand things properly, it is useful to at least get a rough sense of how frequentist ideas work. The goal of this, and the next few chapters, is to provide such an overview. We’re going to avoid all of the challenging mathematics that underlies this, and try to focus instead on the important concepts. That doesn’t mean the ideas are simple. It’s not critical for all of this to make perfect sense. You certainly won’t be assessed on your ability to explain how frequentist statistics works. However, if you can wrap your head around the core ideas you will find it easier to understand the output from the various statistical tests we’ll learn later. We’re going to start, in this chapter, by laying out a somewhat simplified overview of the steps involved in ‘doing frequentist statistics’. We’ll also introduce a few key ideas and definitions along the way. Later chapters will drill down into the really important ideas—things like sampling variation, standard errors, null hypotheses and p-values. These are the concepts we really need to understand. 6.1 Populations When a biologist talks about a population they mean a group of individuals of the same species who interbreed. This definition, or at least something similar, should be familiar to you. What does a statistician mean when they talk about populations? The word has a different meaning in statistics. Indeed, it is a much more abstract concept: a statistical population is any group of items that share certain attributes or properties. This is best understood by example… The readers of this book could be viewed as a statistical population. APS students have a common interest in biology, they are mostly in their late teens and early 20s, and they tend to have similar educational backgrounds and career aspirations. As a consequence of these similarities, APS students tend to be more similar to one another than they would be to a randomly chosen inhabitant of the UK. The different areas of peatland in the UK comprise a statistical population. There are many peatland sites in the UK, and although their ecology varies somewhat from one location to the next, they are also very similar in many respects. For example, all peatland is generally characterised by low-growing vegetation (blanket bog, etc) and acidic soils. If you visit two different peatland sites in the UK, they will seem quite similar compared to, for example, a neighbouring calcareous grassland (think of the Peak District). A population of plants or animals—as understood by biologists—can also be thought of as a statistical population. Indeed, this is often the kind of population organismal biologists are most interested in. The individuals that comprise a biological population share common behaviours, physiology and life history characteristics. Much of organismal biology is concerned with learning about these properties of organisms, often with the goal to explaining the variation we see among individuals. Populations are conceptualised as fixed but unknown quantities within the framework of frequentist statistics. The goal of an analysis is to learn something about populations by collecting data. Note that ‘the population’ is defined by the investigator, and the ‘something we want to learn about’ is anything we’re interested in and know how to measure. Consider the examples again. A social scientist might be interested in understanding the political attitudes of undergraduates, so they might choose to survey a group of students in their university. A climate change scientist might measure the mass of carbon that is stored in peatland areas at sites across Scotland and northern England. A behavioural ecologist might want to understand how much time beavers spend foraging for food, so they might study one of the two Scottish populations. What are the steps involved in these kinds of studies? 6.2 Learning about populations The examples discussed above involve very different kinds of populations and questions. Nonetheless, there are fundamental commonalities in how these questions are addressed, which involve collecting data and applying the appropriate statistical tools. The process can be broken down into a number of distinct steps: Step 1: Refine your questions, hypotheses and predictions This step was discussed in The scientific process chapter so there’s no need to go over it again here. The key point to keep in mind is that we should not start collecting data until we’ve set out the relevant questions, hypotheses and predictions. This might seem blindingly obvious, but it is surprising how often people don’t get these things straight before diving into data collection. Take our word for it, collecting data without a clear scientific objective and rationale for the work is a guaranteed way to waste your time. Step 2: Decide which population(s) is (are) important The second step is to decide which population (or populations) we need to study. This is a more subtle problem than you might think. What constitutes ‘the population’ might be fairly obvious in some kinds of study, e.g. observational studies that don’t involve an experimental approach. In each of the three cases considered above, the corresponding populations we choose to study could be undergraduate students in APS, peatland habitats from across the UK, and beavers in Scotland, respectively. But what happens if we’re planning an experiment? Imagine we want to test the prediction that nutrient addition reduces biodiversity in chalk grasslands. We could set up an experiment where we have two kinds of plots: 1) manipulated plots where we add fertiliser, and 2) control plots where we do nothing. Comparing these would allow us to assess the impact of adding nutrients on biodiversity. There are two statistical populations in this setting—control and manipulated communities, which are defined by the experimental design we adopted. The nutrient addition plots don’t exist until you do the experiment, and even then, we want to be able to generalise our results beyond the one experiment. The weird mental contortion that a frequentist does is to imagine that the experimental plots are part of some larger, unobserved population of nutrient addition plots. Don’t worry too much if that is confusing (it is!). The important point is that, for any given problem, a relevant statistical population is something the investigator defines. It might be ‘real’, like the undergraduates in APS, or they might be something that doesn’t even exist in a meaningful way, like a population of not-yet-realised experimentally manipulated plots. In either case, we can use the same statistical techniques to learn about ‘the populations’. Step 3: Decide which variables to study The next step is to decide which features of the population we need to measure to address our question. In practise, this comes down to deciding which variable (or variables) we need to measure. In the examples above, the appropriate variables might be things like a standardised measure of political attitude, the mass of carbon stored per unit area, or the body mass of individuals in the biological population. This step is often reasonably straightforward, though some effort may be required to pick among different options. There isn’t a whole lot of ambiguity associated with a physical variable like body mass, but something like ‘political attitude’ needs careful thought. Can we quantify this by studying just one thing, like voting patterns? Probably not. Part of the art of designing a good data collection protocol is deciding what to measure. We discussed some of the considerations in the Data and variables chapter, but what really matters most is that we choose the right kind of variables to address the substantive research question. Step 4: Decide which population parameters are relevant Once we have decided which variable(s) to study, we have to decide which ‘population parameter’ is relevant. A population parameter is simply a numeric quantity that describes a particular aspect of the variable(s) in the population. Actually, to be more precise, it describes a feature of the distribution of the variable(s) in the population. A simple population parameter you are familiar with is the population mean. We often study means, because they allows us to answer questions like, “how much, on average, of something is present?”. Much of this course is about asking questions of population means, though other population parameters may also be important, e.g. The goal of statistical genetics is to partition variablity among individuals—we want to know how much phenotypic variation is due to genetic vs. non-genetic sources. In this case, it is population variances we want to learn about. Sometimes we want to understand how two or more aspects of the population are related to one another. In this situation a correlation coefficient (more about this later) might be the right population parameter to focus on. Step 5: Gather a representative sample If we could measure every object in a population we wouldn’t need to use statistics. We could just calculate the quantity we needed using an exhaustive sample and we’d have our answer. In the real world we are faced with resource constraints, i.e. we have limited time and money to invest in a problem, no matter how important it is. This means we have to work with a sample of a population. A sample is just a subset of the wider population, which has been chosen so that it is representative of that population. That word ‘representative’ is very important. If we can’t collect a representative sample it will be very difficult to infer anything useful about the population it came from. For example, if we aim to understand the reproductive characteristics of our favourite study organism, but we only sample young or old individuals, it will be impossible to generalise our findings if reproductive performance changes with age (which is almost always true). The study of how to generate useful samples from a population is an important part of statistics. It falls under the banners of experimental design and sampling theory. These are large, technical topics, so it is well beyond the scope of this course to study them in any great deal. We will touch on a few of the more important practical aspects as we move through this course, particularly in the Principles of experimental design chapter. Step 6: Estimate the population parameter(s) Once we have a representative sample from a population we can calculate something called a point estimate of the population parameter. Remember, the population parameter is unknown; that’s why we collect samples. A point estimate is simply a number that represents our “best guess” at the true value of the parameter. For example, if we are interested in a population mean of a variable, then the obvious point estimate to use is the mean of the sample (this is just “the average” you learned how to calculate in school). By the way, people often just say/write “estimate” instead of “point estimate”, for the simple reason that using “point estimate” all the time is tedious. The exact terminology isn’t really all that important to be honest. We’ll mostly use the word “estimate” from now on. Step 7: Quantify the uncertainty of estimate(s) A point estimate is virtually useless on its own. Why? Because it is always derived from a limited sample of the wider population. Even if we are very careful about how we sample a population, and we collect a really big sample, there is no way to guarantee that the composition of the sample exactly matches that of the population. Why is this important? It means that any point estimate we derive from a sample will always be imperfect, in the sense that it won’t exactly match the true population value. So… there is always uncertainty associated with an estimate of a population parameter. What can we do about this? We have to find a way to quantify that uncertainty. This bit of the process can be tricky to understand. We’re going to spend a fair bit of time thinking about it in the Sampling error chapter, so we’ll leave it there for now. Step 8: Answer the question! Once we have point estimates and measures of uncertainty we’re in a position to start answering questions. We have to be very careful about how we go about this though. Let’s say we want to answer a seemingly simple question, such as, “Are there more than 200 tonnes of carbon per hectare stored in the peatland of the Peak District?” We could go out and sample a number of sites, measure the stored carbon at each site, and then calculate the mean of these measurements. What can we conclude if that sample mean is 210 t h-1? Not much, at least not until we have a sense of how reliable that mean is likely to be. To answer our question, we have to know how to assess whether or not the difference we observe (210 - 200 = 10) was just a fluke. The tools we’ll learn about in this course are designed to answer a range of different kinds of scientific question. Nonetheless, they all boil down to the same basic question: Is the pattern I see ‘real’, or is it instead likely to be a result of chance variation? To tackle this, we combine point estimates and measures of uncertainty in various ways. The good news is that statistical software like R will do all the hard work for us. We just have to learn how to understand what is happening and interpret the results it gives us. 6.3 A simple example The best way to begin getting some sense of how all this fits together is by working through an example. We’ll finish this chapter by introducing an example that we’ll come back to in later chapters. We’ll just skim through steps 1-6 here. The final two steps are sufficiently tricky that they need their own chapters. Imagine we are working on a plant species that is phenotypically polymorphic. There are two different ‘morphs’, a purple morph and a green morph. We can depict this situation visually with a map showing where the purple and green plants are located on a hypothetical landscape: Figure 6.1: Stylised landscape showing a population of purple and green plants These idealised data were generated using a simulation in R. The details of how we did this aren’t important, but basically, we placed ‘individuals’ onto the landscape at random locations (every location is equally likely), and then assigned them purple morph status with a certain probability (we made them green otherwise). We’ll come back to the probability we actually used in the next chapter. Let’s proceed as though this were a real situation… Step 1: Refine your questions, hypotheses and predictions Imagine we had previously been studying a neighbouring population that exhibits the same polymorphism. We’re fairly sure both populations were once connected, but habitat loss over the last few hundred years has significantly reduced gene flow between them. Our studies with the neighbouring population have shown that… The colour polymorphism is controlled by a single gene with two alleles: a recessive mutant allele (‘P’) confers the purple colour, and the dominant wild-type allele (‘G’) makes plants green. Population genetic studies have shown that the two alleles are present in a ratio of about 1:1. There seems to be no observable fitness difference between the two morphs in the neighbouring population. What’s more, about 25% of plants are purple, i.e. the alleles seem to be in Hardy-Weinberg equilibrium. These two observations indicate that there is no selection operating on the polymorphism (it’s ‘neutral’). Things are different in the new study population. The purple morph seems to be about as common as the green morph. What’s more, some preliminary work indicates that purple plants seem to produce more seeds than green plants. Our hypothesis is, therefore, that purple plants have a selective advantage in the new study population. The corresponding prediction is that the frequency of the purple morph will be greater than 25% in the new study population, as selection should be driving the ‘P’ allele to fixation. (This isn’t the strongest test of our hypothesis, by the way. Really, we need to study allele and genotype frequencies, not just phenotypes. Sadly, since Brexit happened, the government has pulled the research funding for genetic research on plant polymorphism, so this is the best we can do.) Step 2: Decide which population is important Our situation is made up, so questions about the statistical population are not hugely relevant to be honest. In reality, we would consider various factors, such as whether we can study the whole population or need to restrict ourselves to a smaller scale (e.g. to one sub-population). Working at a large scale should produce a more general result, but it could also present a significant logistical challenge. Step 3: Decide which variables to study This step is easy in this example. We could measure all kinds of different attributes of our plants—biomass, height, seed production, etc—but to study the polymorphism, we only need to collect information about the colour of different individuals. This means we are going to be working with a nominal (i.e. categorical) variable, which takes two values: ‘purple’ or ‘green’. Step 4: Decide which population parameters are relevant The prediction we want to test is about the purple morph frequency (or equivalently, the percentage, or proportion, of purple plants). Therefore, the relevant population parameter is the frequency of purple morphs in the wider population. We need to collect ‘data’ so that we can learn about this unknown quantity. Step 5: Gather a representative sample A representative sample here is one in which every individual on the landscape has the same probability of being sampled (i.e. a ‘random sample’). Gathering a random sample of organisms from across a landscape is surprisingly hard to do in reality, but it is at least easy to do in a simulation. Let’s seen what happens if we sample 20 plants at random… Figure 6.2: Sampling plants. Sampled plants are circled in red The new plot shows the original population of plants, only this time we’ve circled the sampled individuals in red. Step 6: Estimate the population parameter Estimating a frequency from a sample is simple enough. We can express a frequency in different ways. Let’s use a percentage. We found 13 green plants and 7 purple plants in our sample, which means our point estimate of the purple morph frequency is 35%. This is certainly greater than 25%—the value of observed in the original population—but it isn’t that far off. Maybe the purple plants aren’t at a selective advantage after all? Or maybe they are? We’ll eventually see how to use a statistical test to rigorously evaluate our prediction. First we need to learn a few more concepts. Time to learn about something called sampling error… "],
["sampling-error.html", "Chapter 7 Sampling error 7.1 Sampling error 7.2 Sampling distributions 7.3 The effect of sample size 7.4 The standard error 7.5 What is the point of all this!?", " Chapter 7 Sampling error In the previous chapter we introduced the idea that a point estimate of a population parameter will be imperfect, in the sense that it won’t exactly reflect the true value of that parameter. This uncertainty is always present, so it’s not enough to have just estimated something. We have to know about the uncertainty (i.e. the precision) of the estimate. We use the machinery of statistics to quantify this uncertainty. Once we have pinned down the uncertainty we can start to provide meaningful answers to our scientific questions. We will arrive at this ‘getting to the answer step’ in the next chapter. First we have to develop the uncertainty idea a bit more. We need to learn about sampling error, sampling distributions and standard errors. 7.1 Sampling error Let’s carry on with the plant polymorphism example from the previous chapter: the green-purple plant polymorphism. Skim back over the example if you can’t remember it, as you need to know what we’re trying to do for this chapter to be useful. So far, we had taken one sample of 20 plants from our hypothetical population and found that the frequency of purple plants in that sample was 35%. This is a point estimate of purple plant frequency based on a random sample of 20 plants. What happens if we repeat the same process, leading to a new, completely independent sample? Here’s a reminder of what the population looked like, along with a new sample highlighted with red circles: Figure 7.1: Plants sampled on the second occasion This time we ended up sampling 16 green plants and 4 purple plants, so our second estimate of the purple morph frequency is 20%. This is quite different from the first estimate. Notice that it is actually lower than that seen in the original study population. Our hypothesis that the purple morph will be more prevalent in the new study population is beginning to look a little shaky… Note that nothing about the study population changed between the first and second sample. What’s more, we used a completely reliable sampling scheme to generate these samples (you’ll have to trust us on that one). There was nothing biased or ‘incorrect’ about the way individuals were sampled—every individual had the same chance of being selected. The two different estimates of the purple morph frequency simply arise from chance variation in selection. This variation, which arises whenever we observe a sample instead of the whole population, has a special name. It is called the sampling error. (Another name for sampling error is ‘sampling variation’. Which one is better? Neither really. We tend to use both terms—‘sampling error’ and ‘sampling variation’—in this book because they are both widely used.) Sampling error is the main reason why we have to use statistics. Any estimate you derive from a sample is affected by it. Sampling error is not really a property of any particular sample though. The form of sampling error in any given problem is a consequence of the population distribution of the variable(s) we’re studying, and the sampling method used to investigate this. That may seem a little cryptic now. Don’t worry, we will start to get a sense of what it really means in this chapter. 7.2 Sampling distributions We can develop our simple simulation example a bit more to explore the consequences of sampling error. However, rather than taking one sample at a time, we’ll use R to simulate thousands of independent samples. The number of plants sampled (‘n’) will always be 20. Here’s the important bit: every sample is drawn from the same population, i.e. the population parameter (purple morph frequency) will never change across samples. This means any variation we observe will be due to nothing more than sampling error. Here is a summary of one such repeated simulation exercise: Figure 7.2: Distribution of number of purple morphs sampled (n = 20) This bar plot summarises the result from 100000 samples. In each sample, we took 20 individuals from our hypothetical population and calculated the number of purple morphs found. The bar plot shows the number of times we found 0, 1, 2, 3, … purple individuals, all the way up to the maximum possible (20). We could have converted these numbers to frequencies, but instead we’re just summarising the raw distribution of purple morph counts that we found. This distribution has a special name. It is called a sampling distribution. The sampling distribution is just the distribution we expect a particular estimate (or more generally, a ‘statistic’) to follow. In order to to work this out, we have to postulate values for the population parameters, and we have to know how the population was sampled. Rather than use mathematical reasoning, we used brute-force simulation to approximate the sampling distribution of purple morph counts that arises when we sample 20 individuals from our hypothetical population. What does the sampling distribution show? It shows us the range of outcomes we can expect when we repeat the same sampling process over and over again. The most common outcome is 8 purple morphs, which would yield an estimate of 8/20 = 40% for the purple morph frequency. This is the frequency that was actually used to simulate the data (we didn’t tell you that before). The population parameter we’re trying to learn about turns out to be the most common point estimate we should expect to see under repeated sampling. (So now we know the answer to our question. The purple morph frequency is 40%. Of course we cheated though, because we used information from 1000s of samples. In the real world we only have one, limited sample.) The sampling distribution is the key to ‘doing statistics’. Look at the spread (dispersion) of the sampling distribution above. The range of outcomes is roughly 2 to 15, which corresponds to estimated frequencies of the purple morph in the range of 10-75%, because we sampled 20 individuals on each occasion. This tells us that when we sample only 20 individuals, the sampling error is expected to be quite large. Note that the sampling distribution we summarised above is only relevant for the case where 20 individuals are sampled, and the frequency of purple plants in the population is 40%. If we change either of those two things we would end up with a different sampling distribution. That’s what we meant when we said, “The form of sampling error in any given problem is a consequence of the population distribution of the variable(s) we’re studying, and the sampling method used to investigate this.” Once we know how to calculate the sampling distribution for a particular problem, we can start to make statements about sampling error (to quantify uncertainty), and we can begin to make meaningful comparisons to address scientific questions. We don’t have to work any of this out for ourselves — statisticians have done the hard work for us. 7.3 The effect of sample size One of the most important aspect of a sampling scheme is the sample size (denoted ‘n’). This is just the number of observations (individuals, objects, items, etc) in a sample. What happens when we change the sample size? We’ll carry on with the example to see how sample size influences the sampling distribution, and to understand why it matters. Let’s repeat the multiple sampling exercise, but this time do it with two different sample sizes. First we’ll use a sample size of 40 individuals, and then we’ll take a sample of 80 individuals each time. As before, we’ll take a total 100000 samples each time: Figure 7.3: Distribution of number of purple morphs sampled (n = 40) Figure 7.4: Distribution of number of purple morphs sampled (n = 80) What do these plots tell us about the effect of changing sample size? Notice that we plotted each of them over the full range of possible outcomes (the x axis runs from 0-40 and 0-80, respectively, in the first and second plot). This is so we can meaningfully compare the spread of each sampling distribution relative to the range of possible outcomes. The range of outcomes in the first plot (n = 40) is roughly 6 to 26, which corresponds to estimated frequencies of the purple morph in the range of 15-65%. The range of outcomes in the second plot (n = 80) is roughly 16 to 48, which corresponds to estimated frequencies in the range of 20-60%. The implications of this not so rigorous assessment are probably obvious. When we increase the sample size we can expect to encounter less sampling error. This makes intuitive sense: the composition of a large sample should more closely approximate that of the true population than a small sample. How much data do we need to collect to accurately estimate a frequency? Here is the approximate sampling distribution of the purple morph frequency estimate when we sample 500 individuals: Figure 7.5: Distribution of number of purple morphs sampled (n = 500) Now the range of outcomes is about 160 to 240, corresponding to purple morph frequencies in the 32-48% range. This is a big improvement over the smaller samples that we just considered, but even with 500 individuals in a sample, we should still expect quite a lot of uncertainty in our estimate. The take home message is that you need a lot of data to reduce sampling error. 7.4 The standard error We’ve been fairly relaxed about how we quantified the spread of a sampling distribution up until this point. We just estimated the approximate range of purple morph counts “by eye”. This is fine for investigating general patterns, but to make rigorous comparisons, we really need a quantitative measure of this variability. This is called the standard error. The standard error is actually quite a simple idea, though its definition often causes confusion. Here is that definition: a standard error is the standard deviation of the sampling distribution of an estimate, like a mean or a frequency. Don’t worry if that makes absolutely no sense. The key point is that it is a standard deviation, so it a measure of the spread, or dispersion, of a distribution. The distribution in the case of a standard error is the sampling distribution of some kind of estimate. (It is common to use a shorthand abbreviations such “SE”, “S.E.”, “se” or “s.e.” in place of ‘standard error’ when referring to the standard error in text.) We can use a simulation in R to calculate the expected standard error of an estimate of purple morph frequency. In order to do this we have to specify the value of the population frequency, and we have to decide what sample size we want to evaluate. Let’s find the expected standard error when the purple morph frequency is 40% and the sample size is 80. First we set up the simulation by assigning values to different variables to control what the simulation does: purple_prob &lt;- 0.4 sample_size &lt;- 80 n_samples &lt;- 100000 The value of purple_prob is the probability a plant will be purple (0.4 — R doesn’t like percentages), the value of sample_size is the sample size for each sample, and the value of n_samples is the number of independent samples we’ll take. That’s simple enough. raw_samples &lt;- rbinom(n = n_samples, size = sample_size, prob = purple_prob) percent_samples &lt;- 100 * raw_samples / sample_size You don’t have to understand how this works, but if you did A-level statistics you might be able to guess what the rbinom function is doing. Honestly though, the R code isn’t important here. We’re just showing it to you to demonstrate that seemingly complex simulations are often easy to do in R. It is more or less the same code we used to generate those plots above (the only difference is that this time we converted the numbers into proportions). The result is what matters. We simulated the percentage of purple morph individuals found in 100000 samples of 20 individuals, assuming the purple morph frequency is always 40%. The results are stored the result in a vector called percent_samples. Here are the first 50 values of that vector: head(percent_samples, 50) ## [1] 42.50 37.50 36.25 30.00 35.00 35.00 51.25 35.00 41.25 36.25 32.50 ## [12] 37.50 32.50 38.75 37.50 36.25 33.75 35.00 42.50 41.25 46.25 35.00 ## [23] 42.50 45.00 43.75 40.00 41.25 52.50 43.75 38.75 38.75 47.50 42.50 ## [34] 40.00 43.75 42.50 36.25 42.50 43.75 43.75 35.00 42.50 35.00 37.50 ## [45] 41.25 40.00 45.00 40.00 41.25 38.75 These numbers are all part of the sampling distribution of morph frequency estimates. So… how to calculate the standard error? This is the standard deviation of these numbers, so we just use the sd function: sd(percent_samples) ## [1] 5.494687 Why is this useful? The standard error gives us a standard means to compare the variability we expect to see, or the variability we actually see, in different sampling distributions. As long as the sampling distribution is ‘well-behaved’, then, roughly speaking, most estimates (~95%) can be expected to lie in a range of about four standard errors. If you’re not convinced, look at the second bar plot we produced above (where the sample size = 80, and the purple morph frequency = 40%). What is the approximate range of simulated values? How close is this to \\(4 \\times 5.5\\)? Pretty close we think… So in summary, the standard error gives us a way to quantify how much variability we expect to see in a sampling distribution. We said in the previous chapter (Learning from data) that a point estimate is useless without some kind of associated measure of uncertainty. A standard error is one such measure. 7.5 What is the point of all this!? By this point you might (quite reasonably) be wondering why we have spent so much time looking at properties of repeated samples from a population. After all, when we collect data in the real world we’ll only have a single sample to work with. We can’t just keep collecting more and more data. We also won’t know anything about the population parameter of interest. This lack of knowledge is the reason for collecting the data in the first place! The short answer to this question is that before we can start to use frequentist statistics—remember, that’s our ultimate goal—we need to have a sense of… how point estimates behave under repeated sampling (i.e. sampling distributions), and how ‘sampling error’ and ‘standard error’ relate to sampling distributions. Once we understand these links, we’re able to start exploring the techniques that underlie frequentist statistics. That’s what we’ll do in the next block of work… "],
["statistical-significance-and-p-values.html", "Chapter 8 Statistical significance and p-values 8.1 Estimating a sampling distribution 8.2 Statistical significance 8.3 Concluding remarks", " Chapter 8 Statistical significance and p-values We have already pointed out that we use frequentist statistics in this book. While it isn’t possible to give a precise description of how frequentist statistics works in an introductory book, we can at least provide a rough indication. Frequentist statistics works by asking what would have happened if we were to repeat an experiment or data collection exercise many times, assuming that the relevant population parameters remain the same each time. That was the basic idea we employed to generate those sampling distributions in the last chapter. The choice of population parameter(s) to work with depend on what kind of question we are asking. This obviously varies from one situation to another. The thing that is common to every frequentist technique is that we ultimately have to work out how a sampling distribution of some kind should look. If we can do that, then we can ask, for a given scenario, how likely or unlikely a particular result is. This naturally leads onto two of the most important ideas in this book: statistical significance and p-values. The goal of this chapter is to introduce these ideas. 8.1 Estimating a sampling distribution Let’s carry on with the plant polymorphism example (yes, again). Our ultimate goal is to evaluate whether the purple morph frequency is greater than 25% in the new study population. The suggestion in the preamble of this chapter is that, to get to this point, we need to work out what the sampling distribution of the purple morph frequency estimate looks like. At first glance this seems like an impossible task. We can’t use simulations, because we don’t know the true frequency of purple morphs in the population. All we have is the one sample. The solution to this problem is surprisingly simple (or at least the basic idea is simple): since we don’t know much about the population, we use the sample to approximate some aspect(s) of it, and then work out what the sampling distribution of our estimate should look like using this approximation. Let’s unpack this idea a bit more, and then try it out for real. 8.1.1 Overview of bootstrapping There are many different ways to approximate a population from a sample. One of the simplest methods—for easy problems at least—is to pretend the sample is the true population. Then, all we have to do to get at a sampling distribution is draw new samples from this pretend population. That may sound a lot like cheating, but it turns out that this is a perfectly valid way to construct useful sampling distributions for many kinds of problems. Here is how it works for our example, using a physical analogy. Imagine that we only have one sample, and have written down the colour of each sampled plant on a different piece of paper, and then placed all of these bits of paper into in a hat. We then do the following: Pick a piece of paper at random, record its value (purple or green), put the paper back into the hat, and shake the hat about to mix up the bits of paper. (The shaking here is meant to ensure that each piece of paper has an equal chance of being picked, i.e. we’re taking a random sample. This might not work in reality, but let’s assume it does.) Pick another piece of paper (you might get the same one), record its value, and then put that back into the hat, remembering to shake everything up. Repeat this process until we have a recorded new sample of colours which is the same size as your real sample. Now we have a ‘new sample’. (This process is called ‘sampling with replacement’. Each artificial sample is called a ‘bootstrapped sample’.) For each bootstrapped sample, calculate whatever quantity is of interest. In our example, this is the proportion of purple plants sampled. Repeat steps 1-4 until we have generated a large number of bootstrapped samples. About 10000 is often sufficient for many problems. Although it may seem like cheating (it’s not!), this process really does produce an approximation of the sampling distribution of the quantity we’re interest in. It is called bootstrapping (or ‘the bootstrap’). The bootstrap was invented by a very smart statistician called Bradley Efron. We’re introducing it here because it allows you to see how frequentist methodology works without having to do any nasty mathematics. We’re not expecting you to learn it, so don’t worry too much if you find it tricky to understand. It is an odd concept. 8.1.2 Doing it for real Let’s assume we’ve sampled 250 individuals from our new plant population. We’re going to use this hypothetical sample to implement the bootstrap in R. The best way to understand what follows is to actually work through the example. You’re strongly encouraged to do this… A data set representing this situation is stored in a Comma Separated Value (CSV) text file called ‘MORPH_DATA.CSV’. Download the file from MOLE and place it in your working directory (you should set this at the beginning of the R session). Next, run through the following steps: Read the data into an R data frame using read.csv, assigning the data frame the name morph.data. Use a function like glimpse (from dplyr) or str (from base R) to inspect the structure of morph.data. How many variables are in the data set? What are their names? What kind of variables are they? Use the View function to inspect the data. Is this what you expected? Are the values of the different variables as you would expect them to be? The point of all this is to ‘sanity check’ the data, i.e. to make sure we understand the data and that there are no obvious problems with it. We should always check our data after we’ve read it in. There is really no point messing about with the likes of dplyr and ggplot2, or carrying out a statistical analysis, until we have done this. If we don’t understand how our data is organised, and what variables we are working with, there is a very real risk that we will make a lot of avoidable mistakes. What you should have found is that morph.data contains 250 rows and two columns/variables: Colour and Weight. Colour is a categorical variable (a ‘factor’, in R-land) and Weight is a numeric variable. The Colour variable obviously contains the colour of each plant in the sample. What about Weight? We don’t need this now, but we’ll use it in the next chapter. Now that we understand the data, we’re ready to implement bootstrapping (using R of course, no hats or paper required). We’re going to introduce a few new R tricks here. We’ll explain them as we go, but if you’re not a huge R fan, there’s really no need to remember them. Focus on the logic of what we’re doing. We want to construct a sampling distribution for the frequency of purple morphs, so the variable that matters here is Colour. Rather than work with this inside the data frame, we’re going to pull it out using the $ operator, assign it a name (plant_morphs), and then take a look at the first 20 values: plant_morphs &lt;- morph.data$Colour levels(plant_morphs) ## [1] &quot;Green&quot; &quot;Purple&quot; head(plant_morphs, 100) # just show the first 100 values ## [1] Green Green Green Purple Green Green Green Green Green Green ## [11] Green Green Green Purple Green Green Purple Purple Green Green ## [21] Green Green Green Purple Green Green Green Green Purple Purple ## [31] Green Green Green Purple Purple Green Green Green Green Purple ## [41] Green Purple Green Green Purple Purple Green Green Green Green ## [51] Green Green Purple Purple Purple Green Green Green Purple Green ## [61] Purple Green Purple Green Purple Purple Green Green Purple Green ## [71] Green Purple Purple Green Purple Green Green Green Green Purple ## [81] Purple Green Purple Green Green Green Purple Purple Green Purple ## [91] Green Green Green Green Green Green Purple Green Green Green ## Levels: Green Purple Hopefully you followed that—plant_morphs is just a simple vector (a factor, with 2 categories) containing the colour information. Let’s calculate and store the sample size (samp_size), and the point estimate of purple morph frequency (mean_point_est) from this sample: samp_size &lt;- length(plant_morphs) samp_size ## [1] 250 mean_point_est &lt;- 100 * sum(plant_morphs == &quot;Purple&quot;) / samp_size mean_point_est ## [1] 30.8 So… 30.8% of plants were purple among our sample of 250 plants. We are now ready to start bootstrapping. We’ll construct 10000 bootstrap samples, and for convenience, we’ll store this number in n_samp: n_samp &lt;- 10000 We need to resample the plant_morphs vector. The sample function will do this for us (replace = TRUE makes it sample with replacement): samp &lt;- sample(plant_morphs, replace = TRUE) head(samp, 100) # just show the first 100 values ## [1] Purple Green Green Green Purple Green Green Green Purple Purple ## [11] Green Purple Green Green Green Green Purple Green Green Green ## [21] Green Green Green Purple Green Purple Green Green Green Green ## [31] Green Purple Purple Purple Green Green Green Green Green Green ## [41] Green Green Purple Green Green Green Purple Green Green Green ## [51] Green Green Green Green Green Green Purple Green Green Purple ## [61] Purple Green Green Green Green Purple Purple Green Green Purple ## [71] Green Green Green Green Purple Green Green Purple Green Purple ## [81] Green Purple Green Green Green Green Green Green Green Green ## [91] Green Purple Green Green Green Green Green Green Purple Green ## Levels: Green Purple Compare this one bootstrapped sample to the real one. It’s a random sample of the values in the first sample, as expected. We only need one number from this sample, which is the frequency of purple morphs: first_bs_freq &lt;- 100 * sum(samp == &quot;Purple&quot;) / samp_size head(first_bs_freq, 100) # just show the first 100 values ## [1] 27.2 That’s one bootstrapped value of purple morph frequency. Simple! We need \\(10^{4}\\) values though, and we really don’t want to have to keep doing this over an over ‘by hand’ (making second_bs_freq, third_bs_freq, and so on). That would be very dull… Fortunately, computers are very good at carrying out repetitive tasks like this. Here is some R code that repeats what we just did n_samp times and stores the bootstrapped sample in a vector called boot_out: boot_out &lt;- replicate(n_samp, { samp &lt;- sample(plant_morphs, replace = TRUE) 100 * sum(samp == &quot;Purple&quot;) / samp_size }) The replicate function does exactly what you might think it does. It replicates an R expression many times (n_samp in this case) and returns the set of results. Remember, you don’t have to understand this R code, but do ask a TA if you want to know more about it. The end result of the above is that boot_out now contains a bootstrapped sample of frequency estimates. Let’s take a quick look at the first 25 values, rounding each of them to 2 decimal places (we’ll used the pipe %&gt;% to remind you it exists; just keep in mind this won’t work unless you have loaded the dplyr package): head(boot_out, 25) %&gt;% round(1) ## [1] 26.0 28.0 32.0 26.8 32.8 35.6 31.2 32.4 33.6 30.0 27.6 29.6 30.8 34.0 ## [15] 32.4 38.8 31.6 24.4 31.6 28.4 30.0 32.8 32.4 33.6 32.0 These numbers represent different values of the purple morph frequency that we might expect to generate if we repeated the data collection exercise, assuming the observed purple morph frequency really is equal to that of the actual sample. This is a bootstrapped sampling distribution. We can use this bootstrapped sampling distribution in a number of useful ways. Let’s plot it first get a sense of what it looks like. A histogram is a good choice here because we have a reasonably large number of cases: Figure 8.1: Bootstrapped sampling distribution of purple morph frequency The mean of the sampling distribution looks to be round about 31%, which is fairly close to the sample mean. We can of course calculate this using R: mean(boot_out) %&gt;% round(1) ## [1] 30.8 This is essentially the same as the sample estimate we’re working with. This is guaranteed to be the case when we construct a large enough sample, because we’re just resampling the data used to estimate the purple morph frequency. A more useful quantity is the bootstrapped standard error (SE). This is the standard deviation of the sampling distribution, so all we have to do is apply the sd function to the bootstrapped sample: sd(boot_out) %&gt;% round(1) ## [1] 2.9 The standard error is a useful quantity in its own right. Remember, the standard errror is a measure of the precision of an estimate (e.g. a large SE would imply that our sample size was too small to reliably estimate the population mean). It is standard practise to summarise the precision of a point estimate by reporting its standard error. Whenever we report a point estimate, we should also report the standard error, like this… The frequency of purple morph plants (n = 250) was 30.8% (s.e. ± 2.9). Notice that we also report the sample size. 8.2 Statistical significance Now back to the question that motivated all the work in the last few chapters. Is the purple morph frequency greater than 25% in the new study population? We can never answer a question like this definitively from a sample. Instead, we have to carry out some kind of probabilistic assessment. To make this assessment, we’re going to do something that looks rather odd (frequentist statistics is odd, to be honest). Don’t panic… The ideas in this next section are really quite abstract and can be difficult to understand. You aren’t expected to understand them straight away, and you certainly won’t be asked to explain them in an assessment. We’re going to make two important assumptions: Assume that the true value of the purple morph frequency in our new study population is 25%, i.e. we’ll assume the population parameter of interest is the same as that of the original population that motivated this work. To put it another way, we’re assuming there is really no difference between the populations. Assume that the sampling distribution we just generated (via bootstrapping) would have been the same if the null hypothesis were true, but for the fact that the mean would be different (it would be equal to 25%). That is, the expected ‘shape’ of the sampling distribution doesn’t change under the null hypothesis. That first assumption is an example of something called a null hypothesis. It is called this because it is an hypothesis of ‘no effect’ or ‘no difference’. The second assumption is necessary for the reasoning below to be valid. It is often a reasonable assumption in many situations (you’ll have to trust us on this one). Now we ask, if the purple morph frequency in the population is really 25%, what would the corresponding sampling distribution look like? This is called the null distribution because it’s a distribution expected under the null hypothesis. We calculate a bootstrapped sample from this null distribution, using the second assumption, as follows: null_dist &lt;- boot_out - mean(boot_out) + 25 All we actually did here was shift the bootstrapped sampling distribution left until the mean is at 25%. Here’s what this null distribution looks: Figure 8.2: Sampling distribution of purple morph frequency under the null hypothesis What does this figure tell us? The red line shows where the point estimate from the true sample lies. It looks like the observed purple morph frequency would be quite unlikely to have arisen through sampling variation if the population frequency is 25%. We can say this because the observed frequency (red line) lies at the end of one ‘tail’ of the sampling distribution. We need to be able to make a more precise statement than this though. We need to quantify how often the values of the bootstrapped null distribution are greater than the value we estimated from the sample. This is easy to do in R: p_value &lt;- sum(null_dist &gt; mean_point_est) / n_samp p_value ## [1] 0.0265 This number (generally denoted ‘p’) is called a p-value. We’re going to calculate a lot of p-values in this book. Actually, R will do it for us. We just have to understand what they mean. What are we supposed to do with the finding p = 0.0265? This is the probability of obtaining a result equal to, or ‘more extreme’ than, that which was actually observed, assuming that the hypothesis under consideration (the null hypothesis) is true. The null hypothesis is one of no effect (or no difference), and so a low p-value can be interpreted as evidence for an effect being present. It’s worth reading that a few times… In our example, it appears that the purple morph frequency we observed is fairly unlikely to occur if its frequency in the new population really is 25%. In biological terms, we take the low p-value as evidence for a difference in purple morph frequency among the populations. Specifically, it looks like there is support for the prediction that the purple morph is present at a frequency greater than 25% in the new study population. One question remains: How small does a p-value have to be before we are happy to conclude that the effect is probably present? In practise, we do this by applying a threshold, called a significance level. If the p-value is less than the chosen significance level we say the result is said to be statistically significant. Most often (in biology at least), we use a significance level of p &lt; 0.05 (5%). Why? The short answer is that this is just a convention. Nothing more. Really, there is nothing special about this 5% threshold other than the fact that it’s the one most often used. What we just did was to carry out something called a significance test. It took quite a lot of convoluted reasoning to get there—we meant it when we said frequentist statistics is odd. Nonetheless, that rather non-intuitive chain of reasoning, or at least something similar, underlies all the statistical techniques we use in this course. The good news is that we don’t have to understand the low-level details to use these tools effectively. We just need to be able to identify the null hypothesis being used in a particular significance test and understand how to interpret the associated p-values. These ideas are so important that we’ll discuss null hypotheses and p-values a little more in the next two chapters. 8.3 Concluding remarks The bootstrap is a very powerful tool in the right hands, but it is an advanced technique that can be difficult to apply in complex settings (e.g. analysis of experiments). We’ll not apply it routinely in this course. Indeed, you are not expected to be able to use it at all. We used the technique to help us understand how frequentist ideas are used to decide if an effect (i.e. a difference) is really there or not. The details vary from one problem to the next, but ultimately, if we are using frequentist ideas we always have to find a way to do the following… assume that there is actually no ‘effect’ (the null hypothesis), where an effect is expressed in terms of one or more population parameters, construct the corresponding null distribution of the estimated parameter by working out what would happen if we were to take frequent samples in the ‘no effect’ situation, (Do you see where the word ‘frequentist’ comes from now?) and then compare the estimated population parameter to the null distribution to arrive at a p-value, which evaluates how frequently the data would be observed under the hypothesis of no effect. "],
["comparing-populations.html", "Chapter 9 Comparing populations 9.1 Making comparisons 9.2 A new example 9.3 Evaluating differences between population means 9.4 A permutation test 9.5 What have we learned?", " Chapter 9 Comparing populations 9.1 Making comparisons Scientific inquiry often requires that we evaluate predictions about ‘natural’ or experimentally induced differences between populations. In its simplest form this involves just a pair of populations, e.g. ‘Do male and female locusts differ in length?’ ‘Do maize plants photosynthesise at different rates at 25°C and 20°C?’ ‘Do eagle owls feed on rats of different sizes during winter and summer?’ ‘Do purple and green plants differ in their biomass?’ What we’re evaluating in this setting is whether or not underlying populations are different in some way. In the previous chapter we wanted to know whether the purple morph frequency was different from 25%. In one sense, we were comparing two populations in this study, because the 25% number arose from observations of a neighbouring population. However, that number was also an estimate (though we never actually said how it was arrived at), which must carry with it some uncertainty. In truth, we should have used a methodology that accounts for this uncertainty to arrive at a more reliable comparison. In order to do this, we have to step through the same kind of process discussed in the last few chapters. This chapter demonstrates how to compare two populations by employing ideas like null hypotheses and p-values. However, the goal is not really to learn how to compare populations using frequentist techniques. Instead, we want to continue learning how these ideas are used to construct significance tests and evaluate predictions. We’re also going to introduce something called a ‘test statistic’. First, we need to introduce a new example… 9.2 A new example Let’s step through the process introduced in the Learning from data chapter. We want to tackle the following question: “Is there a fitness difference between the purple and green morphs in the new population?” Based on various observations (that we’ve already discussed), our hypothesis is that purple plants are generally fitter than green plants. Since fitness is often strongly correlated with size in plants, we predict that purple morphs will be larger. That’s our question-hypothesis-prediction sorted out. What is (are) the population(s)? Our definition of the population(s) is different than before. This time we are going to conceive of each morph as a separate population, i.e. there are two populations in this new problem. This change of focus is perfectly valid. Remember, statistical populations are not really concrete things. That’s what we were getting at when we said statistical populations are defined by the investigator. What variable should we study? One way to address the prediction of size differences would be to measure the dry weight biomass of individuals of each morph. That’s a pretty reliable measure of how ‘big’ a plant is. Dry weight is a numeric variable, measured on a ratio scale (i.e. zero really does mean ‘nothing’). Which population parameter(s) should we work with? Our prediction is that purple morphs will be larger than green morphs, but what do we really mean by that? We probably don’t mean that every purple plant is bigger than every green plant. That’s a really strong precision, which, in any case, is not something we could ever validate with a sample. Instead, we want to know if purple plants are generally bigger than green plants. This can be thought of as a statement about central tendency, i.e. we want to evaluate whether purple plants are larger than green plants, on average, in their respective populations. The population parameters of interest are therefore the mean dry weights of each morph. The next step is to gather appropriate samples. Since this is a made-up example, we’ll just cut to the chase. You’ve already seen the samples we’re going to use. When we read in the ‘MORPH_DATA.CSV’ in the previous chapter we found it contained a numeric variable called Weight. This contains our dry weight biomass information. The categorical Colour variable analysed in the previous chapter tells us which kind of colour morph each observation corresponds to. (These data are tidy by the way—each observation is in a separate row and each variable is in only one column. We only use data in this form in this book.) You should work through this example from here onward. Read the data into an R data frame using read.csv, assigning it the name morph.data. Check it over with str or glimpse again. Just do it. Yes, you already know about these data, but it is a good habit to get into. The next step is to calculate point estimates of the mean dry weight of each morph. These are our ‘best guesses’ of the population means. It may be useful to know something about the variability of the samples as well though. We can summarise this with the standard deviation of each sample (these are not the standard errors!). Here is a reminder of how to do this using dplyr: morph.data %&gt;% group_by(Colour) %&gt;% summarise(mean = mean(Weight), sd = sd(Weight), samp_size = n()) ## # A tibble: 2 × 4 ## Colour mean sd samp_size ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Green 707.8424 149.8092 173 ## 2 Purple 766.5638 156.0316 77 This shows that the mean dry weight of the purple morph is greater than that of green morphs. The standard deviation estimates from the two samples indicate that the dry weights of purple morphs are more variable than the green morphs, though this difference isn’t very big. Remember, these numbers are just point estimates derived from limited samples of the populations. If we sampled the populations again, sampling variation would result in different estimates. We are not yet in a position to conclude that purple morphs are bigger than green morphs. We’ll move onto the main event—evaluating the statistical significance of the observed difference in means—in the next section. We should visualise our data first though. We could do this in a variety of ways, but since we only have two samples, we may as well summarise the full sample distribution of each morph weight. Here is some ggplot2 code to make a pair of histograms: ggplot(morph.data, aes(x = Weight)) + geom_histogram(binwidth = 50) + facet_wrap(~Colour, ncol = 1) Figure 9.1: Size distributions of purple and green morph samples (Hopefully this plot will also remind you how to use the facet_wrap function to make a multipanel plot based on the values of categorical variable—Colour in this instance). What does this figure tell us? We are interested in the degree of similarity of the two samples. It looks like purple morph individuals tend to have higher dry weights than green morphs. Didn’t we already know this? Yes, but that difference could have resulted from the odd outlier (an unusually large or small value). The histograms indicate that there really is a general difference in the size of the two morphs. There is also a lot of overlap between the two dry weight distributions though, so maybe the difference between the sample means is just a result of sampling variation. We need to employ some kind of statistical test… What do people mean when they ‘compare samples’? By comparing the central tendency (e.g. the mean) of different samples, we can evaluate whether or not something we have measured changes, on average, among different populations. We do this using the information in the samples to learn about the populations. It’s common to use the phrase ‘comparing samples’ when discussing the statistical tests that underlie these efforts. This is a little misleading though. When someone uses a statistical test to ‘compare samples’, what they are really doing is ‘using information in the sample to compare population parameters’. This distinction might seem unnecessarily pedantic (it is a bit, to be honest). However, it is helpful to keep the right description in mind, because this helps us remember what a statistical test is really doing. That said, saying or writing ‘using information in the sample to compare population parameters’ all the time is dull, so we often revert to the phrase ‘comparing samples’. We’ll do it in this book sometimes, but try to keep in mind what we really mean by that phrase. 9.3 Evaluating differences between population means We’re going to examine one method (there are others) for applying frequentist concepts to evaluate whether two population means are different. Specifically, we’re going to use something called a permutation test to evaluate the statistical significance of the difference between purple and green morph mean dry weights. In order to assess the strength of evidence for a difference between the two population means, we have to do something that seems quite strange (yes, frequentist statistics is odd). We can break this down into four steps: First, we assume there is really no difference between the population means. That is, we hypothesise that all the data are sampled from a pair of populations that are characterised by a single, shared population mean. To put it another way, we pretend there is really only one population. You might recognise this trick. It’s that null hypothesis again. Next, we use information in the samples to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation of ‘no difference between samples’. We summarise this by calculating the null distribution of some kind of test statistic. (We worked directly with the point estimates and their bootstrapped versions in the previous chapter. When dealing with more complicated statistical tests, we tend to work with other kinds of numeric quantities derived from the samples. The generic name for these is ‘test statistic’.) We then ask, “if there were no difference between the two groups, what is the probability that we would observe a difference that is the same as, or more extreme than, the one we actually observed in the true sample?” You may also recognise this probability. It’s a p-value. If the observed difference is sufficiently improbable, then we conclude that we have found a statistically significant result. A statistically significant result is therefore one that is inconsistent with the hypothesis of no difference. You might recognise this logic from the previous chapter. There are many different ways to go about realising this process. Regardless of the details, they all work by trying to evaluate what happens when we repeatedly sample from a population where the effect of interest (e.g. a difference between means) is absent. Let’s return to our example to see how this might work in practice. 9.4 A permutation test In our example, a hypothesis of ‘no difference’ between the mean dry weights of purple and green morphs implies the following: since morphs are sampled from the same population, the labels ‘purple’ and ‘green’ are meaningless. These labels don’t carry any real information so they may as well have been randomly assigned to each individual. This suggests that we can evaluate the statistical significance of the observed difference as follows: Make a copy of the original sample of purple and green dry weights, but do so by randomly assigning the labels ‘purple’ and ‘green’ to this new copy of the data. Do this in such a way that the original sample sizes are preserved. (We have to preserve the original sample sizes because we want to mimic the sampling process that we actually used, i.e. we want to hold everything constant apart from the labelling of individuals. The process of assigning random labels is called permutation.) Repeat this permutation scheme until we have a large number of artificial samples; 1000-10000 randomly permuted samples may be sufficient. For each permuted sample, calculate whatever test statistic captures the relevant information. In our example, this is the difference between the mean dry weight of purple and green morphs in each sample. (It doesn’t matter which way round we do this. We’re going to use mean(purple) - mean(green) when we do this below.) Compare the observed test statistic—the difference between the mean dry weights of purple and green plants in the true sample—to the distribution of sample statistics from the randomly permuted samples. This scheme is called a permutation test, because it involves random permutation of the group labels. Why is it useful? Each unique random permutation yields an observation from the null distribution of the difference among sample means, under the assumption that this difference is really zero in the population. We can use this to assess whether an observed difference is consistent with the hypothesis of no difference, by looking at where it lies relative to this distribution. We have implemented a permutation test in R using the purple/green morph data set for you, using 2500 permutations. We won’t show the code because it uses quite a few new R tricks, and they won’t be needed again. We can look at the first 50 values of the first two permuted samples to get a sense of how this works: ## Purple Green Green Purple Green Green Green ## 714.3592 693.4924 556.2063 653.6619 672.5207 661.0097 445.1001 ## Green Purple Green Purple Green Purple Purple ## 481.5068 647.6679 858.4820 567.4104 597.1629 718.7132 539.8551 ## Green Green Purple Green Green Green Green ## 753.0170 807.7700 1085.7036 926.4972 617.1209 632.5897 859.7013 ## Purple Green Green Green Purple Green Green ## 815.4634 666.7693 573.5907 694.2877 836.6883 665.8489 617.7895 ## Green Green Purple Green Green Green Purple ## 590.5936 775.8980 686.6790 813.4272 506.3904 566.9971 629.5894 ## Purple Green Green Green Purple Green Purple ## 878.2477 823.1128 542.7877 507.7345 786.2809 912.5058 853.5730 ## Purple Green Green Purple Green Purple Green ## 485.2197 879.7922 852.4711 516.7459 534.4548 702.1948 977.8877 ## Green ## 653.5126 ## Purple Purple Green Green Purple Green Green ## 714.3592 693.4924 556.2063 653.6619 672.5207 661.0097 445.1001 ## Green Purple Green Green Green Green Purple ## 481.5068 647.6679 858.4820 567.4104 597.1629 718.7132 539.8551 ## Purple Purple Purple Green Green Purple Green ## 753.0170 807.7700 1085.7036 926.4972 617.1209 632.5897 859.7013 ## Purple Green Purple Green Green Green Green ## 815.4634 666.7693 573.5907 694.2877 836.6883 665.8489 617.7895 ## Green Green Green Green Green Purple Green ## 590.5936 775.8980 686.6790 813.4272 506.3904 566.9971 629.5894 ## Green Green Green Green Green Green Green ## 878.2477 823.1128 542.7877 507.7345 786.2809 912.5058 853.5730 ## Green Green Green Green Green Green Green ## 485.2197 879.7922 852.4711 516.7459 534.4548 702.1948 977.8877 ## Green ## 653.5126 The data from each permutation are stored as numeric vectors, where each element of the vector is named according to which morph type it corresponds to (these are the labels we referred to above). Notice that the set of numbers doesn’t change among the two permuted samples. The only difference between them is the labelling. The difference between the mean dry weights in the first permutation is 7.6698902. This difference is 23.8737542 in the second sample. What we really care about here is the distribution of these differences. This distribution is an approximation to the sampling distribution of the difference between means under the null hypothesis (i.e. the null distribution). You may have to read that last sentence a few times. Here is a histogram that summarises the 2500 mean differences from the permuted samples: Figure 9.2: Difference between means of permuted samples Notice that the distribution is centred at zero. This makes sense; if we take a set of numbers and randomly allocate them to groups, on average, we expect the difference between the mean of these groups to be zero. What does the red line show? This is the estimated value of the difference between the mean purple and green morph dry weights in the real sample (our test statistic). The relevant thing to pay attention to here is the location of this value within the distribution of differences. It looks like the estimated difference is very unlikely to have arisen through sampling variation if the population means of the two groups were identical. We can say this because the estimated difference lies at the end of one ‘tail’ of the sampling distribution. We can quantify the probability of observing the estimated difference from the null distribution we constructed. Only 4 out of the 2500 permutations ended up being equal to, or ‘more extreme’ (more positive) than, the observed difference. The probability of finding a difference in the means equal to or more positive than the observed difference is, therefore, p = 0.0016. This is the p-value associated with our statistical test of significance. Let’s run through the interpretation of that p-value. Here’s the general chain of logic again… The p-value is the probability of obtaining a test statistic (i.e. the difference between means) equal to, or ‘more extreme’ than, the estimated value, assuming the null hypothesis is true. The null hypothesis is one of no effect (i.e. the difference is 0), so a low p-value can be interpreted as evidence for the effect being present. How low does the p-value have to be before we decide we have ‘enough evidence’? A significance threshold of p &lt; 0.05 is conventionally used in biology. If we find p &lt; 0.05, then we conclude that we found a statistically significant effect. Here’s how this logic applies to our example… The permutation test assumed there was no difference between the purple and green morphs, so the low p-value indicates that the estimated difference between the mean dry weight of purple and green morphs was unlikely to have occurred by chance, if there is really no difference at the population level. This means we should interpret the low p-value as evidence for the existence of a difference in mean dry weight among the populations of purple and green morphs. Since p = 0.0016, we say we found a statistically significant difference at the 5% level. We have to be careful at this point. The test we just did is called a ‘one-tailed’ test, because we only looked at one end (the tail) of the null distribution. This kind of test is only appropriate for evaluating directional predictions (e.g. purple &gt; green). If, instead of testing whether purple plants were larger than green plants, we just wanted to know if they were different (in either direction), we should have used a ‘two-tailed’ test. These work by looking at both ends of the null distribution. Don’t worry too much if that isn’t crystal clear at the moment. We’ll return to idea of one- vs. two-tailed tests in the next block of work. For now, it’s enough to know that we used a one-tailed test. Here’s how we might summarise this in a written report: The mean dry weight biomass of purple plants (77) was significantly greater than that of green plants (173) (one-tailed permutation test, p&lt;0.05). Notice that we report the sample sizes used, the type of test employed, and the significance threshold we passed (not the raw p-value). 9.5 What have we learned? Permutation tests are reasonably straightforward to apply in simple situations, but can be tricky to use in a more complex setting. We are not expecting you to be able to implement a permutation test yourself. Just as with bootstrapping in the previous chapter, we used it to demonstrate how frequentist statistics works. In this instance, for making comparisons. The basic ideas are no different from those introduced in the previous chapter… define what constitutes an ‘effect’ (e.g. a difference between means), but then assume that there is ‘no effect’ (i.e. define the null hypothesis), select an appropriate test statistic that can distinguish between the presence of an ‘effect’ and ‘no effect’, (In practice, each particular kind of statistical test uses a standard test statistic. We don’t have to select these ourselves.) construct the corresponding null distribution of the test statistic, by working out what would happen if we were to take frequent samples in the ‘no effect’ situation, and finally, use the null distribution and the test statistic to calculate a p-value, to evaluate how frequently the data would be observed under the hypothesis of no effect. Notice that we only really introduced one new idea in this chapter. When evaluating differences among populations we need to work with a single number that can distinguish between ‘effect’ and ‘no effect’. This is called the test statistic. Sometimes this can be expressed in terms of familiar quantities like means (we just used a difference between means above), but this isn’t always the case. For example, we use something called an F-ratio to evaluate differences among more than two means. We’ll get to this later in the book… "],
["hypotheses-and-p-values.html", "Chapter 10 Hypotheses and p-values 10.1 A few words about the null hypothesis 10.2 Interpreting and reporting p-values 10.3 Biological vs. statistical significance", " Chapter 10 Hypotheses and p-values 10.1 A few words about the null hypothesis When using frequentist statistics we are always asking what would happen if we continually sampled from a population where the effect we are interested in is not present. This idea of an hypothetical ‘no effect’ situation is so important that it has a special name; it is called the null hypothesis. Every kind of statistical test (in this book at least) works by first specifying a particular null hypothesis. You can only fully understand the results of a statistical test if you understand the null hypothesis it relies on. 10.1.1 Hypotheses and null hypotheses When discussing the scientific process, we have said that an hypothesis is a statement of a proposed process or mechanism which might be responsible for an observed pattern or effect. We have also seen that in statistics, we encounter ‘hypothesis’ used in a different, and quite specific way. In particular we frequently see the term: null hypothesis (often written in statistics books as H0). The null hypothesis is simply a statement of what we would expect to see if there is actually no effect of the factor we are looking at (e.g., plant morphology) on the variable that we measure (e.g., dry weight biomass). So in the second plant morph example our null hypothesis was There is no difference in mean biomass of purple and green plants. All statistical tests you are likely to encounter in biology work by specifying a null hypothesis and then evaluating the observed data to see if they deviate from the null hypothesis in a way that is inconsistent with sampling variation. This may seem like a rather odd approach, but there are good theoretical and practical reasons for doing things this way. You need to be aware of what a null hypothesis is, and what it is used for, or you won’t be able to interpret the results of statistical tests. However, in general discussion of tests we normally refer to the effect that is the opposite of the null hypothesis—i.e. the effect you are actually interested in—as the test hypothesis, or the alternative hypothesis (often denoted H1 in statistics books). The alternative hypothesis is essentially a statement of the effect you are interested in evaluating, e.g., purple and green plants differ in their mean size. It is a statement of whatever is implied if the null hypothesis is not true. Having got all the types of hypothesis sorted out, we can then use a particular frequentist technique (e.g. a permutation test) to evaluate the observed result against that expected if the null hypothesis was true. The test gives us a probability (p-value) telling us how likely it is that we would have got the result we observe, or a more extreme result, if the null hypothesis was really true. If the value is sufficiently small—we discuss what ‘sufficiently small’ means in the next section—we judge it unlikely that we would have seen this result if the null hypothesis was true and consequently we reject the null hypothesis (i.e. reject the notion that there is no difference) and instead accept the alternative hypothesis that there is a difference. Note that this is not the same as ‘proving’ the alternative hypothesis is true. You can’t prove anything by collecting data or carrying out an experiment. If the probability is large, then it is quite likely that we could have got the observed result if the null hypothesis was true, i.e. it is due to sampling variation. In this case we cannot reject the null hypothesis. Note that in this situation we say that we “do not reject the null hypothesis”. This is not the same as accepting that the null hypothesis is true, paradoxical though this may seem. One obvious reason for this is that if we only have a small sample then there may be an effect of the factor we are looking at, but we simply can’t detect it because we don’t have enough data. 10.2 Interpreting and reporting p-values It is important to understand the meaning of the probabilities generated by statistical tests. We have already said a p-value is the proportion of occasions on which you would expect to see a result at least as extreme as the one you actually observed if the null hypothesis (of no effect) was true. Conventionally (in biology at least) we accept a result as statistically significant if p &lt; 0.05 (also expressed as 5%). This threshold is called the significance level of a test. We’ve said it before but it is worth repeating: there is nothing special about the p &lt; 0.05 significance level. It is just a widely used convention. A probability of 0.05 is a chance of 1 in 20. This means that if there really was no effect of the factor we are investigating, we would expect to get a result significant at p=0.05 about 5 times in 100 samples. To envisage it more easily, it is slightly less than the chance of tossing a coin 4 times and getting 4 heads in a row (p=0.0625). It’s not all that rare really. This puts a ‘significant’ result into context. Would you launch a new drug on the market or bring a prosecution for pollution on the evidence of the strength of four heads coming up in a row when a coin is tossed? Well of course such things are unlikely to hinge on a single test, but it is always worth bearing in mind what ‘significance’ actually means. In general, the smaller the probability the more confident one can be that the effect we see is real1. A probability of p=0.01 (1 in 100) is pretty good evidence, and p=0.001 (1 in 1000), or less, is better still. For this reason, in some critical applications such as drug testing the value set for accepting a result as significant may be lower (e.g. p=0.01). The costs of using a more stringent threshold is that this increases the possibility of false negatives (called a ‘type II’ error)–i.e. we are more likely to fail to detect an effect when it is really present. Which significance level should you use We will always use the p = 0.05 threshold in this book. You need to rememeber this fact, because we aren’t always going to remind you of it. 10.2.1 What if p is close to 0.05? The thing to remember here is that, although we tend to use p=0.05 as a cut-off, a p-value is really a continuous measure and p=0.055 is not very different from p=0.045. The exact value of p will be affected by how well the data fulfill the assumptions of the test–which will only be approximately with most biological data, so you shouldn’t set too much store by the difference between p=0.045 and p=0.055. It would be irrational on the one hand to reject an idea completely just on the basis of a result of p=0.055, while at the same time being prepared to invest large amounts of time and money implementing policies based on a result of p=0.045. 10.2.2 Presenting p-values R will typically display p-values from a statistical significance test to six decimal places (e.g. p = 0.003672). Often however, the results from tests are presented as one of the following four categories: p &gt; 0.05, for results which are not statistically significant (sometimes also written as ‘NS’), p &lt; 0.05, for results where 0.01 &lt; p &lt; 0.05, p &lt; 0.01, for results where 0.001 &lt; p &lt; 0.01, p &lt; 0.001 for results where p &lt; 0.001, Should we use categories for p? This style of presentation stems from the fact that statistical tests often had to be calculated by hand in the days before everyone had access to a computer. The significance of the result was difficult to calculate directly, so it would have been looked up in a special table. These days, a computer can calculate the exact probability for you, and so there is no particular reason not to present the results as the actual p-value. Determing statistical significance and reporting p-values The significance level is used to determine whether or not a result is deemed to be ‘statistically significant’. This is always p &lt; 0.05 in this book. However, we often use the above categories to report the results of a test. Don’t confuse the category used to report the p-value with the actual significance level an inverstigator is using. Just because someone writes ‘p &lt; 0.01’ when they report the results of a test, it does not mean that they were working at the 1% significance level (p &lt; 0.01). It is not wrong to use the four categories above, but giving the actual probability may be a little more informative to the reader. It could be useful to know that p = 0.014 rather than p = 0.047, but if categories were used both would simply appear as p &lt; 0.05. Similarly it can be informative to know that a test had p = 0.06 rather than simply quoting it just as p &gt; 0.05 or NS. However, no-one much cares about the difference between very small probabilities, so if p is smaller than 0.001 it can sensibly be given as simply p &lt; 0.001. The asterisks convention It is common to see ranges of probabilities coded with asterisks: * for p = 0.05…0.01, ** for p = 0.01…0.001, *** for p &lt; 0.001. This is common in tables and text in figures as it is a more compact and visually obvious representation than numbers. However, you should never use the asterisks convention in the text of a report. 10.3 Biological vs. statistical significance A final, but vital, point: do not confuse statistical significance with biological significance. A result may be statistically highly significant (say p &lt; 0.001) but biologically trivial. To give a real example, in a study of the factors determining the distribution of freshwater invertebrates in a river, the pH of water was measured in the open water and in the middle of the beds of submerged vegetation. There was a statistically significant difference in pH (p &lt; 0.01) but the mean pH values were 7.1 in the open water and 6.9 in the weeds. This is a very small effect, and almost certainly of no importance at all to the invertebrates. The significance of a result depends on a combination of three things (1) the size of the effect, (2) the variability of the data, (3) the sample size. Even a tiny effect can be significant if the data have very little variation and the sample size is large. You should not automatically equate a significant result with a large effect—you need to visualise the data2 , inspect the estimates, and consider the biological implications of the difference. The statistical results can give you some guidance in separating genuine differences from random variation, but they can’t tell you whether the difference is biologically interesting or important—that’s your job! However, it is certainly possible to arrive at small a p-value when there is no real effect present. Most often this occurs when we use a statistical model or test that is inappropriate for the data. Don’t be a slave to the p-value! Think critically about your analysis↩ This is another reason we always plot our data↩ "],
["parametric-statistics.html", "Chapter 11 Parametric statistics 11.1 Introduction 11.2 Mathematical models 11.3 The normal distribution", " Chapter 11 Parametric statistics 11.1 Introduction The majority of statistical tools we’ll learn about in this book share one important feature: they are underpinned by a mathematical model of the population(s) and sampling process that gave rise to the data. Because a mathematical model of some kind is lurking in the background, this particular flavour of statistics is known as parameteric statistics.3 We aren’t going to study the mathematical details of these models in any great detail; after all, this isn’t a maths book. However, there are a few key ideas that are useful to know about, for two reasons: It is important to consider the assumptions underlying the models we’ll be using. Mathematical assumptions are essentially aspects of a system that we accept as true, or at least nearly true. If these aren’t reasonable for a given situation, we can’t be sure that the results of the corresponding analysis (e.g. a statistical test) will be reliable. We should always evaluate teh assumptions of an analysis to determine whether we trust it. We explored a number of concepts from fequentist statistics in the last few chapters, such as sampling variation, null distributions, and p-values. These ideas will crop up time and time again throughout the book. We want to ensure that we can connect the abstract ideas in the last few chapters to the practical aspects of ‘doing statistics’. We want to understand, in rough terms at least, how the models and their assumptions lead to particular statistical tests. 11.2 Mathematical models A mathematical model is a description of a system using the language and concepts of mathematics. A statistical model is a particular class of mathematical model that describes how samples of data are generated from a hypothetical population. We’re only going to consider a small subset of the huge array of statistical models people routinely use. In conceptual terms, the models we use in this book describe the data in terms of a systematic component and a random component: \\[\\text{Observed Data} = \\text{Systematic Component} + \\text{Random Component}\\] The systematic component of a model describes the structure, or the relationships, in the data. Often when people are refering to ‘the model’, this is the bit they care about. The random component (also called the stochastic component) captures the left over “noise” in the data. This is essentially the part of the data that the systematic part of the model fails to describe. This is best understood by example. In what follows we’re going to label the individual values in the sample \\(y_i\\) (the \\(i\\) indexes these values; it takes values 1, 2, 3, 4, … and so on). We can think of the collection of the \\(y_i\\)’s as the variable we’re interested in. The simplest kind of model we might consider is one that describes a single sample of one variable. A model for these data can be written: \\(y_i = a + \\epsilon_i\\). With this model, the systematic part is given by \\(a\\). This is usually the population mean. The random component is given by \\(\\epsilon_i\\). This describes how the individual values deviate from the mean. A slightly more complicated model is one that considers a relationship between the values of \\(y_i\\) and those of another variable, which we’ll call \\(x_i\\). A model for these data might be written as: \\(y_i = a + b \\times x_i + \\epsilon_i\\). The \\(a + b \\times x_i\\) bit of this is the systematic component. This is just the equation of a straight line with an intercept \\(a\\) and slope \\(b\\). The random component is given by the \\(\\epsilon_i\\). This describes how the individual values deviate from the line. These two descriptions are incomplete. What is missing is a description of the distribution of the \\(\\epsilon_i\\). In this book, this assumption is almost always the same: we assume that the \\(\\epsilon_i\\) are drawn from a normal distribution… 11.3 The normal distribution If you have studied A-level statistics you will know all about the normal distribution. If not, you may have come across it without realising: the normal distribution is sometimes called the ‘Gaussian distribution’, or more colloquially, ‘the bell-shaped curve’. Here’s a histogram of 100000 values drawn from a normal distribution with a mean of 5 and a standard deviation of 1: set.seed(27081975) data.frame(x = rnorm(100000, mean = 5, sd = 1)) %&gt;% ggplot(aes(x = x)) + geom_histogram(bins = 25) Figure 11.1: Distribution of a large sample of normally distributed variable We don’t have time in this book to really study the normal distribution in much detail, nor is there much benefit to be derived from doing this. Instead, we’ll just list some key facts about the normal distribution, which we’ll refer to from time to time: The normal distribution is appropriate for numeric variables measured on an interval or ration scale. Strictly speaking, the variable should be continuous, but a normal distribution if often a decent approximation for discrete data. The normal distribution is completely described by its mean and its standard deviation. if we know these two quantities for a particular normal distribution, we know everything there is to know about that distribution. This is useful for assesssing the range of likely values of a normally distributed variable… If a variable is normally distributed, then just over 95% of its values will fall inside an interval which is 4 standard deviations wide: the upper bound is equal to the \\(\\text{Mean} + 2 \\times \\text{S.D.}\\); the lower bound is equal to \\(\\text{Mean} - 2 \\times \\text{S.D.}\\). When we add or subtract two normally distributed variables to create a new variable, the resulting variable will also be normally distributed. Similarly, if we multiply a normally distributed by a number to create a new variable, the resulting variable will still be normally distributed. In summary, the mathematical properties of the normal distribution are very well understood, and many of these properties make the distribution easy to work with. Most important of all—for doing statistics at least—is that mathematicians have worked out how the sampling distribution of means (and similar quantities) behave when the underlying variables are normally distributed. 11.3.1 Standard error of the mean Let’s consider an example. Say that we want to estimate the standard error of the sampling distribution of a mean. If we’re happy to assume that the sample was drawn from a normal distribution, then there is a well-known formula for calculating the standard error of the mean. There’s no need to resort to computationally expensive techniques like bootstrapping to work this out. If \\(s^2\\) is the variance of the sample, and \\(n\\) is the sample size, the standard error is given by: \\[\\text{Standard error of the mean} = \\sqrt{\\frac{s^2}{n}}\\] That’s it, if we know the variance and size of a sample, it is easy to estimate the standard error of its mean. In fact, as a result of rule #4 above, we can calculate the standard error of any quantity that is derived by adding or subtracting the means of two or more samples drawn from normal distrubutions. 11.3.2 The t distribution A statistician called W.G. Gosset showed that when we take a sample from a normally distributed variable, estimate its mean, and divide this estimate by its standard error (i.e. calculate: mean / s.e.), the sampling distribution of this new quantity has a particular form. It follows a Student’s t-distribution of some kind4. The same is true if you take two samples from a pair of normal distributions, calculate the difference between their estimated means, and then divide this by its standard error. The sampling distribution of the scaled difference between estimates of means also follows a Student’s t-distribution.5 Because we rescale the mean by its standard error, the form of the t-distribution only depends on one thing: the sample size. It probably isn’t obvious that this is an important result, but trust us, it is! We’re going to use the results we just skimmed over as a basis for simple statistical tests in the next few chapters. These will be different kinds of t-test—you can guess where the name comes from. We won’t delve any further into the t-distribution here, but we are going to be using it a lot (actually, R will use it—we’ll mostly be interpreting results). In this context, the word ‘parametric’ refers to the fact that the behaviour of a model is defined by one or more quantities known as ‘parameters’.↩ Why is it called Student’s t? The t-distribution was discovered by W.G. Gosset, a statistician employed by the Guinness Brewery. He published his statistical work under the pseudonym of ‘Student’, because Guinness would have claimed ownership of his work if he had used his real name.↩ In fact, any ‘linear combination’ of normally variables will have this property.↩ "],
["one-sample-t-tests.html", "Chapter 12 One sample t-tests 12.1 When do we use one-sample t-test? 12.2 How does the one-sample t-test work? 12.3 Carrying out a one-sample t-test in R", " Chapter 12 One sample t-tests 12.1 When do we use one-sample t-test? The one-sample t-test is one of the simplest statistical tests. It is appropriate in situations where we are studying a sample of numeric variable from a single population, and we need to to compare the mean of the variable to an expected value. The one-sample t-test test uses information in the sample to evaluate whether the population mean is likely to be different from an expected value. The expected value might be something predicted from theory, or some other prespecified value we are interested in. Here are a couple of examples: We have a theoretical model of foraging behaviour that predicts an animal should leave a food patch after 10 minutes. If we have data on the actual time spent by 25 animals observed foraging in the patch, we could test whether the mean foraging time is significantly different from the prediction using a one-sample t-test. We are monitoring sea pollution and have a series of water samples from a beach. We wish to test whether the mean density of faecal coliforms (bacteria indicative of sewage discharge) for the beach can be regarded as greater, or less than the legislated limit. A one-sample t-test will enable us to test whether the mean value for the beach as a whole exceeds this limit6. 12.2 How does the one-sample t-test work? The key assumption of one-sample t-test that the variable is normally distributed in the population. Imagine that we have taken a sample of a variable (imaginatively called ‘X’) from a population we’re investigating. Here’s an example of what these data might look like, assuming a sample size of 50 is used: Figure 12.1: Example of data used in a one-sample t-test The distribution of the sample look roughly bell-shaped, so it seems plausible that it was drawn from a normal distribution7. The red line shows the sample mean, and the blue line shows the expected value (this is 10, so this example could correspond to the foraging study mentioned above). The observed sample mean is about one unit larger than the expected value. The question is, how do we decide whether the population mean is really different from the expected value? Perhaps the difference between the observed and expected value is due to sampling variation. We’re still doing frequentist statistics, so we tackle this question by first setting up an appropriate null hypothesis. The null hypothesis in this instance is that the population mean is equal to the expected value. We then have to work out what the sampling distribution of the mean looks like under this null hypothesis (this is the ‘null distribution’), and use this to assess how likely the observed result is. This chain of reasoning is quite similar to that developed in the bootstrapping example considered in the Statistical significance and p-values chapter. There are some differences in the details, but these aren’t so important. The really important change is that now, because we’re now prepared to make the normality assumption, the whole process of carrying out the statistical test is much simpler. The consequence of the normality assumption is that the null distribution will have a known mathematical form (yes, it’s related to the t-distribution). We can use this knowledge to construct the test of statistical significance, but instead of using the whole sample as we did for the bootstrapping, we only need three pieces of information to construct the test: the sample size, the sample variance, and the sample means. No resampling of data is involved. OK, how does a one-sample t-test it actually work? It is carried out as follows: Step 1. Estimate the standard error of the sample mean. This estimate gives us an idea of how much sampling variation we can expect to observe. It turns out that as long as the variability in the population is constant, the standard error doesn’t depend on the true value of the mean. So the standard error of the observed sample mean is also the standard error of any mean under any particular null hypothesis. Ultimately, this step boils down to applying a simple formula involving the sample size and the standard deviation of the sample … \\[\\text{Standard Error of the Mean} = \\sqrt{\\frac{s^2}{n}}\\] …where \\(s^2\\) stands for the sample variance and \\(n\\) stands for the sample size. This is the formula we introduced in the Parametric statistics chapter. Notice that standard error of the mean gets smaller as the sample sizes grows or the sample variance shrinks. Step 2. Calculate the mean. That’s simple enough. This is our ‘best guess’ of the unknown population mean. However, its role in the one-sample t-test is to allow us to construct a test statistic in the next step. Step 3. Calculate a test statistic from the sample mean and standard error. We calculate the test-statistic by dividing the sample mean (from step 2) by its estimated standard error (from step 1):8 \\[\\text{t} = \\frac{\\text{Sample Mean}}{\\text{Standard Error of the Mean}}\\] Why is this useful? If our normality assumption is appropriate, then this test-statistic will follow a t-distribution. This is guaranteed by the normality assumption. So this test statistic is in fact a type of t-statistic. That’s why we label it t. This knowledge leads to the final step… Step 4. Compare thet-statistic to the theoretical predictions of the t-distribution, in order to assess the statistical significance of the difference between observed and expected value. That is, we calculate the probability that we would have observed a difference between means with a magnitude as large as, or larger than, the observed difference, if the null hypothesis were true. That’s the p-value for the test. We could step through the actual calculations involved in these steps in detail, using R to help us, but there isn’t a whole lot to be gained by doing this. We’re going to let R to handle all the heavy lifting. Before we see how to do this we should review the assumptions of the one-sample t-test first. 12.2.1 Assumptions of the one-sample t-test There are a number of assumptions that need to be met in order for a one-sample t-test to be valid. Some of these are more important than others. We’ll start with the most important and work down the list in order of importance: Independence. People tend to forget about this one, probably because they can’t do much about it if there is a problem. We’re going to discuss the important idea of independence later in the book when we think about experimental design. For now, we’ll briefly tell you why it matter: if the data are not independent, then the p-values generated by the one-sample t-test will not be reliable. Even mild non-independence can be a serious problem, which why it is so important to design your data collection / experiment well. Measurement scale. The variable that you are working with should be measured on an interval or ratio scale, i.e. it should be a numeric variable. It generally doesn’t make much sense to apply a one-sample t-test to a variable that isn’t measured on one of these scales. Normality. The one-sample t-test will only produce completely reliable p-values if the variable is normally distributed in the population. However, this assumptions is less important than many people think. The t-test is fairly robust to mild departures from normality when the sample size is small, and when the sample size is large the normality assumption matters even less9. We don’t have the time to properly explain why the normality assumption is not too important for large samples, but we will at least state the reason: it is a consequence of something called the ‘central limit theorem’. How do we go about evaluating these assumptions? The first two are really aspects of experimental design, i.e. we can only evaluate them by thinking carefully about how the data were gathered and what was measured. What about the 3rd assumption? This is best evaluated by plotting the distribution of the sample, using something like a histogram or a dot plot. If the sample size is small, and the sample looks approximately normal when we visualise its distribution, then it is probably fine to use the t-test. Just remember, if we have a large sample we don’t need to worry much about moderate departures from normality. Ask someone with experience of data analysis if you run into this situation and are not sure whether there is a problem or not. 12.3 Carrying out a one-sample t-test in R You should work through the example in this section. We’re going to use the plant morph example again to learn how to carry out a one-sample t-test in R. Remember, the data were ‘collected’ to: 1) compare the frequency of purple morphs to a prediction; and 2) compare the mean dry weight of purple and green morphs. Neither of these questions can be tackled with a one-sample t-test. Instead, let’s pretend that we have unearthed a report from 30 years ago that found the mean size of purple morphs to be 710 grams. We want to evaluate whether the mean size of purple plants in the contemporary population is different from this expectation (perhaps we think they have adapted to local conditions). You probably already have the MORPH_DATA.CSV in your working directory (download it from MOLE again if not). Read the data in MORPH_DATA.CSV into an R data frame, giving it the name morph.data. We only need the purple morph data for this example, so we need to filter the data as well: morph.data &lt;- read.csv(file = &quot;MORPH_DATA.CSV&quot;) morph.data &lt;- filter(morph.data, Colour == &quot;Purple&quot;) The first thing we need to do do is examine the data… 12.3.1 Visualising the data and checking the assumptions We should first calculate a few summary statistics and then visualise the sample distribution purple morph dry weights. We already did this in the Comparing populations chapter. Here is the dplyr code for the descriptive statistics again: morph.data %&gt;% summarise(mean = mean(Weight), sd = sd(Weight), samp_size = n()) ## mean sd samp_size ## 1 766.5638 156.0316 77 So we have 77 purple plants in the sample. Not bad, but we should keep an eye on the normality assumption. Next, we check this distributional assumption: ggplot(morph.data, aes(x = Weight)) + geom_histogram(binwidth = 50) Figure 12.2: Size distributions of purple morph dry weight sample These is nothing too ‘non-normal’ about this sample distribution, so it seems reasonable to assume it came from normally distributed population. 12.3.2 Carrying out the test It is straightforward to carry out a one-sample t-test in R. The function we need to use is called t.test (no surprises there). Remember, we read the data into a data frame called morph.data. This has two columns: Weight contains the dry weight biomass of purple plants, and Colour is an index variable that indicates which sample (plant morph) an observation belongs to. We don’t really need the Colour column. Here is the R code to carry out a one-sample t-test: t.test(morph.data$Weight, mu = 710) We have suppressed the output for now as we want to focus on how to use t.test function. We have to assign two arguments (remember function arguments?–these control what a function does): The first argument (morph.data$Weight) is just a numeric vector containing the sample values. Unfortunately, we can’t give t.test a data frame when doing a one-sample test. Instead, we have to pull out the column we’re interested in. This is what morph.data$Weight does. That’s all the $ is doing. The second argument (called mu) sets the expected value we want to compare the mean to—so mu = 710 tells the function to compare the mean to a value of 710. This can be any value we like of course. That’s it for setting up the test. Let’s take a look at the output: t.test(morph.data$Weight, mu = 710) ## ## One Sample t-test ## ## data: morph.data$Weight ## t = 3.1811, df = 76, p-value = 0.002125 ## alternative hypothesis: true mean is not equal to 710 ## 95 percent confidence interval: ## 731.1490 801.9787 ## sample estimates: ## mean of x ## 766.5638 The first part of the output reminds us what we did. The first line tells us what kind of t-test we used. This says: One Sample t-test, so we know that we have used the one-sample t-test. The next line reminds us about the data. This says: data: morph.data$Weight, which is R-speak for ’we compared the mean of the Weight variable to an expected value. Which value? This is given later. The third line of text is the most important. This says: t = 3.1811, df = 76, p-value = 0.002125. The first part of this, t = 3.1811, is the test statistic (i.e. the value of the t-statistic). The second part, df = 76, summarise the ‘degrees of freedom’. This is essentially a measure of how much power our statistical test has (see the box below). The third part, p-value = 0.002125, is the all-important p-value. This says there is a statistically significant difference in the mean dry weight biomass of the two colour morphs, because p &lt; 0.05. Because the p-value is less than 0.01 but greater than 0.001, we report this as ‘p &lt; 0.01’. You should read through the [Hypotheses and p-values][presenting-p-values] chapter again if you’re not sure how that logic works. Don’t ignore the fourth line of text (alternative hypothesis: true mean is not equal to 710). This reminds us what the alternative to the null hypothesis is (H1). It tells us what expected value was used in the test (i.e. 710). The next two lines show us the ‘95% confidence interval’ for the difference between the means. We don’t really need this information, but we can think of this interval as a summary of the likely values of the true mean (a confidence interval is more complicated than that in reality). The last few lines just summarise the sample mean. This is only useful if we did not bother to calculate this already (which we should already have done!). A bit more about degrees of freedom Degrees of freedom (abbreviated d.f. or df) are closely related to the idea of sample size. In a nutshell, the greater the degrees of freedom associated with a test, the more likely it is to detect an effect (however defined) if it’s present, i.e. we say the test has more ‘statistical power’. To calculate the degrees of freedom, we start with the sample size, and then we reduce this number by one for every mean we have to calculate to construct the test. That’s a bit of an over simplification, but it’s enough to be getting on with. Calculating degrees of freedom for a one-sample t-test is quite simple. The degrees of freedom are just n-1, where n is the number of observations in the sample. We lose one degree of freedom because we have to calculate the sample mean to construct the test. 12.3.3 Summarising the result Having obtained the result we need to write the conclusion. Remember, we are testing an hypothesis so always go back to the original question to write the conclusion. In this case the appropriate conclusion is: The mean dry weight biomass of purple plants is significantly different from the expectation of 710 grams (t = 3.18, d.f. = 76, p &lt; 0.01). This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which value was used in the comparison. It is sometimes appropriate to give the values of the sample mean in the conclusion: The mean dry weight biomass of purple plants (767 grams) is significantly different from the expectation of 710 grams (t = 3.18, d.f. = 76, p &lt; 0.01). When you are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. Simply writing t = 3.18 or p &lt; 0.01 is not an adequate conclusion. There are a number of common questions that arise when presenting t-test results: Help - what do I do if is negative? Don’t worry! A t-statistic can come out negative or positive, it simply depends on which order the two samples are entered into the analysis. Since it is just the absolute value of t that determines the p-value, when presenting the results, just ignore the minus sign and always give t as a positive number. How many significant figures for t? The t-statistic is conventionally given to 3 significant figures. This is because, in terms of the p-value generated, there is almost no difference between t = 3.1811 and t = 3.18. Upper or lower case The t statistic should always be written as lower case when writing it in a report (as in the conclusions above). Similarly, d.f. and p are always best as lower case. There are some statistics we will encounter later which are written in upper case but, even with these, d.f. and p should be lower case. How should I present p? There are various conventions in use for presenting p-values. We discussed these in the Hypotheses and p-values chapter. Make sure you learn them. You won’t be able to understand scientific papers or prepare reports properly if you’re not aware of these conventions. p = 0.00? It’s impossible! p = 1e-16? What’s that? Some computer packages (e.g. Minitab) will sometimes give a probability of p = 0.00. This does not mean the probability was actually zero. A probability of zero would mean something was impossible - and since you cannot show something to be impossible by taking samples, we should never say this. When a computer package says p = 0.00 it just means that the probability was ‘very small’. R uses a different convention for presenting small probabilities. A very small probability is usually given as p-value &lt; 2.2e-16 by R. What does 2.2e-16 mean? This is R-speak for scientific notation, i.e. 2.2e−16 is equivalent to 2.2 × 10−16 The question of whether the average for the whole beach is the thing to be concerned about, or whether we should be considering the peak values is another issue↩ It’s actually quite hard to determine whether a sample was drawn from a normal distribution when the sample size is small.↩ Notice that the magnitude of the test statistic is increased when either the sample means grows, or its standard error shrinks.↩ It’s hard to define what constitutes a ‘large’ sample, but 100s of observations would often be safe.↩ "],
["two-sample-t-test.html", "Chapter 13 Two-sample t-test 13.1 When do we use a two-sample t-test? 13.2 How does the two-sample t-test work? 13.3 Carrying out a two-sample t-test in R", " Chapter 13 Two-sample t-test 13.1 When do we use a two-sample t-test? The two-sample t-test is essentially a parametric version of the permutation procedure we studied in the Comparing populations chapter (although the mechanics of the approach are quite different). This test is appropriate in situations where we are studying a numeric variable that has been sampled from two independent populations. The aim is to evaluate whether or not the mean of this variable is different in the two populations. Here are two examples: We’re studying how the the dietary habits of Scandinavian eagle-owls vary among seasons. We suspect that the dietary value of a prey item is different in the winter and summer. To evaluate this prediction, we measure the size of Norway rat skulls in the pellets of eagle-owls in summer and winter, and then compare the mean size of rat skulls in each season using a two-sample t-test. We’re interested in the costs of anti-predator behaviour in Daphnia spp. We’ve conducted an experiment where we added predator kairomones (chemicals that signal the presence of a predator) to jars containing individual Daphnia. We have a control group where no kairomone was added. We’ve measured the change in body size of individuals over a one week period. We could use a two-sample t-test to compare the difference in mean growth rate. 13.2 How does the two-sample t-test work? The key assumption of the two-sample t-test that the variable is normally distributed in both populations (e.g. skulls in winter and summer). Imagine that we have taken a sample of a variable (again called ‘X’) from two populations, labelled ‘A’ and ‘B’. Here’s an example of what these data might look like if we took a sample of 50 items from each population: Figure 13.1: Example of data used in a two-sample t-test The distribution of each sample look roughly bell-shaped, so it seems plausible that they were drawn from a normal distribution. They also overlap quite a lot. However, this isn’t all that important because we’re interested in the means. The red lines are the mean of each sample—sample B obviously has a larger mean than sample A. The question is, how do we decide whether this difference is ‘real’, or purely a result of sampling variation? As always, we begin to tackle this question by setting up an appropriate null hypothesis. The null hypothesis in this case is that there is actually no difference between the population means. We then work out what the sampling distribution of the differences between sample means looks like under this null hypothesis (this is the ‘null distribution’). The key assumption that makes these calculations ‘doable’ is that the variable is normally distributed in each population. If this assumption is valid, then the null distribution will have a known form (yes, it’s related to the t-distribution). We can use this knowledge to carry out a test of statistical significance. How is this different from the permutation test? The logic is virtually identical. The only difference is that because we’re prepared to make the normality assumption, we only need to use a few pieces of information to construct out the significance test. These are basically the same bits of information we needed to construct the one-sample t-test, except that now there are two samples involved. We need the sample sizes of A and B, the sample variances of A and B, and the estimated difference between the sample means. No nasty resampling of data is involved. OK, so how does it actually work? The two-sample t-test is carried out as follows: Step 1. Estimate the standard error of the difference between the sample means under the null hypothesis of no difference. This estimate gives us an idea of how much sampling variation we can expect to observe in the estimated difference, if there were actually no difference between the population means. There are a number of different options for estimating this standard error—the choice of method depends on the assumptions we’re prepared to make about the variability of the two populations. Whatever choice we make, the calculation boils down to applying a simple formula involving the sample sizes and sample variances. The standard error gets smaller when the sample sizes grow, or when the sample variances shrink. That’s the important point really. Step 2. Calculate the two sample means, then calculate the difference between these estimates. This estimate of the difference between means can be thought of as our ‘best guess’ of the true difference. However, as with the one-sample t-test, its role in the two-sample t-test is to allow us to construct a test statistic. Step 3. Once we have estimated the difference between sample means and the standard error of this estimate, we can calculate the test statistic. You probably won’t be surprised to hear that this test statistic is a type of t-statistic. We calculate this t-statistic by dividing the difference between sample means (from step 2) by the estimated standard error of the difference (from step 1):10 \\[\\text{t} = \\frac{\\text{Difference Between Sample Means}}{\\text{Standard Error of the Difference}}\\] If our normality assumption is appropriate for both populations, then this t-statistic will follow a t-distribution. This is guaranteed by the normality assumption. That knowledge leads to the final step… Step 4. Compare the test statistic to the theoretical predictions of the t-distribution to assess the statistical significance of the observed difference. That is, we calculate the probability that we would have observed a difference between means with a magnitude as large as, or larger than, the observed difference, if the null hypothesis were true. That’s the p-value for the test. We could step through the various calculations involved in these steps, but there isn’t a whole lot to be gained by doing this. The key formulas and their different variants aren’t very informative unless you already know a reasonable amount of statistics. By all means take a look at the Wikipedi page if you are interested, and feel free to ask a TA to explain these if you’re curious how they work. However, there is absolutely no need to learn them. We’re going to let R to handle the heavy lifting again. We should review the assumptions of the two-sample t-test first though… 13.2.1 Assumptions of the two-sample t-test There are a number of assumptions that need to be met in order to use a two-sample t-test. These are basically the same assumptions that matter for the one-sample t-test. Once again, we start with the most important and work down the list in decreasing order of importance: Independence. Remember what we said in our discussion of the one-sample t-test? We said that if the data are not independent, the p-values generated by the test will be unreliable, and that even mild non-independence can be a serious problem. The same is true of the two-sample t-test. Measurement scale. The variable that we are working with should be measured on an interval or ratio scale. It makes no sense to apply a two-sample t-test to a variable that isn’t measured on one of these scales. Normality. The two-sample t-test will produce exact p-values if the variable is normally distributed in each of the two populations. Just as with one-sample t-test, the two-sample t-test is fairly robust to mild departures from normality when the sample sizes are small, and this assumption matters even less when the sample sizes are large. How do we evaluate the first two assumptions? Just as with the one-sample t-test, these are really aspects of experimental design—we have to evaluate them by thinking about how the data were gathered. The normality assumption may be checked by plotting the distribution of each sample, using histograms or a dot plots. Notice that we examine the distribution of each sample, not the combined distribution of both samples. If both samples looks approximately normal then it should be fine to proceed with the two-sample t-test (and if we have a large sample we don’t need to worry too much about departures from normality, unless they are large). 13.2.2 What about the equal variance assumption? If you learned about the two-sample t-test at some point in the past you may have been told that the variance (i.e. the dispersion) of each sample must be the same, or at least similar. This isn’t really true, for two reasons: The original version of Student’s two-sample t-test was derived by assuming that the variance of each population was identical, so it is the population variances, not the sample variances, that matter. There are methods available to compare population variances. However, there’s really no need to do this… What matters from a practical perspective is that R uses the “Welch” version of the two-sample t-test by default11. The Welch two-sample t-test does not rely on the equal variance assumption, so as long as we stick with this version of the t-test, the equal variance assumption isn’t one we need to worry about. Is there any reason not to use the Welch two-sample t-test? The alternative is to use the original Student’s t-test. This version of the t-test is potentially a little more powerful than Welch’s version, in the sense that it is more likely to detect a difference if it is really there. However, the difference in statistical power is really quite small when the sample sizes are similar, and the original test is only correct when the population variances are identical. Since we can never prove the ‘equal variance’ assumption—we can only ever reject it—it is generally safer to just use the Welch two-sample t-test. 13.3 Carrying out a two-sample t-test in R You should work through the example in this section. We’re going to use the plant morph example to learn how to carry out a one-sample t-test in R. By this point you should be very familiar with this data set. We’re going to use the test evaluate whether or not the mean dry weight of purple plants is different from that of green plants. You probably already have the MORPH_DATA.CSV in your working directory (download it from MOLE again if not). Read the data in MORPH_DATA.CSV into an R data frame, giving it the name morph.data: morph.data &lt;- read.csv(file = &quot;MORPH_DATA.CSV&quot;) Now, let’s work through the analysis… 13.3.1 Visualising the data and checking the assumptions As always, we should first calculate a few summary statistics and visualise the sample distributions of the green and purple morph dry weights. We already did this in the Comparing populations chapter, but here is the dplyr code for the descriptive statistics again: morph.data %&gt;% group_by(Colour) %&gt;% summarise(mean = mean(Weight), sd = sd(Weight), samp_size = n()) ## # A tibble: 2 × 4 ## Colour mean sd samp_size ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 Green 707.8424 149.8092 173 ## 2 Purple 766.5638 156.0316 77 The sample sizes 173 (green plants) and 77 (purple plants). These are good sized samples, so hopefully the normality assumption isn’t a big deal here. Still, we should check the distributional assumptions: ggplot(morph.data, aes(x = Weight)) + geom_histogram(binwidth = 50) + facet_wrap(~Colour, ncol = 1) Figure 13.2: Size distributions of purple and green morph samples These is nothing too ‘non-normal’ about the sample distributions, so it seems reasonable to assume they both came from normally distributed populations. 13.3.2 Carrying out the test It is quite simple to carry out a two-sample t-test in R. The function we need to use is called t.test (the same function as for the one-sample test). Remember, we read the data into a data frame called morph.data. This has two columns: Weight contains the dry weight biomass of each plant, and Colour is an index variable that indicates which sample (plant morph) an observation belongs to. Here is the R code to carry out a two-sample t-test t.test(Weight ~ Colour, morph.data) We have suppressed the output for now as we want to focus on how to use t.test function. We have to assign two arguments: The first argument is a formula. We know this because it includes a ‘tilde’ symbol: ~. The variable name on the left of the ~ should be the variable that contains the actual data (i.e. the numbers we want to compare). The variable on the right should be the indicator variable that says which group each observation belongs to. These are weight and Colour, respectively. The second argument is the name of the data frame that contains the two variables listed in the formula. That’s it. Let’s take a look at the output: t.test(Weight ~ Colour, morph.data) ## ## Welch Two Sample t-test ## ## data: Weight by Colour ## t = -2.7808, df = 140.69, p-value = 0.006165 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -100.46812 -16.97476 ## sample estimates: ## mean in group Green mean in group Purple ## 707.8424 766.5638 The first part of the output reminds us what we did. The first line tells us what kind of t-test we used. This says: Welch two-sample t-test, so we know that we have used the Welch version of the two-sample t-test which accounts for the possibility of unequal variance in the samples. The next line reminds us about the data. This says: data: weight by Colour, which is R-speak for ‘we compared the means of the Weight variable, where the sample membership is defined by the values of the Colour variable’. The third line of text is the most important. This says: t = -2.7808, d.f. = 140.69, p-value = 0.006165. The first part of this, t = -2.7808, is the test statistic (i.e. the value of the t-statistic). The second part, df = 140.69, summarise the ‘degrees of freedom’ (see the box below). The third part, p-value = 0.006165, is the all-important p-value. This says there is a statistically significant difference in the mean dry weight biomass of the two colour morphs, because p&lt;0.0512 The fourth line of text (alternative hypothesis: true difference in means is not equal to 0) just reminds us what the alternative to the null hypothesis is (H1). The next two lines show us the ‘95% confidence interval’ for the difference between the means. We don’t really need this information, but we can think of this interval as a summary of the likely values of the true difference (again, a confidence interval is more complicated than that in reality). The last few lines just summarise the sample means of each group. This is only useful if we did not bother to calculate these already. A bit more about degrees of freedom In the original version of the two-sample t-test (the one that assumes equal variances) the degrees of freedom of the test are give by (nA-1) + (nB-1), where nA is the number of observations in sample A, and nB the number of observations in sample B. The plant morph data included 77 purple plants and 173 green plants, so if we had used the original version of the test we would have (77-1) + (173-1) = 248 d.f. However, the Welch version of the t-test reduces the numbers of degrees of freedom using a formula which takes into account the difference in variance in the two samples. In a nutshell, the greater the difference in the two sample sizes the smaller the number of degrees of freedom becomes: When the sample sizes are similar, the adjusted d.f. will be close to that in the original version of the two-sample t-test. When the sample sizes are very different, the adjusted d.f. will be close to the sample size of the samller sample. Note that the ‘unequal sample size accounting’ results in degrees of freedom that are not whole numbers. It’s not critical that you remember all this. Here’s the key point: whatever flavour of t-test we’re using, a test with high degrees of freedom is more powerful than one with low degrees of freedom, i.e. the higher the degrees of freedom, the more likely we are to detect an effect if it is present. This is why degrees of freedom matter. 13.3.3 Summarising the result Having obtained the result we need to write the conclusion. Remember, we are testing an hypothesis so go back to the original question to write the conclusion. In this case the appropriate conclusion is: Mean dry weight biomass of purple and green plants differs significantly (Welch’s t = 2.78, d.f. = 140.7, p &lt; 0.01), with purple plants being the larger. This is a concise and unambiguous statement in response to our initial question. The statement indicates not just the result of the statistical test, but also which of the mean values is the larger, although our initial hypothesis was only that there would be a difference. Always indicate which mean is the largest. It is sometimes appropriate to give the values of the means in the conclusion: The mean dry weight biomass of purple plants (767 grams) is significantly greater than that of green plants (708 grams) (Welch’s t = 2.78, d.f. = 140.7, p &lt; 0.01) When we are writing scientific reports, the end result of any statistical test should be a conclusion like the one above. Remember, simply writing t = 2.78 or p &lt; 0.01 is not an adequate conclusion—don’t write this kind of thing in a report! Notice that the magnitude of the t-statistic is increased when either, the difference between the sample means grows, or the standard error of the difference shrinks.↩ Welch was another statistician, in case you’re wondering↩ Remember, because the p-value is less than 0.01 but greater than 0.001, we would report this as ‘p &lt; 0.01’.↩ "],
["paired-sample-t-tests.html", "Chapter 14 Paired-sample t-tests 14.1 When do we use a paired-sample t-test? 14.2 Why do we use a paired-sample design? 14.3 How do you carry out a t-test on paired-samples? 14.4 Carrying out a paired-sample t-test in R", " Chapter 14 Paired-sample t-tests 14.1 When do we use a paired-sample t-test? In the previous chapter we learned how to use a two-sample t-test to compare means among two populations. This test is sometimes called an independent two-sample t-test because the two samples are entirely independent sets of measurements (e.g. purple plant dry weights vs. green plant dry weights). A measurement in one sample has no link to any particular measurement in the second sample. That’s what ‘independent’ means in the context of a two sample t-test. However, there are situations in which data may naturally form pairs of non-independent observations: the first value in a sample A is linked in some way to the first value in sample B, the second value in sample A is linked with the second value in sample B, and so on. This is known, unsurprisingly, as a paired-sample design. Here are a couple of examples… A common example of a paired-sample design is the situation where we have a set of organisms, and we record some measurement from each organism before and after an experimental treatment. For example, if we were studying heart rate in relation to position (sitting vs. standing) we might measure the heart rate of a number of people in both positions. In this case the heart rate of a particular person when sitting is paired with the heart rate of the same person when standing. One of the consequences of habitat fragmentation is that it increases the relative amount of ‘edge’ habitat. If we are studying habitat fragmentation in forests we may want to know if biodiversity is different at the edges of forest fragments, compared to their centres. We could measure the biodiversity of different forest fragments in two places: at their edges and at their centres. In this case, edges and centres are obviously paired. We should not use a two-sample t-test when our data have this kind of structure. Why? In biology, we often have the problem that there is a great deal of variation between the items we’re studying (individual organisms, forest sites, etc). In some cases, there may be so much among-item variation that the effect of any difference among the situations we’re really interested in is obscured. A paired-sample design is a very useful way to control for this variation. Let’s see why… 14.2 Why do we use a paired-sample design? Consider the following. A drug company wishes to test two drugs for their effectiveness in treating a rare illness in which glycolipids are poorly metabolised. An effective drug is one that lowers glycolipid concentrations in patients. The company is only able to find 8 patients willing to cooperate in the early trials of the two drugs (they might be dangerous!). What’s more, the 8 patients vary in their age, sex, body weight, severity of symptoms and other health problems. If the 8 patients were randomly assigned to one or other drug treatments and their performance monitored, it would be likely be very difficult to detect statistically significant differences between the treatments. This is because we have an experiment which provides very little replication, yet we expect considerable variability from one person to another in the levels of glycolipid before any treatment is applied. We expect this variability to lead to a large standard error in the difference between means. One solution to this problems is to treat each patient with both drugs in turn and record the glycolipid concentrations in the blood, for each patient, after a period taking each drug. One arrangement would be for four patients to start with drug A and four with drug B, and then after a suitable break from the treatments, they could be swapped over onto the other drug. This would give us eight replicate observations on the effectiveness of each drug and we can determine for each patient which drug is more effective.13 The experimental design, and one hypothetical outcome, is represented in the diagram below… Figure 14.1: Data from glycolipid study, showing paired design. Each patient is denoted by a unique number. Each patient is represented by a unique number (1-9) (the order of the drugs in the plot does not matter—it doesn’t mean that Drug A was tested before Drug B just because Drug A appears first). Notice that there is a lot of variability in these data, both in the glycolipid levels of each patient, and also in the amount by which the drugs differ in their effects (e.g. the drugs have roughly equal effects for patient 5, while drug B appears to be more effective for patient 2). What is obvious from this pattern is that although the glycolipid levels vary a good deal between patients, Drug B seems to reduce glycolipid levels more than Drug A. The advantage to using a paired-sample design in this case is clear if we look at the results we might have obtained on the same patients but by dividing them into two groups of four and giving one group Drug A and one group Drug B: Figure 14.2: Data from glycolipid study, ignoring paired design. The patients and their glycolipid levels are identical to those in the previous diagram, but only patients 2, 3, 4 and 8 (selected at random) were given Drug A, while only patients 1, 5, 6, and 7 were given Drug B. If you calculated the means of the two groups, they would be different, with Drug B performing better, but the associated standard error would also be large relative to this difference. A two-sample t-test would certainly fail to identify a significant difference between the two drugs. So, it would be quite possible to end up with two groups where there was no clear difference in the mean glycolipid levels between the two drug treatments even though (as we have seen in the previous diagram) Drug B seems to be more effective in the majority of patients. What the pairing is doing is allowing us to factor out (i.e. remove) the variation among individuals, and concentrate on the differences between the two treatments. The result is a much more sensitive evaluation of the effect we’re interested in. The next question is, how do we go about analysing paired data in a way that properly accounts for the structure in the data? 14.3 How do you carry out a t-test on paired-samples? It should be clear why a paired-sample design might be useful, but how do we actually construct the right test? The ‘trick’ is to work directly with the differences between pairs of values. So in the case of the glycolipid levels illustrated in the first diagram, we noted that there was a greater decrease of glycolipids in 75% of patients using Drug B compared with Drug A. If we calculate the actual differences (i.e. subtracted the value for Drug A from the value for Drug B) for each patient we might see something like… ## -3.9 -4.3 2.5 -4.5 0.5 -3.9 -7.1 -2.6 Notice that there are only two positive values in this sample of differences, one of which is fairly close to 0. The mean difference is -2.9, i.e. on average, glycolipid levels are lower with Drug B. Another way of stating this observation is that within subjects (patients), the mean difference between drug B and drug A is negative. A paired-sample design focusses on the within-subject (or more generally, within-item) change. If, on the other hand, the two drugs had had similar effects then what would we expect to see? We would expect no consistent difference in glycolipid levels between the Drug A and Drug B treatments. Glycolipid levels are unlikely to remain exactly the same over time, but there shouldn’t be any pattern to these changes with respect to the drug treatment, some patients will show increases, some decreases, and some no change at all. The mean of the differences in this case should be somewhere around zero (although sampling variation will ensure it isn’t exactly equal to zero). So, what we do to carry out a t-test on paired-sample data is to find the mean of the difference of all the pairs and test this to see if it is significantly different from zero. You already know how to do this! This is just an application of the one-sample t-test, where the expected value (i.e. the null hypothesis) is 0. The thing to remember here is that although we started out with two sets of values, the sample we are actually evaluating are the differences between pairs. The population we’re interested in a ‘population of differences’. When used to analyse paired data in this way, the test is referred to as a paired-sample t-test. This is not wrong, but it important to remember that a paired-sample t-test is is really it is just a one-sample t-test applied to the sample of differences between pairs of associated observations. A paired-sample t-test isn’t really a new kind of test. Instead, it is a one-sample t-test applied to a new kind of situation. R offers the option of a paired-sample t-test to save us the effort of calculating differences. It calculates the differences between pairs for us as part of the test, although ultimately, it just carries out a one-sample test on those differences. We can easily calculate differences ourselves and then carry out a one-sample t-test with the new sample. We’ll look at how to do it the ‘old fashioned’ way first—calculating the differences and running a one-sample test—before using the short-cut method provided by R. Let’s briefly review the assumptions of the test first. This won’t take long… 14.3.1 Assumptions of the one-sample t-test The assumptions of a paired-sample t-test are no different from the one-sample t-test—ultimately they boil down to the same test! We just have to be aware of the target sample. The key point to keep in mind is that it is the sample of differences that is important, not the original data. There is no requirement for the original data to be drawn from a normal distribution because the normality assumption applies to the differences. This is very useful, because even where the original data seem to be drawn from a non-normal distribution, the differences between pairs can often be acceptably normal. The differences do need to be measured on an interval or ratio scale, but this is guaranteed if the original data are on one of these scales. 14.4 Carrying out a paired-sample t-test in R You should work through the example in this section. Staying with the problem of trials of two drugs for controlling glycolipid levels, the serum glycolipid concentration data from the trial illustrated above are stored in the GLYCOLIPID.CSV file. Download the this file from MOLE and place it in your working directory. Read GLYCOLIPID.CSV into an R data frame, giving it the name glycolipid. As always, we should start by looking at the raw data. We’ll use glimpse to do this: glimpse(glycolipid) ## Observations: 16 ## Variables: 4 ## $ Patient &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8 ## $ Sex &lt;fctr&gt; Male, Female, Male, Female, Female, Male, Female, ... ## $ Drug &lt;fctr&gt; A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B ## $ Glycolipid &lt;dbl&gt; 142.9, 140.6, 144.7, 144.0, 142.4, 146.0, 149.1, 15... There are four variables in this data set: Patient indexes the patient identity, Sex is the sex of the patient (we don’t need this), Drug denotes the drug treatment, and Glycolipid is the glycolipid level. Next, we need to calculate the differences between each pair. We can do this with the dplyr functions group_by and summarise: glycolipid_diffs &lt;- glycolipid %&gt;% group_by(Patient) %&gt;% summarise(Difference = diff(Glycolipid)) That might look a little cryptic. What we did was group the data by the values of Patient, and then used a function called diff to calculate the difference between the two Glycolipid concentrations within each patient. We stored the result of this calculation in a new data frame called glycolipid_diffs. This is the data we’ll use to carry out the paired-sample t-test: glycolipid_diffs ## # A tibble: 8 × 2 ## Patient Difference ## &lt;int&gt; &lt;dbl&gt; ## 1 1 -3.8 ## 2 2 -4.2 ## 3 3 2.6 ## 4 4 -4.6 ## 5 5 0.6 ## 6 6 -3.8 ## 7 7 -7.0 ## 8 8 -2.5 We should try to check that the differences could plausibly have been drawn from a normal distribution, but to be honest, normality is quite hard to assess with only 8 observations: ggplot(glycolipid_diffs, aes(x = Difference)) + geom_dotplot() + theme_grey(base_size = 22) ## `stat_bindot()` using `bins = 30`. Pick better value with `binwidth`. Figure 14.3: Within-individual differences from glycolipid study That’s an uninspiring figure, but the data seem OK, so let’s carry out a one-sample t-test on the calculated differences (remember, the null hypothesis is one where the population mean is zero). This is test is easy to do in R: t.test(glycolipid_diffs$Difference) ## ## One Sample t-test ## ## data: glycolipid_diffs$Difference ## t = -2.6209, df = 7, p-value = 0.03436 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -5.397549 -0.277451 ## sample estimates: ## mean of x ## -2.8375 We don’t have to set the data argument to carry out a one-sample t-test on the differences. We just passed along one argument: numeric vector of differences extracted from glycolipid_diffs using the $ operator. What happened to the mu argument used to set up the null hypothesis? R assumes that this is 0 if we don’t supply it, so no need to set it here. The output has much the same structure as with two-sample t-tests… The first line reminds us what kind of test we did, and the second line reminds us what data we used to carry out the test. It is the third line that matters: t = -2.6209, df = 7, *p*-value = 0.03436. This gives the t-statistic, the degrees of freedom, and the all-important p-value associated with the test. The p-value is tell us that the mean within-individual difference is significant at the p &lt; 0.05 level. The next line (alternative hypothesis: true mean is not equal to 0) reminds us that R has tested whether the population mean is equal to a value of zero, versus the alternative possibility that it is not equal to (i.e. greater or less than) zero. We now need to express these results in a clear sentence incorporating the relevant statistical information to indicate whether we accept or reject our test hypothesis: Individual patients had significantly lower serum glycolipid concentrations when treated with Drug B than when treated with Drug A (t = 2.62, d.f. = 7, p = 0.034). Notice tha twe chose to report the actual p-value here because it is close to the p &lt; 0.05 significance level. There’s nothing wrong with writing ‘p &lt; 0.05’ instead, but ‘p = 0.034’ gives the reader a bit more information. There are a couple of things to point out in interpreting the result of such a test. The sample of differences was used in the test, not the sample of paired observations. This means the degrees of freedom for a paired-sample t test are one less than the number of differences (the number of pairs); not one, or two, less than the total number of values. Note that since we have used a paired-sample design our conclusion stresses the fact that the use of the Drug B results in a lower glycolipid level in individual patients; it doesn’t say that the use of Drug B resulted in lower glycolipid concentrations for everyone given Drug B than for anyone given Drug A. 14.4.1 Using the paired = TRUE argument As mentioned, R does have a built in procedure for doing paired-sample t-tests. Now that we’ve done it the hard way, let’s try carrying out the test using that procedure. This looks very similar to a two-sample t-test, except that now we have to set the paired argument of the t.test function to TRUE: t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE) ## ## Paired t-test ## ## data: Glycolipid by Drug ## t = 2.6209, df = 7, p-value = 0.03436 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.277451 5.397549 ## sample estimates: ## mean of the differences ## 2.8375 Notice that we work with the original glycolipid data frame rather than the glycolipid_diffs data frame we constructed above. R takes care of the differencing for us. Order matters Be careful when using the built in procedure for doing paired-sample t-tests. The only information R uses to associate pairs of observations is their order in each group. The first observation in the ‘A’ group is paired with the first observation in the ‘B’ group, the second observation in the ‘A’ group is paired with the second observation in the ‘B’ group, and so on. If the items/individuals aren’t ordered the same way in each group, you’ll end up with a meaningless p-value… R makes it easy to do paired-sample t-test. It is up to you which method you use for doing paired-sample t-tests, just don’t forget it is really only a one-sample test wearing fancy clothes. This kind of experimental design is called a cross-over study. It can be problematic if, for example, “carry-over” effects occur, e.g., the effect of one drug is altered when the other drug has previously been administered. We won’t worry about these problems here though.↩ "],
["one-tailed-vs-two-tailed-tests.html", "Chapter 15 One-tailed vs. two-tailed tests 15.1 Introduction 15.2 An example of a one-tailed hypothesis 15.3 So how do we perform a one-tailed t-test?", " Chapter 15 One-tailed vs. two-tailed tests 15.1 Introduction The tests we have considered so far have been appropriate for questions of the type: ‘is the population mean, or the difference between population means, different from some value’ (often this value is 0, but it doesn’t have to be). Usually, we simply want to know whether there is any difference at all between means. If our test revealed a difference between the populations, we would be interested in it whichever way round it was. A test of this sort is called a two-tailed test. However, there are occasions when we may wish to test a more specific hypothesis. For example if we have measured a variable in two groups, A and B, we might only be interested in whether the population mean of A is bigger than that of B. That is, we set out to evaluate the specific hypothesis that the mean of A is bigger than the mean of B. A test this sort of directional hypothesis is called a one-tailed test. Let’s drill down into this idea a little more… 15.2 An example of a one-tailed hypothesis A farmer has been persuaded to try a new pesticide called Toxic Death on his broad bean crop. He sprays 20 fields of beans with Toxic Death and leaves 20 fields untreated. To test the effectiveness of Toxic Death he is only interested in detecting a positive response in his crop. It makes no difference to the farmer if the pesticide has no effect or proves to reduce the crop yield: in either case the product is a waste of money to the farmer and he will not use it again. The farmers’ test hypothesis is quite specific, in terms of the direction of the effect that is being tested for: ‘Toxic Death increases the mean yield of broad beans’. This is what is meant by a one-tailed test. In a one-tailed test we are only interested in a positive response, or a negative response. One rapidly descends into a philosophical quagmire when considering what ‘only interested in’ actually means, but let’s work with that definition for now. Where can we use a one-tailed test? In what follows we’re only going to consider one- vs. two-tailed tests in the context of t-tests. However, keep in mind that although the one- vs. two-tail distinction mostly pops up when discussing t-tests, it is relevant to some other kinds of statistical tests. 15.3 So how do we perform a one-tailed t-test? We can use of any of the t-tests we’ve seen (one-sample, two-sample, paired-sample) to examine an hypothesis about means where the direction of the effect is pre-specified. We are interested in testing the hypothesis ‘Toxic Death increases the mean yield of broad beans’ since this is the hypothesis of greatest relevance to the farmer. If the yield of beans in the two treatments was as illustrated below… Figure 15.1: Toxic death data …we would not even need to perform a test. The mean yield in the Toxic Death treatment is actually lower than in the control — we can automatically reject the hypothesis that treated fields have higher yield. The Toxic Death salesman might be in for some of his own medicine! However, if the results indicated that the mean yield was greater where Toxic Death was used we then need to to perform the test to determine how confident we can be that this is a real increase rather than a consequence of sampling variation. The aim is to compare two independent means, so we would use a two-sample t-test. The one-tailed version of the test is no different from the two-tailed version, but for one small tweak: when we come to find the p-value value to judge the significance of the test, the correct probability for a one-tailed test is half that found for the two-tailed test. So, suppose we had performed a two-tailed test (i.e. a test of the hypothesis ‘Toxic Death changes the yield of broad beans’––no direction specified) and found a positive effect of Toxic Death, but with a probability p = 0.08. Performing a one-tailed test of the hypothesis ‘Toxic Death increases the yield of broad beans’ would give a probability of exactly half this (p = 0.04). In this case using the two-tailed test we would have concluded that there was no significant effect of Toxic Death on the yield of broad beans (p = 0.08), whereas with the one-tailed test we would conclude that Toxic Death significantly increased the yield of broad beans (p = 0.04). This is a rather striking change of conclusion, which may seem like a fiddle14. It is not a fiddle (at least not if used properly). But because using a one- rather than two-tailed test can alter the conclusion we draw, such tests should be used with caution, and the rules about how and when to use them strictly adhered to. These rules are discussed below. First, we will see how to actually do a one-tailed test in R. 15.3.1 Carrying out one-tailed t-tests in R You should work through the example in this section. We said one-tailed tests were not a different sort of t-test to those we’ve seen so far—we can do one-tailed tests with any of these t-tests. We’ll show you how to do it using one example, the paired-sample t-test, applied to drug data. Let’s go back to the data on glycolipid concentrations in eight patients being treated with Drugs A and B. Imagine now that rather than A and B being two new drugs, Drug A is the existing treatment for the disease, while Drug B is a new type of drug being tested for effectiveness. In this case the drug company is obviously only interested in whether the new drug causes a greater reduction in the glycolipid levels of individual patients than the old one. If it has the same effect, or if it is less effective than the existing treatment it will not be worth spending time and money developing to the production stage. So the company’s test hypothesis is: ‘The new drug (B) is more effective than the existing treatment (A) at reducing glycolipid levels’. Let’s test this. Download the GLYCOLIPID.CSV file from MOLE again if you don’t have it and place it in your working directory. Read GLYCOLIPID.CSV into an R data frame, giving it the name glycolipid, then remind yourself what the data looks like, e.g. glimpse(glycolipid) ## Observations: 16 ## Variables: 4 ## $ Patient &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8 ## $ Sex &lt;fctr&gt; Male, Female, Male, Female, Female, Male, Female, ... ## $ Drug &lt;fctr&gt; A, A, A, A, A, A, A, A, B, B, B, B, B, B, B, B ## $ Glycolipid &lt;dbl&gt; 142.9, 140.6, 144.7, 144.0, 142.4, 146.0, 149.1, 15... We’re going to use a paired-sample t-test again in this example. We just want to rerun the previous analysis using a one-tailed version of the test. In order to do this we have to set one more argument in the t.test function, called alternative. This can take one of three values: &quot;two.sided&quot;, &quot;less&quot;, or &quot;greater&quot;. The first option is the default used by the t.test function. We pick a one-side test with an associated direction of effect by choosing “less” or “greater”. Here is how it works: t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE, alternative = &quot;greater&quot;) ## ## Paired t-test ## ## data: Glycolipid by Drug ## t = 2.6209, df = 7, p-value = 0.01718 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## 0.7863436 Inf ## sample estimates: ## mean of the differences ## 2.8375 Most of that output should seem quite familiar by now. However, why did we set the alternative to “greater”? We wanted to assess whether drug B really leads to lower glycolipid concentrations. Let’s make sure we understand what is going on… Look carefully at the mean of the differences in the output. This is a positive number: it looks like R has assumed we want to examine the ‘Drug A - Drug B’ differences between pairs. It does this because the letter ‘A’ comes before the letter ‘B’ in the alphabet. That’s all! It doesn’t actually matter which way round we calculate the differences—the t-statistic will be the same. However, we do have to be very careful to ensure that the direction of the alternative hypothesis we choose is correct. It is very easy to get this wrong if we are not paying attention, which is why R always prints the mean.15 Compare this with the output from the two-tailed paired-sample t-test on the glycolipid data (we just drop the alternative = &quot;greater&quot; argument): t.test(Glycolipid ~ Drug, data = glycolipid, paired = TRUE) ## ## Paired t-test ## ## data: Glycolipid by Drug ## t = 2.6209, df = 7, p-value = 0.03436 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## 0.277451 5.397549 ## sample estimates: ## mean of the differences ## 2.8375 The output differs in two respects: The alternative hypothesis in the one-tailed test (the alternative hypothesis: line) is: true difference in means is greater than 0. This tells us that R has tested whether the mean of our sample of differences is greater than zero16. The p-value in the one-tailed test is exactly half the value it is in two-tailed test, i.e. the result is ‘more significant’ when done as a one-tailed test. This is the primary advantage of the one-tailed test: it has more statistical power. Also note that in this case, the drug effect was significant in both one- and two-tailed tests, but it is not always so. Our conclusion from this test would be: Individual patients had significantly lower serum glycolipid concentrations when treated with Drug B than when treated with Drug A (one-tailed test: t=2.62, d.f.=7, p=0.017). Note two things about the conclusion. First, we should specify that a one-tailed test was used. If no information is given it is conventional to assume a two-tailed test has was used. Second, it is sensible to put the actual probability level, rather than just giving the category for p. This is because if anyone does then want to see what the significance of the two-tailed test would have been, they can easily double the probability, which cannot be done if we just say p &lt; 0.05. 15.3.2 When to use, and not to use, one-tailed t-tests Whether we use a one- or two-tailed tests can sometimes appear to change our conclusions rather dramatically… There is an obvious temptation here! It would be easy to collect data, determine which mean value is larger, and then test for differences in that direction using a one-tailed test.This would increase the apparent significance of the results, but it is a fundamentally flawed analysis. Why? Well, if we do this we are implicitly doing a two-tailed test—we are going to test the effect whichever direction it goes in—while using the extra power of a one-tailed test by pretending only one direction was ever going to be considered. It is very important to get clear in our mind about what one-tailed tests do and when (if ever) we might legitimately use them. The key principle is that the direction of the predicted effect must be specified before the data are collected. We are then effectively forfeiting the right to test for differences in the opposite direction to that predicted—we are putting all our statistical eggs in one basket. We are saying we are not interested in testing the result if it is in the opposite direction to that we predicted. That’s a very strong stance to adopt. What this means is that one-tailed tests are never appropriate in investigative research. Just because we have an idea about which direction we think an experimental result might go in is not a good reason to just test for that and effect in that direction. Instead we need to ask whether or not we would genuinely be prepared to ignore a result in the other direction. Usually the answer is no. For example, if we dose the soil in which experimental plants are growing with a low concentration of a particular compound we suppose will be toxic to them, we might expect that their growth will be reduced. However, if in fact they show higher growth with the compound than without it, we would almost certainly want to test to see if this was a real effect17. We would, therefore, be better off using a two-tailed test. Similarly in the case of testing a new drug against an existing treatment, on the face of it we may primarily be interested in whether the new treatment is better than the old one, and might consequently think of the analysis as one-tailed. However, we may also be interested in whether the new treatment is actually worse that the old one, rather than simply the same—i.e. an effect in the opposite direction to that we predict. Why? Well perhaps the new treatment has fewer side effects, so even if it is only of the same efficacy as the old one, it may still be preferable. However, we would most definitely want to ensure that it was no worse. oo a two-tailed test is probably still be the most appropriate analysis. So if there are so many problems why use them? There are situations where we are only interested in the direction of the effects, rather than understanding mechanisms. Here one-tailed tests are a useful tool. Testing medical or veterinary interventions for efficacy might be one (as discussed above). Another is in situations such as industrial processes: if we are in charge of managing the production of blood test kits, and we are considering a change to the production process you might want to sample the production line under the old and new systems and test whether the new system has a lower failure rate. We are only interested in an improvement—if the change has a higher failure rate, or simply makes no difference, we are not going to convert the entire production process to the new system. Here the extra power to detect an effect in a specified direction would certainly be worth considering18. One-tailed tests have their uses (and you will see them in statistics books and used in biological studies, so you need to know what they are) but they should be used with caution. The default procedure should be to use a two-tailed test unless there are very good reasons for doing otherwise. For one, the p &lt; 0.05 threshold is just a convention↩ This is also another reason for why it is important to really understand your data before you start analysing it. You have been warned!↩ Remember, the differences are ‘the wrong way around’ relative to our scientific hypothesis, so this is the right test.↩ Perhaps the compound also contains important trace nutrients, or affects the microbial community in the soil.↩ In this situation we might also need to consider whether a p &lt; 0.05 threshold is really appropriate. If it is going to cost millions of pounds to alter the industrial processes, do we really want to make such a change when our test has a 1 in 20 false positive rate?↩ "],
["working-with-frequencies.html", "Chapter 16 Working with frequencies 16.1 Introduction 16.2 A new kind of distribution 16.3 Types of test", " Chapter 16 Working with frequencies 16.1 Introduction Much of the time in biology we are dealing with whole objects (plants, animals, cells, eggs, islands, etc.) or discrete events (attacks, matings, nesting attempts, etc.). We are often interested in making measurements of numeric variables (length, weight, number, etc.) and then either comparing means from samples (e.g. mean leaf size of plants from two habitat types), or investigating the association between different measurements (e.g. mean leaf size and herbivore damage). However, we sometimes find a situation in which the ‘measurement’ we are interested in is not a quantitative measure (i.e. is not on a ratio or interval, scale), but is categorical. You will recall that categorical data are things like sex, colour or species. Such variables cannot be treated in the same way as numeric variables because although we can ‘measure’ each object (e.g. record if an animal is male or female), obviously we can’t calculate numeric quantities such as the ‘mean colour morph’, ‘mean species’ or ‘median sex’ of animals in a sample. Instead, we work with the observed frequencies, in the form of counts, of different categories, or combinations of categories. 16.2 A new kind of distribution There are a quite a few options for dealing with categorical data19. We’re just going to look at one option in this book: \\(\\chi^2\\) tests. This is pronounced, and sometimes written, ‘chi-square.’20. This isn’t necessarily the best approach for every problem, but \\(\\chi^2\\) tests are widely used in biology so they are a good place to start. It is not critical that you understand everything in this section. This material is here to help those who like to have a sense of how statistical tests work. You certainly won’t be assessed on it. The \\(\\chi^2\\) tests that we’re going to study borrow their name from a particular theoretical distribution, called, you guessed it, the \\(\\chi^2\\) distribution. We aren’t going to study this in much detail. However, just as with the normal distribution and the t-distribution, it can be helpful to know a little bit about it. The \\(\\chi^2\\) distribution pops up a lot in statistics. However, in contrast to the normal distribution, it isn’t often used to model the distribution of a variable we’ve sampled (i.e. ‘data’). Instead, the \\(\\chi^2\\) distribution is usually associated with a test statistic of some kind. The standard \\(\\chi^2\\) distribution is completely described by one parameter, called the degrees of freedom. Yes, this is closely related to the degrees of freedom idea introduced in the last few chapters on t-tests. The \\(\\chi^2\\) distribution is appropriate for positive-valued numeric variables. That is, negative values can’t be accommodated. This is because the \\(\\chi^2\\) distribution arises when we take one or more normally distributed variables, square these, and then add them up. Things are a bit more complicated than this in reality, but that’s the basic idea behind where it comes from. Let’s take a look at the \\(\\chi^2\\) distribution with one degree of freedom: Figure 16.1: Distribution of a large sample of chi-square distributed variable with one degree of freedom As we just pointed out, only positive values are allowed. We can also see that the distribution is asymmetric—it is skewed to the right—and most values lie between about 0 and 10. OK, so why is any of this useful? Let’s look at the plant morph example again. Imagine that we are able to take repeated samples from a population when the purple morph frequency is 25% . We’ll take a sample of 1000 plants each time21. If we know the true frequency, we expect to sample 250 plants each time. We’ll call this number the ‘expected value’. The observed frequency of purple plants in each sample will vary because of sampling error. We’ll call this latter number the ‘observed value’. So far we’re not doing anything we haven’t seen before. Here’s the new bit… Imagine that every time we sample the 1000 plants, we calculate the following test statistic: \\[2*\\frac{(O-E)^{2}}{E}\\] …where \\(O\\) is the observed value and \\(E\\) is the expected value defined above. What does the distribution of this test statistic look like? We can find out by simulating the scenario in R and plotting the results: Figure 16.2: Distribution of the test statistic That looks a lot like the theoretical \\(\\chi^2\\) distribution we plotted above. This wasn’t just good luck. It turns out that observed frequencies (‘counts’) that have been standardised with respect to their expected values—via the \\(\\frac{(O-E)^{2}}{E}\\) statistic—are expected to have a \\(\\chi^2\\) sampling distribution (at least approximately). This is the basis for using the \\(\\chi^2\\) distribution in various statistical tests involving categorical variables and frequencies. 16.3 Types of test We’re going to learn about two different types of \\(\\chi^2\\) test. Although the two tests work on the same general principle, it is still important to distinguish between them according to where they are used: 16.3.1 \\(\\chi^{2}\\) goodness of fit test A goodness-of-fit test is applicable in a situation where we have a single categorical variable (with any number of categories in it) and some hypothesis from which we can predict the expected proportions of observations falling in each category. For example… We might be interested in whether the number of males is equal to the number of females among second year students doing biology at Sheffield. We could record the numbers of males and females in a cohort, ending up with a sample containing one nominal variable (Sex) with only two categories (Male and Female). We want to know if there is any evidence for sex-related bias in the decision to study biology. Based on information about human populations, we know that the sex ratio among 18 year olds is fairly close to 1:122. We are thus able to compare the goodness of fit of the number of males and females in a sample of students with the expected value predicted by the 1:1 ratio. If we had a total of 164 students we might get this sort of table: Male Female Observed 64 100 With a 1:1 sex ratio, if there is no sex-bias in the decision to go to university and to study biology, we would expect 82 of each sex. In this case it looks as though there may be some discrepancy between the expected values and those actually found. However, this discrepancy could be entirely consistent with sampling variation—perhaps females are no more likely to choose biology and we ended up with a higher proportion of female students by chance. The \\(\\chi^{2}\\) goodness of fit test allows us to test how likely it is that such a discrepancy has arisen through sampling variation. 16.3.2 \\(\\chi^{2}\\) contingency table test A contingency table test is applicable in situations where each object is classified according to more than one categorical variable. Contingency table tests are usually used to test whether there is an association (or conversely, independence) between the variables. For example… Consider biology students again. We might be interested in whether eye colour was in any way related to sex. It would be simple to record eye colour (e.g. brown vs. blue) along with the sex of each student in a sample. Now we would end up with a table that had two classifications: Blue eyes Brown eyes Male 44 20 Female 58 42 Now it is possible to compare the proportions of brown and blue eyes among males and females… The total number of males and females are 64 and 100, respectively. The proportion of males with brown eyes is 20/64 = 0.31, and that for females 42/100 = 0.42. It appears that brown eyes are somewhat less prevalent among males. A contingency table test will tell us if the difference in eye colour frequencies is likely to have arisen through sampling variation. Notice that we are not interested in judging whether the proportion of males (or the proportion of blue-eyed students) are different from some expectation. That’s the job of a goodness of fit test. We want to know if there is an association between eye colour and sex. That’s the job of a contingency table test. 16.3.3 The assumptions and requirements of \\(\\chi^{2}\\) tests It’s important to realise that in terms of the assumptions they rely on, contingency table and goodness-of-fit tests aren’t fundamentally different from one another. The difference between the two types lies in the type of hypothesis evaluated. The only real difference is that when we carry out a goodness-of-fit test we have to supply the expected values, whereas the calculation of expected values is embedded in the formula used to carry out a contingency table test. That will make more sense once we’ve seen the two test in action… \\(\\chi^{2}\\) tests are often characterised non-parametric tests because they do not assume any particular form for the distribution of the data. In fact, as with any statistical test, there are some assumptions, but these are relatively mild: The data are independent counts of objects or events which can be classified into mutually exclusive categories. The expected counts are not very low. The general rule of thumb is that the expected values should be greater than 5. (We’ll explain exactly what we mean by ‘expected values’ later.) The most important thing to remember about \\(\\chi^{2}\\) tests is that they must always be carried out on the actual counts. Although the \\(\\chi^{2}\\) is really telling us how the proportions of objects in categories vary, the analysis should never be carried out on the percentages or proportions, only on the original count data. Similarly, \\(\\chi^{2}\\) cannot be used with means. e.g. the ‘log-linear model’, ‘Fisher’s exact test’, and the ‘G-test’.↩ The ‘ch’ is a hard ‘ch’, as in ‘character’.↩ The actual frequency and sample size we use isn’t all that important by the way. This example works for a wide range of situations.↩ Human sex-ratio is actually slightly biased toward males at birth, but since males experience a higher mortality rate in their teens, the sex ratio among 18 year olds is closer to 1:1.↩ "],
["goodness-of-fit-tests.html", "Chapter 17 Goodness of fit tests 17.1 When do we use a chi-square goodness of fit test? 17.2 How does the chi-square goodness of fit test work? 17.3 Carrying out a chi-square goodness of fit test in R 17.4 Determining appropriate expected values", " Chapter 17 Goodness of fit tests 17.1 When do we use a chi-square goodness of fit test? A \\(\\chi^{2}\\) goodness of fit test is appropriate in situations where we are studying a categorical variable and we want to to compare the frequencies of each category to a set of expected values. Here are a couple of examples: We’ve already seen a situation where a goodness of fit test might be useful: the analysis of the sex ratio among biology undergraduates. We have a prior prediction about what the sex ratio should be in the absence of bias (1:1), and we wanted to know if there was any evidence for sex-related bias in the decision to study biology. We could use a goodness of fit test to compare of the number of males and females in a sample of students with the predicted values to determine whether the data are consistent with the prediction. Red campion (Silene dioica) has separate male (stamen bearing) and female (ovary and stigma bearing) plants. Both sexes can be infected by the anther smut Ustillago violacea. This smut produces spores in the anthers of the plant which are then transported to other host plants by insect vectors. On infecting the female flowers, Ustillago causes their sex to change, so their flowers produce anthers containing infective spores. In populations of Silene in which there is no infection by Ustillago the ratio of male to female flowers is 1:1. Significant amounts of infection by the fungus may be indicated if there is an increase in the proportion of apparently male flowers relative to the expected 1:1 ratio. The two examples considered above are about as simple as things get: there are only two categories (i.e. Males and Females) and we expect a 1:1 ratio. The \\(\\chi^{2}\\) goodness of fit test can also be employed in more complex settings… The test can be applied to any number of categories. For example, we might have a diet choice experiment where squirrels are offered four different food types in equal proportions, and the chosen food is recorded over a short period. The study variable would then have four categories: one for each food type. The expected ratio need not be 1:1. For example, the principles of Mendelian genetics predict that the offspring of two plants which are heterozygous for flower colour (white recessive, pink dominant) will be either pink or white flowered, in the ratio 3:1. Plants from a real breeding experiment could be tested against this expected ratio. 17.2 How does the chi-square goodness of fit test work? The \\(\\chi^{2}\\) goodness of fit test uses raw counts to address questions about proportions, or probabilities of events23. As always, we start by setting up the appropriate null hypothesis. This will be question specific, but it must always be framed in terms of ‘no effect’ or ‘no difference’. We then work out what a sampling distribution of some kind looks like under this null hypothesis, and use this to assess how likely the observed result is (i.e. we calculate a p-value). In reality, we don’t need to work directly with the sampling distributions of counts in each category. Instead, we calculate an appropriate \\(\\chi^{2}\\) test statistic. The way to think about this is that the \\(\\chi^{2}\\) statistic reduces the information in the separate category counts down to a single number. Let’s see how the \\(\\chi^{2}\\) goodness of fit test works using the Silene example discussed above. Imagine we have collected data on the frequency of plants bearing male and female flowers in a population of Silene dioica: Male Female Observed 105 87 We want to test whether the ratio of males to female flowers differs significantly from that expected in an uninfected population. The ‘expected in an uninfected population’ situation is the null hypothesis for the test. Step 1. Calculate the counts expected when the null hypothesis is correct. This is the critical step. In the Silene example, we need to work out how many male and female plants we expected to sample if the sex ratio really were 1:1. These numbers are: (105 + 87)/2 = 192/2 = 96 of each sex24. Step 2. Calculate the \\(\\chi^{2}\\) test statistic from the observed and expected counts. We will show you how to do this later, but this calculation isn’t all that important, in the sense that we don’t learn much by actually doing it. The resulting \\(\\chi^{2}\\) statistic summarises—across all the categories—how likely (or unlikely) the observed data are under the null hypothesis. Step 3. Compare the \\(\\chi^{2}\\) statistic to the theoretical predictions of the \\(\\chi^{2}\\) distribution to assess the statistical significance of the difference between observed and expected counts. The interpretation of this p-value is the same as for any other kind of statistical test: it is probability we would see the observed frequencies, or more extreme values, under the null hypothesis. 17.2.1 Assumptions of the chi-square goodness of fit test Now is a good time to remind ourselves about the assumptions of the \\(\\chi^{2}\\) goodness of fit test: The data are independent counts of objects or events which can be classified into mutually exclusive categories. For example, we couldn’t aggregate Silene sex data from different surveys unless we were absolutely certain each survey had sampled different populations. The expected counts are not too low. The rule of thumb is that the expected values (not the observed counts!) should be greater than 5. If any of the expected values are below 5 the p-values generated by the test start to become less reliable. 17.3 Carrying out a chi-square goodness of fit test in R Work through the example in this section. Let’s see how to use R to carry out a \\(\\chi^{2}\\) goodness of fit test with the Silene sex data. There is no need to download any data for this example. The data used in a \\(\\chi^{2}\\) goodness of fit test are so simple that we usually just include it in the R script. There is nothing stopping us from putting the data into a CSV file and reading it into R if we wanted to25. The first step is to construct a numeric vector (not a data frame) containing the observed frequencies for each category. We’ll call this observed_freqs: observed_freqs &lt;- c(105, 87) observed_freqs ## [1] 105 87 It doesn’t matter what order the two category counts are supplied in. Next, we need to calculate the expected frequencies of each category. Rather than expressing these as expected counts, R is expecting these to be proportions. Again, we need to construct a numeric vector containing this information. We’ll call this expected_probs: n_cat &lt;- length(observed_freqs) expected_probs &lt;- rep(1, n_cat) / n_cat expected_probs ## [1] 0.5 0.5 Finally, we use the chisq.test function to calculate the \\(\\chi^{2}\\) value, degrees of freedom and p-value. The first argument is the numeric vector of counts (i.e. the data) and the second is the expected proportions in each category: chisq.test(observed_freqs, p = expected_probs) ## ## Chi-squared test for given probabilities ## ## data: observed_freqs ## X-squared = 1.6875, df = 1, p-value = 0.1939 The vectors containing the data and expected proportions have to be the same length for this to work (R will throw an error otherwise). The output is straightforward to interpret. The first part (Chi-squared test for given probabilities) reminds us what kind of test we did. The phrase ‘for given probabilities’ is R-speak informing us that we have carried out a goodness of fit test. The next line (data: observed_freqs) reminds us about the data we used. The final line is the one we care about: X-squared = 1.6875, df = 1, p-value = 0.1939. This output shows us the \\(\\chi^{2}\\) test statistic (R uses an ‘X’ for this), the degrees of freedom associated with the test, and the p-value. Since p &gt; 0.05, we conclude that the sex ratio is not significantly different from a 1:1 ratio. Degrees of freedom for a χ2 goodness of fit test We need to calculate the degrees of freedom associated with the test: in a χ2 goodness-of-fit test these are n − 1, where n − 1 is the number of categories. This comes from the fact that we have to calculate one expected frequency per category (= n frequencies), but since the frequencies have to add up to the total number of observations, once we know n − 1 frequencies the last one is fixed. 17.3.1 Summarising the result Having obtained the result we need to write the conclusion. As always, we always go back to the original question to write the conclusion. In this case the appropriate conclusion is: The observed sex ratio of Silene dioica does not differ significantly from the expected 1:1 ratio (\\(\\chi^{2}\\) = 1.69, d.f. = 1, p = 0.19). 17.3.2 A bit more about goodness of fit tests in R There is a useful shortcut that we can often employ when using R when we expect equal numbers in every category (as was the case above). In this case there is no need to calculate the expected proportions as R will assume we meant to use the ‘equal frequencies’ null hypothesis. So the following… chisq.test(observed_freqs) ## ## Chi-squared test for given probabilities ## ## data: observed_freqs ## X-squared = 1.6875, df = 1, p-value = 0.1939 …is exactly equivalent to the longer method we used first. We showed you the first method in case you ever need to carry out a goodness of fit test assuming unequal expected proportions. Note that R will warn you if it thinks the data are not suitable for a \\(\\chi^{2}\\) test. It is sometimes safe to ignore the warnings produced by R, but this is one situation where you should pay attention to it. We can see what this warning looks like by using chisq.test with a fake data set: chisq.test(c(2,5,7,5,5)) ## Warning in chisq.test(c(2, 5, 7, 5, 5)): Chi-squared approximation may be ## incorrect ## ## Chi-squared test for given probabilities ## ## data: c(2, 5, 7, 5, 5) ## X-squared = 2.6667, df = 4, p-value = 0.6151 The warning Chi-squared approximation may be incorrect is telling us that there might be a problem with the test. Can you see why R flagged a problem in this example? The expected counts are below 5, which means the p-values produced by chisq.test will not be reliable. 17.3.3 Doing it the long way… Nobody really does a \\(\\chi^{2}\\) test by hand these days. Nonetheless, just to prove that R isn’t really doing anything too clever in the background, let’s work through the calculations involved in goodness of fit test. The \\(\\chi^{2}\\) test statistic is found by taking the difference of each observed and expected count, squaring these differences, dividing each of these squared differences by the expected frequency, and finally, summing these numbers over the categories. That’s what this formula for the \\(\\chi^{2}\\) test statistic means: \\[\\chi^{2}=\\sum\\frac{(O_i-E_i)^{2}}{E_i}\\] In this formula the \\(O_i\\) are the observed frequencies, the \\(E_i\\) are the expected frequencies, the \\(i\\) in \\(O_i\\) and \\(E_i\\) refer to the different categories, and the \\(\\Sigma\\) means summation (‘add up’). Here’s how to apply this to our example: Work out how many male and female plants we would have sampled on average if the sex ratio really were 1:1. We already did this—the numbers are: (105 + 87)/2 = 192/2 = 96 of each sex. Calculate the \\(\\frac{(O_i-E_i)^2}{E_i}\\) term associated with each category. For the males, we have (105-96)2 / 96 = 0.844, and for females we (87-96)2 / 96 = 0.84426. The \\(\\chi^{2}\\) statistic is the sum of these two values. Here’s some R code that does this for us: Calculate observed and expected frequencies** O_freqs &lt;- c(105, 87) E_freqs &lt;- rep(sum(O_freqs)/2, 2) Calculate \\(\\chi^{2}\\) test statistic Chi_test_stat &lt;- sum((O_freqs-E_freqs)^2 / E_freqs) Chi_test_stat ## [1] 1.6875 Calculate degrees of freedom Chi_df &lt;- length(O_freqs) - 1 Chi_df ## [1] 1 Once we have obtained the \\(\\chi^{2}\\) value and the degrees of freedom we need to calculate the p-value associated with these values. Once upon a time we would have looked this up in a table of some kind. It is much easier to let R handle the calculations for us, using a function called pchisq. This does the ‘probability we would see the observed frequencies, or more extreme values’ calculation mentioned above. pchisq takes three arguments: the first is the \\(\\chi^{2}\\) value, the second is the degrees of freedom, and the third says which tail of the distribution it should use. Here’s how to calculate the required p-value from the \\(\\chi^{2}\\) and d.f. values we just calculated: pchisq(Chi_test_stat, Chi_df, lower.tail = FALSE) ## [1] 0.1939309 No surprises there… The p-value is the same as before. 17.4 Determining appropriate expected values Obviously unless you can find some expected values with which to compare your observed counts, a goodness of fit test will be of little use. Equally obviously, by using inappropriate expected values almost anything can be made significant! This means you need to have a good justifiable basis for choosing the expected values. In many cases the experiment can be designed, or the data collected, in such a way that if whatever it is we are interested in is not having an effect then we would expect to find equal numbers in each category. At other times the expectation can be generated by knowledge, or prediction, of a biological process e.g. a 1:1 sex ratio, a 3:1 ratio of phenotypes. At other times the expectation may need a bit more working out. We said it before, but it’s worth saying again. Do not apply the goodness of fit test to proportions or means.↩ Note that we actually work with expected proportions, not counts, when using R. These get converted to expected counts.↩ In general, it is a good idea to keep data and R code separate, but we sometimes break this rule for simple analyses.↩ These are the same because there are only two categories in this example.↩ "],
["contingency-tables.html", "Chapter 18 Contingency tables 18.1 When do we use a chi-square contingency table test? 18.2 How does the chi-square contingency table test work? 18.3 Carrying out a chi-square contingency table test in R 18.4 Working with larger tables", " Chapter 18 Contingency tables 18.1 When do we use a chi-square contingency table test? A \\(\\chi^{2}\\) contingency table test is appropriate in situations where we are studying two or more categorical variables—i.e., each object is classified according to more than one categorical variable—and we want to evaluate whether categories are associated. Here are a couple of examples: Going back to the data on biology students, we suggested that we might want to know if eye colour was related to sex in any way. That is, we want to know whether brown and blue eyes occur in different proportions among males and females. If we recorded the sex and eye colour of male and female students we could use a \\(\\chi^{2}\\) contingency table test to evaluate whether eye colour is associated with sex. The two-spot ladybird (Adalia bipunctata) occurs in two forms: the typical form, which is red with black spots and the dark form, which has much of the elytral surface black, with the two spots red. We want to know whether the relative frequency of the colour morphs is different in industrial and rural habitats. We could address this question by applying a \\(\\chi^{2}\\) contingency table test to an aggregate sample of two-spot ladybirds taken from rural and industrial habitats. Let’s think about what these kinds of data look like. Here are the biology student sex and eye colour data again, organised into a table: Blue eyes Brown eyes Male 22 10 Female 29 21 This is called a two-way contingency table. It is called a two-way contingency table because it summarises the frequency distribution of two categorical variables at the same time27. If we had measured three variables we would have ended up with a three-way contingency table (e.g. 2 x 2 x 2). A contingency table takes its name from the fact that it captures the ‘contingencies’ among the categorical variables: it summarises how the frequencies of one categorical variable are associated with the categories of another28. Associations are evident when the proportions of objects in one set of categories (e.g. R1 and R2) depends on a second set of categories (e.g. C1 and C2). Here are two possibilities: Table 18.1: Contingency tables without an association (left table), and with an association (right table), among two categorical variables C1 C2 R1 10 20 R2 40 80 C1 C2 R1 10 80 R2 40 20 In the first table (left) there is no association: the numbers in category R1 are 1/4 of those in category R2, whether the observations are in category C1 or C2. Notice that this reasoning isn’t about the total numbers in each category (e.g. there are 100 category C2 cases and only 50 category C1 cases). In the second table (right) this is evidence of an association: the proportion of observations in R1 changes markedly depending on whether we are looking at observations for category C1 or category C2. The R1 cases are less frequent in the C1 column, relative to the C2 column. Again, it is the proportions that matter, not the raw numbers. A variety of different questions can be asked of data in a contingency table, but most often, they are used to test for associations between the variables they summarise. That’s the topic of this chapter. There are different ways to carry out such tests of association. We’ll focus on the most widely used tool—the ‘Pearson’s Chi Square’ (\\(\\chi^{2}\\)) contingency table test29. 18.2 How does the chi-square contingency table test work? The \\(\\chi^{2}\\) contingency table test uses data in the form of a contingency table to address questions about the dependence between two or more different kinds of outcomes, or events. As always, we tackle this question by setting up the appropriate null hypothesis. This null hypothesis is always the same for the standard contingency table test of association: we assume the different kinds of events are independent of one another. This means we are assuming the occurrence of one kind of event does not depend on the other kind of event, i.e. they are not associated. Once the correct null hypothesis has been worked out, the remaining calculations are no different from those used in a goodness of fit test. We calculate the frequencies expected in each cell under the null hypothesis, we calculate a \\(\\chi^{2}\\) test statistic to summarise the mismatch between observed and expected values, and then use this to assess how likely the observed result is under the null hypothesis, resulting in the p-value. Let’s carry on with the two-spot ladybird (Adalia bipunctata) example from the beginning of this chapter. Here’s a bit more information… The dark (melanic) form is under the control of a single gene. Melanic and red types occur at different frequencies in different areas. Two observations are pertinent to this study: In London melanics comprise about 10%, whereas in rural towns in northern England the frequency is greater (e.g. Harrogate 63%, Hexham 75%). The frequency of melanics has decreased in Birmingham since smoke control legislation was introduced. It was thought that the different forms might be differentially susceptible to some toxic component of smoke, but this doesn’t explain the geographic variation in proportions of melanics. It turns out that the effect is a rather subtle one in which melanic forms do rather better in conditions of lower sunshine than red forms, due to their greater ability to absorb solar radiation. So where the climate is naturally less sunny melanics are favoured (giving geographic variation), but there will also be smaller scale variations due to local environmental conditions such as smoke, that affect solar radiation. To test whether this effect still occurs in industrial areas, a survey was carried out of Adalia bipunctata in a large urban area and the more rural surrounding areas. The following frequencies of different colour forms were obtained. Black Red Totals Rural 30 70 100 Industrial 115 85 200 Totals 145 155 300 We want to test whether the the proportions of melanics are different between urban and rural areas. In order to make it a bit easier to discuss the calculations involved we’ll refer to each cell in the table by a letter… Black Red Totals Rural \\(a\\) \\(b\\) \\(e\\) Industrial \\(c\\) \\(d\\) \\(f\\) Totals \\(g\\) \\(h\\) \\(k\\) We can now step through the steps involved in a \\(\\chi^{2}\\) contingency table test: Step 1. We need to work out the expected numbers in cells a-d, under the null hypothesis that the two kinds of outcomes (colour and habitat type) are independent of one another. If you happen to have studied a bit of basic probability theory at some point you might be able work these numbers out. We’ll demonstrate the logic of the calculation, though you don’t have to remember it. Let’s see how it works for the Black-Rural cell (‘a’): Calculate the probability that a randomly chosen individual in the sample is from a rural location (\\(p(\\text{Rural})\\)). This is the total ‘Rural’ count (\\(e\\)) divided by the grand total (\\(k\\)): \\[p(\\text{Rural}) = \\frac{e}{k}\\] Calculate the probability that a randomly chosen individual in the sample set is black (\\(p(\\text{Black})\\)). This is the total ‘Black’ count (\\(g\\)) divided by the grand total (\\(k\\)): \\[p(\\text{Black}) = \\frac{g}{k}\\] Calculate the probability that a randomly chosen individual is both ‘Rural’ and ‘Black’, assuming these events are ‘independent’ . This is given by the product of the probabilities from steps 1 and 2: \\[p(\\text{Rural, Black}) = \\frac{e}{k} \\times \\frac{g}{k}\\] Convert this to the expected number of individuals that are ‘Rural’ and ‘Black’, under the independence assumption. This is the probability from step 3 multiplied by the grand total (\\(k\\)): \\[E(\\text{Rural, Black}) = \\frac{e}{k} \\times \\frac{g}{k} \\times {k} = \\frac{g \\times e}{k}\\] In general, the expected value for any particular cell in a contingency table is given by multiplying the associated row and column totals and then dividing by the grand total. This is just a short cut for the full calculation we just worked through. Step 2 Calculate the \\(\\chi^{2}\\) test statistic from the four observed cell counts and their expected values. The resulting \\(\\chi^{2}\\) statistic summarises—across all the cells—how likely the observed frequencies are under the null hypothesis of no association (= independence). Step 3 Compare the \\(\\chi^{2}\\) statistic to the theoretical predictions of the \\(\\chi^{2}\\) distribution in order to assess the statistical significance of the difference between observed and expected counts. This p-value is the probability we would see the observed pattern of cell counts, or a more strongly associated pattern, under the null hypothesis of no association. A low p-value represents evidence against the null hypothesis. Method for hand calculation of 2 x 2 contingency tables For a 2 x 2 table there is a short cut method which is quicker than the general method outlined above. The formula for χ2 for a 2 x 2 table table (using the letter labels as in the table above) is: \\[\\chi^{2}=\\frac{k(bc-ad)^{2}}{efgh}\\] The only problem with this short cut method is that this formula does not show us what the expected values are. If we think it might be a problem, then we should pick the smallest column and row totals and calculate the expected value for the corresponding cell, using the formula above—this is the smallest expected value. 18.2.1 Assumptions of the chi-square contingency table test The assumptions of the \\(\\chi^{2}\\) goodness of fit test are essentially the same as those of the goodness of fit test: The observations are independent counts. For example, it would not be appropriate to apply a \\(\\chi^{2}\\) goodness of fit test where observations are taken before and after an experimental intervention is applied to the same objects. The expected counts are not too low. The rule of thumb is that the expected values (again, not the observed counts) should be greater than 5. If any of the expected values are below 5, the p-values generated by the test start to become less reliable. 18.3 Carrying out a chi-square contingency table test in R Walk through example You should work through the example in this section. Let’s carry on with the ladybird colour and habitat type example. You need to download three data sets to work through this section: LADYBIRDS1.CSV, LADYBIRDS2.CSV, and LADYBIRDS3.CSV. These all contain the same information—it is just organised differently in each case. Carrying out a \\(\\chi^{2}\\) contingency table test in R is very simple: we use the chisq.test function again. The only slight snag is that we have to ensure the data is formatted correctly before it can be used. Whenever we read data into R using read.csv we end up with a data frame. Unfortunately, the chisq.test function is one of the few statistical functions not designed to work with data frames. This means we first have to use a function called xtabs to construct something called a contingency table object.30. The xtabs function does categorical ‘cross tabulation’31, i.e., it sums up the number of occurrences of different combinations of categories among variables. We’ll look at how to use xtabs before running through the actual test… 18.3.1 Step 1. Getting the data into the correct format It is not difficult to use, but the precise usage of xtabs depends upon how the raw data are organised. We’ll examine the three main cases in turn. Case 1… Data suitable for analysis with a \\(\\chi^{2}\\) contingency table test are often represented in a data set with one column per categorical variable, and one row per observation. The LADYBIRDS1.CSV file contains the data in this format. Read it into an R data frame: lady_bird_df &lt;- read.csv(file = &quot;LADYBIRDS1.CSV&quot;) We called the data lady_bird_df to emphasise that they are stored in a data frame at this point. We can use glimpse, head and tail to get a sense of how the data are organised: glimpse(lady_bird_df) ## Observations: 300 ## Variables: 2 ## $ Habitat &lt;fctr&gt; Rural, Rural, Rural, Rural, Rural, Rural, Rural, Rura... ## $ Colour &lt;fctr&gt; Black, Black, Black, Black, Black, Black, Black, Blac... head(lady_bird_df, 10) ## Habitat Colour ## 1 Rural Black ## 2 Rural Black ## 3 Rural Black ## 4 Rural Black ## 5 Rural Black ## 6 Rural Black ## 7 Rural Black ## 8 Rural Black ## 9 Rural Black ## 10 Rural Black tail(lady_bird_df, 10) ## Habitat Colour ## 291 Industrial Red ## 292 Industrial Red ## 293 Industrial Red ## 294 Industrial Red ## 295 Industrial Red ## 296 Industrial Red ## 297 Industrial Red ## 298 Industrial Red ## 299 Industrial Red ## 300 Industrial Red We only showed you the first and last 10 values—you should take a full look at the data with the View function. You will see that the data frame contains 300 rows—one for each ladybird—and two variables (Habitat and Colour). The two variables obviously contain the information about the categorisation of each ladybird in the sample. We require a two-way table that contains the total counts in each combination of categories. This is what xtabs does. It takes two arguments: the first is a formula (involving the ~ symbol) that specifies the required contingency table; the second is the name of the data frame containing the raw data. When working with data in the above format—one observation per row—we use a formula that only contains the names of the categorical variables on the right hand side of the ~ (i.e., ~ Habitat + Colour): lady_bird_table &lt;- xtabs(~ Habitat + Colour, data = lady_bird_df) lady_bird_table ## Colour ## Habitat Black Red ## Industrial 115 85 ## Rural 30 70 When used like this, xtabs will sum up the number of observations with different combinations of Habitat and Colour. We called the output lady_bird_table to emphasise that the data from xtabs are now stored in a contingency table. When we print this to the console we see that lady_bird_table does indeed refer to something that looks like a 2 x 2 contincency table of counts. Case 2… Sometimes data suitable for analysis with a \\(\\chi^{2}\\) contingency table test are partially summarised into counts. For example, imagine that we had visited five rural sites and five urban sites and recorded the numbers of red and black colour forms found at each site. Data in this format are stored in the LADYBIRDS2.CSV file. Read this into an R data frame and examine this with the View function: lady_bird_df &lt;- read.csv(file = &quot;LADYBIRDS2.CSV&quot;) glimpse(lady_bird_df) ## Observations: 20 ## Variables: 4 ## $ Habitat &lt;fctr&gt; Rural, Rural, Rural, Rural, Rural, Rural, Rural, Rura... ## $ Site &lt;fctr&gt; R1, R2, R3, R4, R5, R1, R2, R3, R4, R5, U1, U2, U3, U... ## $ Colour &lt;fctr&gt; Black, Black, Black, Black, Black, Red, Red, Red, Red... ## $ Number &lt;int&gt; 10, 3, 4, 7, 6, 15, 18, 9, 12, 16, 32, 25, 25, 17, 16,... lady_bird_df ## Habitat Site Colour Number ## 1 Rural R1 Black 10 ## 2 Rural R2 Black 3 ## 3 Rural R3 Black 4 ## 4 Rural R4 Black 7 ## 5 Rural R5 Black 6 ## 6 Rural R1 Red 15 ## 7 Rural R2 Red 18 ## 8 Rural R3 Red 9 ## 9 Rural R4 Red 12 ## 10 Rural R5 Red 16 ## 11 Industrial U1 Black 32 ## 12 Industrial U2 Black 25 ## 13 Industrial U3 Black 25 ## 14 Industrial U4 Black 17 ## 15 Industrial U5 Black 16 ## 16 Industrial U1 Red 17 ## 17 Industrial U2 Red 23 ## 18 Industrial U3 Red 21 ## 19 Industrial U4 Red 9 ## 20 Industrial U5 Red 15 This time we printed at the whole dataset (it’s easier to use View, but that won’t render in this book). The counts at each site are in the Number variable, and the site identities are in the Site variable. We need to sum over the sites to get the total number within each combination of Habitat and Colour. We use xtabs again, but this time we have to tell it which variable to sum over: lady_bird_table &lt;- xtabs(Number ~ Habitat + Colour, data = lady_bird_df) lady_bird_table ## Colour ## Habitat Black Red ## Industrial 115 85 ## Rural 30 70 When working with data in this format—more than one observation per row—we use a formula where the name of the variable containing the counts is on left hand side of the ~, and the names of the categorical variables to sum over are on the right hand side of the ~ (i.e., Number ~ Habitat + Colour). When used like this xtabs will sum up the counts associated with different combinations of Habitat and Colour. The lady_bird_table object produced by xtabs is no different than before. Good! These are the same data. Case 3… Data suitable for analysis with a \\(\\chi^{2}\\) contingency table test are sometimes already summarised into total counts. Data in this format are stored in the LADYBIRDS3.CSV file. Read this into an R data frame and print it to the console (it’s very small, so there’s no need to use View): lady_bird_df &lt;- read.csv(file = &quot;LADYBIRDS3.CSV&quot;) lady_bird_df ## Habitat Colour Number ## 1 Industrial Black 115 ## 2 Industrial Red 85 ## 3 Rural Black 30 ## 4 Rural Red 70 The total counts are already in the Number variable so there is no real need to sum over anything to get the total for each combination of Habitat and Colour. However, we still need to convert the data from a data frame to a contingency table. There are various ways to do this, but it is easiest to use xtabs. The R code is identical to the previous case: lady_bird_table &lt;- xtabs(Number ~ Habitat + Colour, data = lady_bird_df) lady_bird_table ## Colour ## Habitat Black Red ## Industrial 115 85 ## Rural 30 70 In this case xtabs doesn’t change the data at all because it’s just ‘summing’ over one value in each combination of categories. We’re just using xtabs to convert it from a data frame to a contingency table. The resulting lady_bird_table object is the same as before. 18.3.2 Step 2. Doing the test Once we have the data in the form of a contingency table the associated \\(\\chi^{2}\\) test of independence between the two categorical variables is easy to carry out with chisq.test: chisq.test(lady_bird_table) ## ## Pearson&#39;s Chi-squared test with Yates&#39; continuity correction ## ## data: lady_bird_table ## X-squared = 19.103, df = 1, p-value = 1.239e-05 That’s all we have to do. Just pass one argument to chisq.test: the contingency table. This output should make sense in the light of what we saw in the previous chapter. R first prints a reminder of the test employed (Pearson's Chi-squared test with Yates' continuity correction) and the data used (data: lady_bird_table). We’ll come back to the “Yates’ continuity correction” bit in a moment. R then summarises the \\(\\chi^{2}\\) value, the degrees of freedom, and the p-value: X-squared = 19.103, df = 1, p-value = 1.239e-05 The p-value is highly significant (p&lt;0.001) indicating that the colour type frequency varies among the two kinds of habitats.32 Degrees of freedom for a χ2 contingency table test We need to know the degrees of freedom associated with the test: in a two-way χ2 contingency table test these are (nA − 1)×(nB − 1), where nA − 1 is the number of categories in the first variable, and nB − 1 is the number of categories in the second variable. So if we’re working with a 2 x 2 table the d.f. are 1, if we’re working with a 2 x 3 table the d.f. are 2, if we’re working with a 3 x 3 table the d.f. are 4, and so on. What was that “Yates’ continuity correction” all about? The reasoning behind using this correction is a bit beyond this course, but in a nutshell, it generates more reliable p-values under certain circumstances. By default, the chisq.test function applies this correction to all 2 x 2 contingency tables. We can force R to use the standard calculation by setting correct = FALSE if we want to: chisq.test(lady_bird_table, correct = FALSE) ## ## Pearson&#39;s Chi-squared test ## ## data: lady_bird_table ## X-squared = 20.189, df = 1, p-value = 7.015e-06 Both methods give similar results in this example, though they aren’t exactly the same—the \\(\\chi^{2}\\) value calculated when correct = FALSE is very slightly higher than the value found when using the correction. Don’t use the correct = FALSE option! The default correction is a safer option for 2 x 2 tables. 18.3.3 Summarising the result We have obtained the result so now we need to write the conclusion. As always, we go back to the original question to write the conclusion. In this case the appropriate conclusion is: There is a significant association between the colour of Adalia bipunctata individuals and habitat, such that black individuals are more likley to be found in industrial areas (\\(\\chi^{2}\\) = 19.1, d.f. = 1, p &lt; 0.001). Notice that we summarised the nature of the association alongside the statistical result. This is easy to do in the text when describing the results of a 2 x 2 contingency table test. It’s much harder to summarise the association in written form when working with larger tables. Instead, we often present a table or a bar chart showing the observed counts. 18.4 Working with larger tables Interpreting the results of a contingency table test is fairly straightforward in the case of a 2 x 2 table. We can certainly use contingency table tests with larger two-way tables (e.g. 3 x 2, 3 x 3, 3 x 4, etc), and higher dimensional tables (e.g. 2 x 2 x 2), but the results become progressively harder to understand as the tables increase in size. If you get a significant result, it is often best to compare the observed and expected counts for each cell and look for the highest differences to try and establish what is driving the significant association. Visualising the data with a bar chart can also help with interpretation. There are ways of subdividing large tables to make subsequent \\(\\chi^{2}\\) tests on individual parts of a table, in order to establish specific effects, but these are not detailed here. Unless we plan to make the more detailed comparisons before we started collecting the data, it is hard to justify this kind of post hoc analysis. This is called their ‘joint distribution’, in case you were wondering.↩ The term association is use here to describe the non-independence of categories among categorical variables. You may also come across many other terms used to refer to the same thing (e.g. linkage, heterogeneity, non-independence, and interaction), but we’ll generally stick with the word ‘association’.↩ This is the same Pearson who invented the correlation coefficient for measuring linear associations by the way.↩ The table objects produced by xtabs are not the same as the dplyr table-like objects: ‘tibbles’ and ‘tbls’. This is one of the reasons that dplyr adopted the name ‘tibble’. The overlap in names is unfortunate, but we’ll have to live with it—there are only so many ways to name things that look like tables.↩ Pivot tables can be used to do the same thing in Excel.↩ We could have summarised the result as: habitat type varies among the two colour types. This way of explaining the result seems odd though. Ladybirds are found within habitats, not the other way around. Just keep in mind that this is a semantic issue. The contingency table test doesn’t make a distinction between directions of effects.↩ "],
["relationships-and-regression.html", "Chapter 19 Relationships and regression 19.1 Introduction 19.2 What does linear regression do? 19.3 How does simple linear regression work? 19.4 What do you get out of a regression? 19.5 The assumptions of regression", " Chapter 19 Relationships and regression 19.1 Introduction The simple t-tests we encountered earlier were concerned with how to compare mean(s) of numeric variables. We learned how to: (1) compare one mean to any particular value via the one-sample t-test, (2) compare means among two groups or conditions via the two-sample t-test, (3) compare two conditions within items / objects via the paired-sample t-test. One way to think about the two-sample and paired-sample t-tests is that they evaluate whether or not the variable changes among two groups or experimental conditions. Membership of the different groups/conditions can be encoded by a categorical variable. In R, we use a formula involving the numeric (num_var) and categorical (cat_var) variables to set up the test (e.g. num_var ~ cat_var). There is a good reason for doing things this way. The formula reflects the fact that we can conceptualise the t-tests as considering a relationship between between a numeric and categorical variable33. This chapter introduces a different kind of relationship. The question we want to address is: what should we do when we want to understand how two numeric variables are related? Much of biology is concerned with relationships between numeric variables. For example… We sample fish and measure their length and weight because we want to understand how weight scales with respect to length. We survey grassland plots and measure soil pH and species diversity because we want to understand how species diversity depends on soil pH. We manipulate temperature and measure fitness in insects because we want to characterise their thermal tolerance. In each of these settings the goal is to understand how one numeric variable depends on the values of another. Graphically, we evaluate such dependencies using a scatter plot. We may be interested in knowing: Are the variables related or not? There’s not much point studying a relationship that isn’t there: Is the relationship positive or negative? Sometimes we can answer a scientific question just by knowing the direction of a relationship: Is the relationship a straight line or a curve? It is important to know the form of a relationship if we want to make predictions: Although sometimes it may be obvious that there is a relationship between two variables from a plot of one against the other, at other times it may not. Take a look at the following: We might not be very confident in judging which, if either, of these plots provides evidence of a positive relationship between the two variables. Maybe the pattern that we perceive can just be explained by sampling variation, or maybe it can’t. Clearly it would be useful to have a measure of how likely it is that the relationship could have arisen as a result of sampling variation. In addition to judging the statistical significance of a relationship, we may also be interested in describing the relationship mathematically – i.e. finding the equation of the best fitting line through the data. A linear regression analysis allows us to do all this. 19.2 What does linear regression do? Simple linear regression finds the straight-line relationship which best describes the dependence of one variable (the dependent variable) on the other (the independent variable). What does the word ‘simple’ mean here? A simple linear regression is a regression model which only accounts for one independent variable. If more than one independent variable is considered, the correct term to describe the resulting model is ‘multiple regression’. Multiple regression is a very useful tool but we’re only going to study simple regression in this book. What does the word ‘linear’ mean here? In statistics, the word linear is used in two slightly different, but closely related ways. When discussing simple linear regression the term linear is often understood to mean that the relationship follows a straight line. That’s all. The more technical definition concerns the relationship between the parameters of a statistical model. We don’t need to worry about that one here. Writing ‘simple linear regression’ all the time becomes tedious, so we’ll often write ‘linear regression’ or ‘regression’. Just keep in mind that we’re always referring to simple linear regression in this book. These regression models account for a straight line relationship between two numeric variables, i.e. they describe how the dependent variable changes in response to the values of the independent variable. It is conventional to label the dependent variable as ‘\\(y\\)’ and the independent variable as ‘\\(x\\)’. When we present such data graphically, the independent variable always goes on the \\(x\\)-axis and the dependent variable on the \\(y\\)-axis. Try not to forget this convention! ‘dependent vs. independent’ or ‘response vs. predictor’? Another way to describe linear regression is that it allows us to predict how one variable (the response variable) responds to another (the predictor variable). The dependent vs. independent and response vs. predictor conventions for variables in a regression are essentially equivalent. They only differ in the nomenclature they use to describe the variables involved. To avoid confusion, we will stick with dependent vs. independent naming convention in this course. How do we decide how to select which is to be used as the dependent variable and which as the independent variable? The decision is fairly straightforward in an experimental setting: the manipulated variable is the independent variable, and the measured outcome is the dependent variable. Consider the thermal tolerance example from earlier. Temperature was manipulated in this experiment, so it must be designated the independent variable. Moreover, a priori (before conducting the experiment), we can reasonably suppose that changes in temperature may cause changes in enzyme activity, but the reverse seems pretty unlikely. Things may not be so clear cut when we are working with data from an observational study as it may not be obvious that one variable depends upon the other (in a causal sense). These problems will be discussed in more detail in later chapters. There is one important point to be aware of now though: in regression it matters which way round we designate the dependent and independent variables. If you have two variables A and B, the relationship you find from a regression will not be the same for A against B as for B against A. 19.3 How does simple linear regression work? 19.3.1 Finding the best fit line If we draw a straight line through a set of points on a graph then, unless they form a perfect straight line, some points will lie close to the line and others further away. The vertical distances between the line and each point (i.e. measured parallel to the \\(y\\)-axis) have a special name. They are called the residuals. Here’s a visual example: Figure 19.1: Example of data (blue points) used in a simple regression. A fitted line and the associated residuals (vertical lines) are also shown In this plot the blue points are the data and the vertical lines represent the residuals. The residuals represent the variation that is ‘left over’ after the line has been fitted through the data. They give an indication of how well the line fits the data. If all the points lay close to the line the variability of of the residuals would be low relative to the variation in the dependent variable, \\(y\\). When the observations are more scattered around the line the the variability of the residuals would be large relative to the variation in the dependent variable, \\(y\\). Regression works by finding the line which minimises the size of the residuals in some sense. We’ll explain exactly how in a moment. The following illustration indicates the principle of this process: The data are identical in all four graphs, but in the top left left hand graph a horizontal line (i.e. no effect of \\(x\\) on \\(y\\)) has been fitted, while on the remaining three graphs sloping lines of different magnitude have been fitted. To keep the example simple, we assume we know the true intercept of the line, which is at \\(y=0\\), so all four lines pass through \\(x=0\\), \\(y=0\\) (the ‘origin’). Which line is best? One of the four lines is the ‘line of best’ fit from a regression analysis. Spend a few moments looking at the four figures. Which line seems to fit the data best? Why do you think this line is ‘best’? Let’s visualise the data, the candidate lines and the residuals: We said that regression works by finding the intercept and slope that minimises the vertical distances between the line and each observation in some way34. In fact, it minimises something called the ‘sum of squares’ of these distances: we calculate a sum of squares for a particular set of observations and a fitted line by squaring the residual distances and adding all of these up. This quantity is called the residual sum of squares. The line with the lowest residual sum of squares is the best line because it ‘explains’ the most variation in the dependent variable. You should be able to see that, for the horizontal line (‘A’), the residual sum of squares is larger than any of the other three plots with the sloping lines. This suggests that the sloping lines fit the data better. Which one is best among the three we’ve plotted? To get at this we need to calculate the residual sum of squares for each line. These are… ## Line Residual Sum of Squares ## 1 A 17.55067 ## 2 B 11.97966 ## 3 C 10.12265 ## 4 D 12.79674 So it looks like the line in panel C is the best fitting line among the candidates. In fact, it is the best fit line among all possible candidates. Did you manage to guess this by looking at the lines and the raw data? If not, think about why you got the answer wrong. Did you consider the vertical distances or the perpendicular distances? It is very important that you understand what a residual from a regression represents. Residuals pop up all the time when evaluating statistical models (not just regression). If you’re confused about what they represent be sure to ask a TA to explain them to you some time. 19.4 What do you get out of a regression? A regression analysis involves two activities: Interpretation. When we ‘fit’ a regression model to data we are estimating the coefficients of a best-fit straight line through the data. This is the equation that best describes how the \\(y\\) (dependent) variable depends on the \\(x\\) (independent) variable. To put it in slightly more technical terms, it describes the \\(y\\) variable as a function of the \\(x\\) variable. This model may be used to understand how the variables are related or make predictions. Inference. It is not enough to just estimate the regression equation. Before we can use it we need to determine whether there is a statistically significant relationship between the \\(x\\) and \\(y\\) variables. That is, the analysis will tell us whether an apparent association is likely to be real, or just a chance outcome resulting from sampling variation. Let’s consider each of these two activities… 19.4.1 Interpreting a regression What is the form of the relationship? The equation for a straight line relationship is \\(y = a + b \\times x\\), where \\(y\\) is the dependent variable, \\(x\\) is the independent variable, \\(a\\) is the intercept (i.e. the value at which the line crosses the \\(y\\) axis), and \\(b\\) is the slope of the line. The \\(a\\) and the \\(b\\) are referred to as the coefficients (or parameters) of the line. The slope of the line is often the coefficient we care about most. It tells us the amount by which \\(y\\) changes for a change of one unit in \\(x\\). If the value of \\(b\\) is positive (i.e. a plus sign in the above equation) this means the line slopes upwards to the right. A negative slope (\\(y = a - bx\\)) means the line slopes downwards to the right. The diagram below shows the derivation of an equation for a straight line. Having the equation for a relationship allows us to predict the value of the \\(y\\) variable for any value of \\(x\\). For example, in the thermal tolerance example, we want an equation that will allow us to work out how fitness changes with temperature. Such predictions can be made by hand (see below) or using R (details later). In the above diagram, the regression equation is: \\(y = 1 + 0.66 x\\). So to find the value of \\(y\\) at \\(x = 2\\) we use: \\(y = 1 + (0.667 \\times 2) = 2.32\\). Obviously, by finding \\(y\\) values for 2 (or preferably 3) different \\(x\\) values from the equation, the actual line can easily be plotted on a graph manually if required—plot the values and join the dots! It’s much easier to use R to do this kind of thing though. Regression involves a statistical model A simple linear regression is underpinned by a statistical model. If you skim back through the Parametric statistics chapter you will see that the equation y = a + b × x represents the ‘systematic component’ of the regression model. This bit describes the component of variation in y that is explained by the model for the dependence of y on x. The residuals correspond to the ‘random component’ of the model. These represent the component of variation in the y variable that our regression model fails to describe. 19.4.2 Evaluating hypotheses (‘inference’) There is more than one kind of significance test that can be carried out with a simple linear regression. We’re going to focus on the most useful test: the F test of whether the slope coefficient is significantly different from 0. How do we do this? We play exactly the same kind of ‘gambit’ we used to develop the earlier tests: We start with a null hypothesis of ‘no effect’. This corresponds to the hypothesis that the slope of the regression is zero. We then work out what the distribution of some kind of test statistic should look like under the null hypothesis. The test statistic in this case is called the F-ratio. We then calculate a p-value by asking how likely it is that we would see the observed test statistic, or a more extreme value, if the null hypothesis were really true. It’s really not critical that you understand the mechanics of an F-test. However, there are several terms involved that are good to know about because having some sense of what they mean may help to demystify the output produced by R. Let’s step through the calculations involved in the F test. We’ll use the example data shown in the four-panel plot from earlier to do this… Total variation First we need to calculate something called the total sum of squares. The figure below shows the raw data (blue points) and the grand mean (i.e. the sample mean). The vertical lines show the distance between each observation and the grand mean. These vertical lines are just the residuals from a model where the slope of the line is set to zero. What we need to do is quantify the variability of these residuals. We can’t just add them up, because by definition, they have to sum to zero, i.e. they are calculated relative to the grand mean. Instead we calculate the total sum of squares by taking each residual in turn, squaring it, and then adding up all the squared values. We call this the total sum of squares because it is a measure of the total variability in the dependent variable, \\(y\\). This number is 17.55 for the data in the figure above. Residual variation Next we need to calculate the residual sum of squares. We have already seen how this calculation works because it is used in the calculation of the best fit line—the best fit line is the one that minimises the residual sum of squares. Let’s plot this line along with the associated residuals of this line again: The vertical lines show the distance between each observation and the best fit line. We need to quantify the variability of these residuals. Again, we can’t just add up the deviations because they have to sum to zero as a result of how the best fit line is found. Instead we calculate the residual sum of squares by taking each residual in turn, squaring it, and then adding up all the squared values. This number is 10.12 for the figure above. We call this the residual, or error, sum of squares because it is a measure of the variation in \\(y\\) that is ‘left over’ after accounting for the influence of the independent variable \\(x\\). Explained variation Once the total sum of squares and the residual sum of squares are known, we can calculate the quantity we really want: the explained sum of squares. This is a measure of the variation in \\(y\\) that is explained by the influence of the independent variable \\(x\\). We calculate this by subtracting the residual sum of squares from the total sum of squares. This makes intuitive sense: if we subtract the variation in \\(y\\) we can’t explain (residual) from all the variation in \\(y\\) (total), we end up with the amount ‘explained’ by the regression. This number is 7.43 for the example. Degrees of freedom, mean squares and F tests The problem with sums of squares is that they are a function of sample size. The more data we have, the larger our sum of squares will get. The solution to this problem is to convert them into a measure of variability that doesn’t scale with sample size. We need to calculate degrees of freedom (written as df, or d.f.) to do this. We came across the concept of degrees of freedom when we studied the t-test. The idea is closely related to sample size. It is difficult to give a precise definition, but roughly speaking the degrees of freedom of a statistic is a measure of how much ‘information’ it contains. Each of the measures of variability we just calculated for the simple linear regression has a degrees of freedom associated with it. We need the explained and error degrees of freedom: Explained d.f. = 1 Error d.f. = (Number of observations - 2) Don’t worry if those seem a little cryptic. We don’t need to carry out degrees of freedom calculations by hand because R will do them for us. We’ll think about degrees of freedom a bit more when we start to learn about ANOVA models. The reason degrees of freedom matter is because we can use them to standardise the sum of squares to account for sample size. The calculations are very simple. We take each sum of squares and divide it by its associated degrees of freedom. The resulting quantity is called a mean square (it’s the mean of squared deviations): \\[ \\text{Mean Square} = \\frac{\\text{Sum of Squares}}{\\text{Degrees of Freedom}} \\] A mean square is actually an estimate of variance. Remember the variance? It is one of the standard measures of a distribution’s dispersion, or spread. Now for the important bit. The two mean squares can be compared by calculating the ratio between them, which is designated by the letter F: \\[F = \\mbox{Variance Ratio} = \\frac{\\mbox{Explained Mean Square}}{\\mbox{Error Mean square}}\\] This is called the F ratio, or sometimes, the variance ratio. If the explained variation is large compared to the residual variation then the F ratio will be large. Conversely, if the explained variation is relatively small then F will be small. We can see where this is heading… The F ratio is a type of test statistic—if the value of F is sufficiently large then we judge it to be statistically significant. In order for this judgement to be valid we have to make one key assumption about the population from which the data has been sampled: we assume the residuals are drawn from a normal distribution. If this assumption is correct, it can be shown that the distribution of the F ratio under the null hypothesis (the ‘null distribution’) has a particular form: it follows a theoretical distribution called an F-distribution. And yes, that’s why the variance ratio is called ‘F’. All this means we can assess statistical significance of the slope coefficient by comparing the F ratio calculated from a sample to this theoretical distribution. This procedure is called an F test. The F ratio is 7.34 in our example. This is quite high which indicates that the slope is likely to be significantly different from 0. However, in order to actually calculate the p-value we also need to consider the degrees of freedom of the test, and because the test involves an F ratio, there are two different degrees of freedom to consider: the explained and residual df’s. Remember that! We could go one to actually calculate the p-value, but it is much better to let R do this for us when we work directly with a regression model. We’ll leave significance tests alone for now… 19.5 The assumptions of regression Simple linear regression is a parametric statistical technique, i.e. it makes a number of assumptions that we should be aware of. Let’s consider each one in detail, in their approximate order of importance: Independence. The residuals must be independent. Another way of stating this assumption is that the value of each residual does not depend on the value of any others. This can be difficult to check. If the data are from a carefully designed experiment, everything should be OK. If the data are observational, then we need to be a lot more careful. This assumption matters because when the residuals are not independent any p-values we generate will be unreliable. Measurement scale. The dependent (\\(y\\)) and independent (\\(x\\)) variables are measured on an interval or ratio scale. It doesn’t really make sense to use categorical data in a regression35. This one is easy to assess. Linearity. The relationship between the independent \\(x\\) variable and the dependent \\(y\\) variable is linear. Obviously there is little point in fitting a straight line to data which clearly don’t form a straight line relationship. There may also be circumstances in which it is theoretically unlikely for a relationship to be linear, e.g. the length and weight of an animal will not be well described by a linear relationship because weight is roughly a cubic function of length. If the data fail this assumption then applying a mathematical transformation of \\(x\\) or \\(y\\) can help. We will come back to this idea later in the course. Constant variance. The variance of the residuals is constant. This assumption essentially means the variability of the residuals is not related to the value of the independent \\(x\\) variable. It is violated if the magnitude of the residuals increase or decrease markedly as \\(x\\) gets larger. If the data fail this assumption then again, sometimes applying a mathematical transformation of \\(y\\) will help. We will also discuss this idea later in the course. Normality. The residuals are drawn from a normal distribution. This essentially means that for a particular value of \\(x\\) we would expect there to be a range of responses in \\(y\\) which follow a normal distribution. It is the distribution of the deviations of \\(y\\) from the fitted line (the residuals) that are assumed to be normal, not the raw \\(y\\) values This means that we can generally only test this assumption after the line has been fitted. It does not make sense to evaluate this assumption by looking at the raw data. Measurement error. The values of the independent \\(x\\) variable are determined with negligible measurement error36. It is often hard to obtain the \\(x\\) values with absolutely no measurement error, but the error \\(x\\) in should at least be smaller than that in the \\(y\\) values. So for example, in the thermal tolerance experiment the temperature values (set by the experimenter) almost certainly have little error, so it is appropriate to use regression. It is perfectly possible to evaluate differences among means in more than two categories, but we don’t use t-tests to do this. Instead, we us a more sophisticated tool called Analysis of Variance (ANOVA). We’ll learn about ANOVA in later chapters.↩ Notice that it is the vertical distance that matters, not the perpendicular distance from the line.↩ It sometimes makes sense to use a regression analysis when the independent variable is an ordinal categorical variable. It depends what you want to do with the resulting model. However, some people disagree with this approach, so it’s best to avoid doing it unless you’re confident you can justify it.↩ It is the measurement error, not the sampling error, that matters. This means it is fine to use regression when the \\(x\\) variable represent a sample from a population.↩ "],
["simple-regression-in-r.html", "Chapter 20 Simple regression in R 20.1 Carrying out a simple linear regression in R 20.2 First steps 20.3 Model fitting and significance tests 20.4 Presenting results 20.5 What about causation?", " Chapter 20 Simple regression in R Our goal in this chapter is to learn how to work with regression models in R. We’ll do this by working through an example. We’ll start with the problem and the data, and then work through model fitting, evaluating assumptions, significance testing, and finally, presenting the results. 20.1 Carrying out a simple linear regression in R A plant physiologist studying the process of germination in the broad bean (Vicia faba) is interested in the relationship between the activity of the enzyme amylase, and the temperature at which the germinating beans are kept. As part of this work she carries out an experiment to find the relationship between glucose release (from the breakdown of starch by amylase) and temperature (over the range 2 - 20C). The data obtained from such an experiment are given below. Temperature (\\(C\\)) 2 4 6 8 10 12 14 16 18 20 Glucose (\\(\\mu g\\) \\(mg^{-1}\\) dry weight) 1.0 0.5 2.5 1.5 3.2 4.3 2.5 3.5 2.8 5.6 What we want to do is work out whether there a statistically significant relationship between temperature and glucose release (and hence, presumably, amylase activity). That’s obviously a job for linear regression… Walk through example You should work through the example in the next few sections. 20.2 First steps The data are in a CSV file called GLUCOSE.CSV. Downloaded the data read and it into an R data frame, giving it the name vicia_germ: vicia_germ &lt;- read.csv(file = &quot;GLUCOSE.CSV&quot;) Make sure you use View, glimpse, etc to examine the data before you proceed. Run through all the usual questions… How many variables (columns) are in the data? How many observations (rows) are there? What kind of variables are we working with? This is a fairly simple data set. It contains two numeric variables. The first column (Temperature) contains the information about the experimental temperature treatments, and the second column (Glucose) contain the glucose measurements. Notice that we refer to the different temperatures as ‘experimental treatments’. This is because these data are from an experiment where temperature was controlled by the investigator. We’ll discuss this terminology in more detail when we move onto ANOVA models later in the book. 20.2.1 Visualising the data We should visualise the data next so that we understand it more. We just need to produce a simple scatter plot with ggplot2: Remember, Glucose is the dependent variable and Temperature is the independent variable, so they belong on the \\(y\\) and \\(x\\) axes, respectively. Let’s use this plot to help us evaluate the assumptions of the regression model… Variables and axes Be careful when you produce a scatter plot to summarise data in a regression analysis. You need to make sure the two variables are plotted the right way around with respect to the x and y axes: place the dependent variable on the y axis and the independent variable on the x axis. Nothing says, “I don’t know what I’m doing,” quite like mixing up the axes. 20.2.2 Checking the assumptions Assumption 2 (measurement scale) is easy to evaluate. The data are on ratio (glucose release, \\(\\mu g\\) \\(mg^{-1}\\) dry weight) and interval (temperature, °C) scales. Assumptions 1 (independence) and 6 (measurement error) are features of the experimental design and the data collection protocol. They can’t be checked by just looking at the data; we have to think about the data to decide if there are any obvious reasons why they might not be valid. The assumption of negligible measurement error seems perfectly reasonable, given that it was experimentally manipulated, and without more knowledge of the experimental design, we have to assume that the independence assumption is met. There are a special set of tools, called ‘regression diagnostics’, that allow us to evaluate the remaining assumptions. We are going to study these in the Regression diagnostics chapter, so in this chapter we will rely on less effective checks: Assumptions 3 (linearity) and 5 (equal variance) can be informally evaluated by looking at a scatter plot of the data. The scatter plot we just produced suggests that the relationship between \\(x\\) and \\(y\\) is linear, and the scatter in \\(y\\) neither increases nor decreases substantially with increasing values of \\(x\\). Assumption 4 (normality) is quite hard to judge from a scatter plot. Instead, this assumption is best checked by looking at the distribution of the residuals taken from the fitted regression model. We’ll have to check this assumption after we’ve fitted the regression model. 20.3 Model fitting and significance tests Since the data appear to meet the requirements of a regression analysis we can finally move on to obtaining some results. Carrying out a regression analysis in R is really no harder than doing a t-test. However, it is a two step process. The first step involves a process known as fitting the model (or just model fitting). In effect, this is the step where R calculates the best fit line, along with a large amount of additional information needed to generate the results in step two. We call this step model fitting because, well, we end up fitting the straight line model to the data. How do we fit a linear regression model in R? We will do it using the lm function. The letters ‘lm’ in this function name stand for ‘linear model’. We won’t say much more at this point other than point out that a linear regression is a special case of a general linear model. R doesn’t have a special regression function. Here is how we fit a linear regression in R using the enzyme data: vicia_model &lt;- lm(Glucose ~ Temperature, data = vicia_germ) This should look quite familiar. We have to assign two arguments: The first argument is a formula. We know this because it includes a ‘tilde’ symbol: ~. The variable name on the left of the ~ should be the dependent variable. The variable name on the right should be the independent variable. These are Glucose and Temperature, respectively. make sure you get these the right way round when carrying out regression: the dependent variable goes on the left; independent variable goes on the right. The second argument is the name of the data frame that contains the two variables listed in the formula (vicia_germ). How does R knows we want to carry out a regression? How does R know we want to use regression? After all, we didn’t specify this anywhere. The answer is that R looks at what type of variable Temperature is. It is numeric, and so R automatically carries out a regression. If it had been a factor or a character vector (representing a categorical variable) R would have carried out a different kind of analysis, called a one-way Analysis of Variance (ANOVA). Most of the models that we examine in this course are very similar, and can all be fitted using the lm function. The only thing that really distinguishes them is the type of variables that appear to the right of the ~ in a formula: if they are categorical variables we end up carrying out ANOVA, while numeric variables lead to a regression. The key message is that you have to keep a close eye on the type of variables you are modelling to understand what kind of model R will fit. Notice that we did not print the results to the console. Instead, we assigned the result a name (vicia_model). This now refers to a fitted model object. What happens if we print a regression model object to the console? print(vicia_model) ## ## Call: ## lm(formula = Glucose ~ Temperature, data = vicia_germ) ## ## Coefficients: ## (Intercept) Temperature ## 0.5200 0.2018 This prints a quick summary of the model we fitted and some information about the ‘coefficients’ of the model. The coefficients are the intercept and slope of the fitted line: the intercept is always labelled (Intercept) and the slope is labelled with the name of the independent variable (Temperature in this case). We’ll come back to these coefficients once we have looked at how to compute p-values. The second step of a regression analysis involves using the fitted model to assess statistical significance. We usually want to determine whether the slope is significantly different from zero. That is, we want to know if the relationship between the \\(x\\) and \\(y\\) variables is likely to be real or just the result of sampling variation. Carrying out the required F test is actually very easy. The test relies on a function called anova. To use this function, all we have to do is pass it one argument: the name of the fitted regression model object… anova(vicia_model) ## Analysis of Variance Table ## ## Response: Glucose ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temperature 1 13.4411 13.4411 14.032 0.005657 ** ## Residuals 8 7.6629 0.9579 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s step through the output to see what it means. The first line informs us that we are looking at an Analysis of Variance Table—a set of statistical results derived from a general tool called Analysis of Variance. The second line just reminds us what dependent variable we analysed (Glucose). Those parts are simple to describe at least, though the Analysis of Variance reference may seem a little cryptic. Essentially, every time we carry out an F-test we are performing some kind of Analysis of Variance because the test boils down to a ratio of variances (or more accurately, mean squares). The important part of the output is the table at the end: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Temperature 1 13.4411 13.4411 14.032 0.005657 ** ## Residuals 8 7.6629 0.9579 This summarises the different parts of the F-test calculations: Df – degrees of freedom, Sum Sq – the sum of squares, Mean Sq – the mean square, F value – the F-statistic, Pr(&gt;F) – the p-value. If you followed along in the last chapter these should be at least somewhat familiar. The F-statistic (variance ratio) is the key term. When working with a regression model, this quantifies how much variability in the data is explained when we include the best fit slope term in the model. Larger values indicate a stronger relationship between \\(x\\) and \\(y\\). The p-value gives the probability that the relationship could have arisen through sampling variation, if in fact there were no real association. As always, a p-value of less than 0.05 is taken as evidence that the relationship is real, i.e. the result is statistically significant. We should also note down the two degrees of freedom given in the table as these will be needed when we report the results. 20.3.1 Extracting a little more information There is a second function, called summary, that can be used to extract a little more information from the fitted regression model: summary(vicia_model) ## ## Call: ## lm(formula = Glucose ~ Temperature, data = vicia_germ) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.35273 -0.77909 -0.08636 0.74227 1.35818 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.52000 0.66858 0.778 0.45909 ## Temperature 0.20182 0.05388 3.746 0.00566 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9787 on 8 degrees of freedom ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.5915 ## F-statistic: 14.03 on 1 and 8 DF, p-value: 0.005657 This is easiest to understand if we step through the constituent parts of the output. The first couple of lines just remind us about the model we fitted ## Call: ## lm(formula = Glucose ~ Temperature, data = vicia_germ) The next couple of lines aren’t really all that useful—they summarise some properties of the residuals–so we’ll ignore these. The next few lines comprise a table that summarises some useful information about the coefficients of the model (the intercept and slope): ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.52000 0.66858 0.778 0.45909 ## Temperature 0.20182 0.05388 3.746 0.00566 ** The Estimate column shows us the estimated the intercept and slope of the regression. We saw these earlier when we printed the fitted model object to the console. Staying with this table, the next three columns (Std. Error, t value and Pr(&gt;|t|)) show us the standard error associated with each coefficient, the corresponding t-statistics, and the p-values. Remember standard errors? These are a measure of the variability of the sampling distribution associated with something we estimate from a sample. We discussed these in the context of sample means. One can calculate a standard error for many different kinds of quantities, including the intercept and slope of a regression model. And just as with a mean, we can use the standard errors to evaluate the significance of the coefficients via t-statistics. In this case, the p-values associated with these t-statistics indicate that the intercept is not significantly different from zero (p&gt;0.05), but that the slope is significantly different from zero (p&lt;0.01). Notice that the p-value associated with the slope coefficient is the same as the one we found when we used the anova function. This is not a coincidence—anova and summary test the same thing when working with simple linear regression models. This is not generally true for other kinds of model involving the lm function. The only other part of the output from summary that is of interest now is the line containing the Multiple R-squared value: ## Multiple R-squared: 0.6369, Adjusted R-squared: 0.5915 This shows the \\(R\\)-squared (\\(R^{2}\\)) of our model. It tells you what proportion (sometimes expressed as a percentage) of the variation in the data is explained, or accounted for, by the fitted line. If \\(R^{2}=1\\) the line passes through all the points on the graph (all the variation is accounted for) and if \\(R^{2}\\approx 0\\%\\) the line explains little or none of the variation in the data. The \\(R^{2}\\) value here is 0.64. This is very respectable, but still indicates that there are other sources of variation (differences between beans, inaccuracies in the assay technique, etc.) which remain unexplained by the line37. 20.3.2 Residual analysis There are various situations in which we need to be able to extract the residuals from a fitted model. We’ll look at one example now. Remember the 5th assumption: the residuals are drawn from a normal distribution. How might we examine the distributional assumptions of the regression? We can do this with a dot plot or histogram of the residuals, if know how to extract these from a model. This is easy in R—we use the resid function: resid(vicia_model) ## 1 2 3 4 5 6 ## 0.0763636 -0.8272728 0.7690909 -0.6345455 0.6618182 1.3581820 ## 7 8 9 10 ## -0.8454545 -0.2490909 -1.3527273 1.0436363 This just extracts a numeric vector containing the residuals and prints them to the console. In order to plot these we need to put them inside a data frame (ggplot2 only works with data frames), and store the result: resid.data &lt;- data.frame(Residuals = resid(vicia_model)) Once we have extracted the residuals into a data frame we just use ggplot2 in the usual way to plot them. We’ll use a dot plot, as there aren’t many residuals: ggplot(resid.data, aes(x = Residuals)) + geom_dotplot(binwidth = 0.3) It is hard to know if these are normally distributed when we only have 10 observations, but there is nothing that screams ‘non-normal’ here. 20.4 Presenting results From the preceding analysis we can conclude… There is a significant positive relationship between the incubation temperature (°C) and glucose released (\\(\\mu g mg^{-1}\\) dry weight) in germinating bean seeds (\\(y=0.52+0.20x\\), F=14, d.f.=1,8, p&lt;0.01). Don’t forget to quote both degrees of freedom in the result. These are obtained from the ANOVA table produced by anova and should be given as the slope degrees of freedom first (which is always 1), followed by the error degrees of freedom. If the results are being presented only in the text it is usually appropriate to specify the regression equation as well as the significance of the relationship as this allows the reader to see in which direction and how steep the relationship is, and to use the equation in further calculations. It may also be useful to give the units of measurement—though these should already be stated in the Methods. Often, however, we will want to present the results as a figure, showing the original data and the fitted regression line. In this case, most of the statistical detail can go in the figure legend instead. Let’s see how to present the results as a figure… 20.4.1 Plotting the fitted line and the data We already know how to make a scatter plot. The only new trick we need to learn is how to add the fitted line. Remember the output from the summary table—this gave us the intercept and slope of the best fit line. We could extract these (there is a function called coef that does this), and using our knowledge of the equation of a straight line, use them to then calculate a series of points on the fitted line. However, there is an easier way to do this using the predict function. Don’t worry too much if this next segment on predictions is confusing. It looks more complicated than it is, but you may have to come back to it a few times before it all sinks in. At first reading, try to focus on the logic of the calculations without worrying too much about the details. You won’t be assessed on your ability to plot a fitted line and the data together but you might need to be able to do this to prepare a report. In order to use predict we have to let R know the values of the independent variable for which we want predictions. In the bean example the temperature was varied from 2-20 °C, so it makes sense to predict glucose concentrations over this range. Therefore the first step in making predictions is to generate a sequence of values from 2 to 20, placing these inside a data frame: pred.data &lt;- data.frame(Temperature = seq(2, 20, length.out = 25)) We learned about the seq function last year. Here, we used it to make a sequence of 25 evenly spaced numbers from 2 to 20. If you can’t remember what it does, ask a demonstrator to explain it to you (and use View to look at pred.data). Notice that we gave the sequence the exact same name as the independent variable in the regression (Temperature). This is important: the name of the numeric sequence we plan to make predictions from has to match the name of the independent variable in the fitted model object. Once we have set up a data frame to predict from (pred.data) we are ready to use the predict function: predict(vicia_model, pred.data) ## 1 2 3 4 5 6 7 ## 0.9236364 1.0750000 1.2263637 1.3777273 1.5290909 1.6804546 1.8318182 ## 8 9 10 11 12 13 14 ## 1.9831818 2.1345455 2.2859091 2.4372727 2.5886364 2.7400000 2.8913636 ## 15 16 17 18 19 20 21 ## 3.0427273 3.1940909 3.3454545 3.4968182 3.6481818 3.7995454 3.9509091 ## 22 23 24 25 ## 4.1022727 4.2536363 4.4050000 4.5563636 This take two arguments: the first is the name of the model object (vicia_model); the second is the data frame (pred.data) containing the values of the independent variable at which we want to make predictions. The predict function generated the predicted values in a numeric vector and printed these to the console. To be useful, we need to capture these somehow, and because we want to use ggplot2, these need to be kept inside a data frame. We can use mutate to do this: pred.data &lt;- mutate(pred.data, Glucose = predict(vicia_model, pred.data)) Look at the first 10 rows of the resulting data frame: head(pred.data, 10) ## Temperature Glucose ## 1 2.00 0.9236364 ## 2 2.75 1.0750000 ## 3 3.50 1.2263637 ## 4 4.25 1.3777273 ## 5 5.00 1.5290909 ## 6 5.75 1.6804546 ## 7 6.50 1.8318182 ## 8 7.25 1.9831818 ## 9 8.00 2.1345455 ## 10 8.75 2.2859091 The pred.data is set out much like the data frame containing the experimental data. It has two columns, called Glucose and Temperature, but instead of data, it contains predictions from the model. Plotting these predictions along with the data is now easy: ggplot(pred.data, aes(x = Temperature, y = Glucose)) + geom_line() + geom_point(data = vicia_germ) + xlab(&quot;Temperature (°C)&quot;) + ylab(&quot;Glucose concentration&quot;) + theme_bw(base_size = 22) Notice that we have to make ggplot2 use the vicia_germ data (i.e. the raw data) when adding the points. We also threw in a little theming to make the plot look nicer. Let’s summarise what we did: 1) using seq and data.frame, we made a data frame with one column containing the values of the independent variable we want predictions at; 2) we then used the predict function to generate these predictions, adding them to the prediction data with mutate; 3) finally, we used ggplot2 to plot the predicted values of the dependent variable against the independent variable, remembering to include the data. 20.5 What about causation? No discussion of regression would be complete without a little homily on the fact that just because you observe a (significant) relationship between two variables this does not necessarily mean that the two variables are causally linked. If we find a negative relationship between the density of oligochaete worms (the dependent variable) and the density of trout (the independent variable) in a sample of different streams, this need not indicate that the trout reduce the numbers of oligochaetes by predation – in fact oligochaete numbers are often very high in slow-flowing, silty streams where they live in the sediments, trout prefer faster flowing, well oxygenated, stony streams – so a negative correlation could occur simply for that reason. There are many situations in biology where a relationship between two variables can occur not because there is a causal link between them but because each is related to a third variable (e.g. habitat). This difficulty must always be borne in mind when interpreting relationships between variables in data collected from non-experimental situations. However, it is often assumed that because of this problem regression analysis can never be used to infer a causal link. This is incorrect. What is important is how the data are generated, not the statistical model used to analyse them. If a set of ten plants were randomly assigned to be grown under ten different light intensities, with all other conditions held constant, then it would be entirely proper to analyse the resulting data (for, let us say, plant height) by a regression of plant height (\\(y\\)) against light level (\\(x\\)) and, if a significant positive straight-line relationship was found, to conclude that increased light level caused increased plant height. Of course this conclusion still depends on the fact that another factor (e.g. temperature) isn’t varying along with light and causing the effect. But the fact that you are experimentally producing an effect, in plants randomly allocated to each light level (i.e. plants in which it is highly improbable that the heights happened to be positively related to light levels at the start) which gives you the confidence to draw a conclusion about causality. Light might not be the direct causal agent, but it must be indirectly related to plant growth because it was experimentally manipulated. The Adjusted R-squared: value can be ignored in this analysis—it is used when doing a form of regression called multiple regression, in which there is more than one \\(x\\) variable.↩ "],
["regression-diagnostics.html", "Chapter 21 Regression diagnostics 21.1 Introduction 21.2 Diagnostics for simple linear regression 21.3 Regression diagnostics the easy way 21.4 Diagnostics for other kinds of models", " Chapter 21 Regression diagnostics 21.1 Introduction We usually have an analysis in mind when we design an experiment or observational data collection protocol. It may be tempting to jump straight into this analysis without carefully examining the data first. This is never a good idea. In the past few chapters we have repeatedly emphasised that careful data analysis always begins with inspection of the data. Visualising a dataset helps us to understand the data and and evaluate whether or not the assumptions of a statistical tool are likely to be violated. 21.1.1 Understanding data We’ve been using ‘well-behaved’ data sets in this book so far, which tends to give the impression that visual inspections of the data are not all that necessary. Here’s an example of why it matters. Imagine we are interested in quantifying the relationship between two variables, called \\(x\\) and \\(y\\). We might be tempted to carry out a linear regression analysis without first inspecting these data to get straight to ‘the answer’: the coefficients of the linear regression model. This could be very misleading. Take a look at these four scatter plots: These four artificial data sets were constructed by the statistician Francis Anscombe. The means and variances of \\(x\\) and \\(y\\) are nearly identical in all four data sets, and what’s more, the intercepts and slopes of the best fit regression lines are almost identical (the intercept and slope are 3.00 and 0.500, respectively). The nature of the relationship between \\(x\\) and \\(y\\) is quite obviously different among the four cases: “Case 1” shows two linearly related, normally distributed variables. This is the kind of data we often hope for in a statistical analysis. “Case 2” shows two variables that are not normally distributed, but there is a perfect non-linear relationship between the two. “Case 3” shows an example the variables are perfectly linearly associated for all but one observation which ruins the perfect relationship. “Case 4” shows an example where a single outlier generates an apparent relationship where the two variables are otherwise unrelated. Each of these plots tells a different story about the relationship between \\(x\\) and \\(y\\), yet the linear regression model says the same thing is happening in each case. These are obviously somewhat pathological examples, but they clearly illustrate the kinds of issues that can, and do, arise with real data. There is a real risk we will apply an inappropriate analysis if we fail to detect these kinds of problems. 21.1.2 Checking assumptions Every statistical model makes certain assumptions about the data38. Even if a dataset doesn’t exhibit the very obvious problems seen in the Anscombe examples, we still need to assess whether the assumptions of the statistical model we want to use are likely to be valid. We always check these assumptions each time we introduced a new model or test. For example, when working with a linear regression model, we started with a scatter plot of the dependent variable vs. the independent variable. This allowed us to assess whether the two variables are linearly related, and to see whether the variability of the residuals is constant. We then plotted the residuals from the fitted regresssion model to evaluate the normality assumption. These kinds of ad hoc approaches are useful, but they can be often be difficult to apply at times. For example, if we only have a small sample it is difficult to evaluate the normality assumption of a regression using a dot plot or histogram. Similarly, a scatter plot may hide subtle non-linearities in the relationship between two variables. These limitations can be addressed by moving away from plots of raw data and residuals. It is much better to rely on a set of graphical tools called regression diagnostics. Instead of using just the raw data, regression diagnostics use properties of the fitted model to understand how well the model fits the data and evaluate the model assumptions. This chapter is about how to generate and interpret three basic regression diagnostic plots. We’ll learn about these tools in the context of regression, though they can be used with any statistical model produced by the lm function. 21.2 Diagnostics for simple linear regression Before examining some diagnostic plots for linear regression, we should review the underlying assumptions: Independence. The residuals must be independent. Measurement scale. The dependent \\(y\\) variable is measured on an interval or ratio scale. Measurement error. The values of the independent \\(x\\) variable are determined with negligible error. Linearity The relationship between the \\(x\\) and \\(y\\) variable is linear. Normality. The residuals are drawn from a normal distribution. Constant variance. The variance of the residuals is constant. Assumptions 1 (independence), 2 (measurement scale) and 3 (measurement error) are features of the experimental design and the data collection protocol. They generally can not be explicitly checked by looking at the data or by producing regression diagnostics. This leaves assumptions 4 (linearity), 5 (normality) and 6 (constant variance). There is a specific diagnostic plot for each of these. We’ll work with a specific example to learn how these diagnostics work… Walk through example You should work through the example in the next few sections. 21.2.1 A simple linear regression example A survey was carried out to establish whether the abundance of hedgerows in agricultural land had an effect on the abundance of grey partridge. From an area of agricultural land covering several farms, 40 plots were selected which had land uses as similar as possible, but differed in the density of hedgerows (km hedgerow per km2). Plots were selected to cover a wide range of hedgerow densities. The density of partridges was established by visiting all fields in a study plot once immediately after dawn and once just before dusk, when partridges are most likely to be seen. Counts of birds observed were made on each visit and the dawn and dusk data were averaged to give a value for partridge abundance for each study plot. The data are stored in a CSV file PARTRIDG_BIGSTUDY.CSV (not PARTRIDG.CSV!). The density of hedgerows (km per km2) is in the Hedgerow variable and the density of partridges (no. per km2) is in the Partridge variable. Read the data into R, calling it partridge. 21.2.2 Simple checks We relied on simple plots of the raw data to evaluate the assumptions of regression in the last chapter. Let’s revise these evaluations first. We start by constructing a scatter plot of the data: ggplot(partridge, aes(x = Hedgerow, y = Partridge)) + geom_point() + xlab(&quot;Hedgerow density (km per km²)&quot;) + ylab(&quot;Partridge Count&quot;) Spend some time looking at the scatter plot. Think about the linearity, normality and constant variance assumptions of linear regression. Do you think these data satisfy all three of these assumptions? If not, how is (are) the assumption(s) broken? The linearity, normality and constant variance assumptions are features of: 1) the nature of the relationship between the dependent (Partridge) and independent (Hedgerow) variables, and 2) the scatter of the observations around this relationship. These are difficult to evaluate without at least having a sensible reference point: the line of best fit. Let’s look at the data again, but this time include the line of best fit. We know how to do this—there are three steps. First, we have to fit the linear regression model to the data: partridge_model &lt;- lm(Partridge ~ Hedgerow, data = partridge) In the second step, we use the predict function with the fitted model object (partridge_model) to generate some predicted values of the dependent variable, over a range of values of the independent variable: pred.data &lt;- data.frame(Hedgerow = 5:45) %&gt;% mutate(Partridge = predict(partridge_model, newdata = .)) The final step uses ggplot2 to plot the data and add the predictions. It is somewhat easier to evaluate the assumptions now that we have added the line of best fit: ggplot(pred.data, aes(x = Hedgerow, y = Partridge)) + geom_line(colour = &quot;steelblue&quot;) + geom_point(data = partridge) + # remember: use the raw data here xlab(&quot;Hedgerow density (km per km²)&quot;) + ylab(&quot;Partridge Count&quot;) Spend some time looking at the new scatter plot. Think again about the linearity, normality and constant variance assumptions of linear regression. Does including the line of best fit make it easier to assess these assumptions? If your answer is yes, which of the assumption(s) is (are) easier to evaluate now? The second kind of assessment we used is based on the distribution of the residuals from a fitted model. Each observation has one residual associated with it—this is the vertical distance between the relevant value of a partridge density observation and the fitted line. We use the resid function to extract the residuals from a fitted model object: resid(partridge_model) ## 1 2 3 4 5 6 7 8 9 10 ## 13.30 14.42 17.51 19.74 10.02 10.63 5.44 3.41 -6.16 9.11 ## 11 12 13 14 15 16 17 18 19 20 ## 6.64 -5.06 -18.81 -11.39 -11.55 -0.91 -13.99 1.12 -1.40 -2.69 ## 21 22 23 24 25 26 27 28 29 30 ## -15.53 -19.65 -15.76 -10.53 -7.16 -10.75 -38.01 -9.51 -0.86 -25.91 ## 31 32 33 34 35 36 37 38 39 40 ## 7.38 -21.33 27.48 18.02 -44.07 44.00 27.74 9.51 51.15 -5.63 Staring at a long list of residuals is not really all that useful. Instead, we need to summarise their distribution, using either a dot plot or a histogram. We have to place them into a data frame first to use ggplot: # step 1 -- place the residuals in a data frame plt_data &lt;- data.frame(residuals = resid(partridge_model)) # step 2 -- make a dot plot ggplot(plt_data, aes(x = residuals)) + geom_dotplot(binwidth = 5) Which assumption of linear regression—linearity, normality or constant variance—does this histogram allow you to evaluate? Does it look like the assumption has been violated? That’s enough revision. Now it’s time to turn to the regression diagnostics. 21.2.3 Fitted values In order to understand regression diagnostics we have to know what a fitted value is. The phrase ‘fitted value’ is just another expression for ‘predicted value’. Look at the plot below: This shows the raw data (black points), the line of best fit (blue line), the residuals (the vertical grey lines), and the fitted values (red points). We find the fitted values by drawing a vertical line from each observation to the line of best fit. The values of the dependent variable (Partridge in this case) at the point where these touch the line of best fit are the ‘fitted values’. This means the fitted values are just predictions from the statistical model, generated for each value of the independent variable. We can use the fitted function to extract these from a fitted model: fitted(partridge_model) ## 1 2 3 4 5 6 7 8 9 10 11 12 ## -11.6 -11.0 -8.4 -7.2 -4.9 -1.4 1.8 7.3 8.2 10.2 11.1 14.3 ## 13 14 15 16 17 18 19 20 21 22 23 24 ## 22.1 22.7 27.0 28.2 28.8 35.2 37.8 38.1 43.0 45.4 48.3 52.3 ## 25 26 27 28 29 30 31 32 33 34 35 36 ## 57.6 61.0 62.2 62.2 63.7 68.3 71.2 77.0 86.6 87.8 88.1 90.4 ## 37 38 39 40 ## 91.6 96.8 98.0 101.7 Notice that some of the fitted values are below zero. Why do we see negative fitted values? This doesn’t make much sense biologically (negative partridges?). Do you think it is a problem? 21.2.4 Checking the linearity assumption The linearity assumption states that the general relationship between the dependent and independent variable should look like a straight line. We can evaluate this assumption by constructing a residuals vs. fitted values plot. This is a two-step process. First use the fitted and resid functions to construct a data frame containing the fitted values and residuals from the model: plt_data &lt;- data.frame(Fitted = fitted(partridge_model), Resids = resid(partridge_model)) We called the data frame plt_data. Once we have made this data frame, we use ggplot2 to plot the residuals against the fitted values: ggplot(plt_data, aes(x = Fitted, y = Resids)) + geom_point() + xlab(&quot;Fitted values&quot;) + ylab(&quot;Residuals&quot;) What does this plot tell us? It indicates that the residuals tend to be positive at the largest and smallest fitted values, and that they are generally negative in the middle of the range. This U-shaped pattern is indicative of a problem with our model. It tells us that there is some kind of pattern in the association between the two variables that is not being accommodated by the linear regression model we fitted to the data. The U-shape indicates that the relationship is non-linear, and that it ‘curves upward’. We can see where this pattern comes from when we look at the raw data and fitted model again: There is obviously some curvature in the relationship between partridge counts and hedgerow density, yet we fitted a straight line through the data. The U-shape in the residuals vs. fitted value plot comes from the fact that the relationship between the dependent and independent variables is ‘accelerating’ (also called ‘convex’). What other kinds of patterns might we see residuals vs. fitted value plot? Two are particularly common: U-shapes (the one we just saw) and hump-shapes. Look at the two artificial data sets below… The data labelled ‘Accelerating’ is similar to the partridge data: it exhibits a curved, accelerating relationship between the dependent variable and the independent variable. The data labelled ‘Decelerating’ shows a different kind of relationship: there is a curved, decelerating relationship between the two variables. We can fit a linear model to each of these data sets, and then visualise the corresponding residuals vs. fitted value plots: Here we see the characteristic U-shape and hump-shape pattern we mentioned above. The U-shape occurs when there is an accelerating relationship between the dependent variable and the independent variable. The hump-shape occurs when there is an decelerating relationship between the dependent variable and the independent variable. By this point something may be bothering you. This seems like a lot of extra work to evaluate an aspect of the model that we can assess by just plotting the raw data. This is true when we are working with a simple linear regression model. However, it can much harder to evaluate the linearity assumption when working with more complicated models where there is more than one independent variable39. In these situations, a residuals vs. fitted values plot gives us a powerful way to evaluate whether or not the assumption of a linear relationship is reasonable. That’s enough about the residuals vs. fitted values plot. Let’s move on to the normality evaluation… 21.2.5 Checking the normality assumption How should we evaluate the normality assumption of linear regression? That is, how do we assess whether or not the residuals are drawn from a normal distribution? We know how to extract the residuals from a model and plot their distribution, but there is a more powerful graphical technique to available to us: the normal probability plot. The normal probability plot is used to identify departures from normality. If we know what we are looking for, we can identify many different kinds of problems, but to keep life simple we will focus on the most common type of assessment: determining whether or not the distribution of residuals is excessively skewed. Remember the concept of distributional skew? A skewed distribution is just one that is not symmetric. For example, the first distribution below is skewed to the left (‘negative skew’), the second is skewed to the right (‘positive skew’), and the third is symmetric (‘zero skew’): The skewness in the first two distributions is easy to spot because they contain a lot of data and the skewness is quite pronounced. A normal probability plot allows us to pick up potential problems when we are not so lucky. The methodology underlying construction of a normal probability plot is quite technical, so we will only try to give a flavour of it here. Don’t worry if the next segment is confusing—interpreting a normal probability plot is much easier than making one. We’ll work with the partridge example again. We start by extracting the residuals from the fitted model into a vector, using the resids function, and then standardise these by dividing them by their standard deviation: mod_resids &lt;- resid(partridge_model) mod_resids &lt;- mod_resids / sd(mod_resids) The standardisation step is not essential, but dividing the raw residuals by their standard deviation ensures that the standard deviation of the new residuals is equal to 1. Standardising the residuals like this makes it a little easier to compare more than one normal probability plot. We call these new residuals the ‘standardised residuals’. The next step is to find the rank order of each residual. That is, we sort the data from lowest to highest, and find the position of each case in the sequence (this is its ‘rank’). The function order can does this: resid_order &lt;- order(mod_resids) resid_order ## [1] 35 27 30 32 22 13 23 21 17 15 14 26 24 28 25 9 40 12 20 19 16 29 18 ## [24] 8 7 11 31 10 38 5 6 1 2 3 34 4 33 37 36 39 This tells us that the first residual is the 35th largest, the second is the 27th largest, the third is the 30th largest, and so on. The last step is the tricky one. Once we have established the rank order of the residuals, we ask the following question: if the residuals really were drawn from a normal distribution, what is their most likely value, based on their rank? We can’t really explain how to do this without delving into the mathematics of distributions, so this will have to be a ‘trust us’ situation. As usual, R can do this for us, and we don’t even need the ranks—we just calculated them to help us explain what happens when we build a normal probability plot. The function we need is called qqnorm: all_resids &lt;- qqnorm(mod_resids, plot.it = FALSE) all_resids &lt;- as.data.frame(all_resids) The qqnorm doesn’t produce a data frame by default, so we had to covert the result using a function called as.data.frame. This extra little step isn’t really all that important. The all_resids object is now a data frame with two variables: x contains the theoretical values of normally distributed residuals, based on the rank orders of the residuals from the model, and y contains the actual standardised residuals. Here are the first 10 values: head(all_resids, 10) ## x y ## 1 0.7977768 0.6855052 ## 2 0.8871466 0.7431596 ## 3 0.9842350 0.9021176 ## 4 1.2133396 1.0174265 ## 5 0.6356570 0.5162945 ## 6 0.7143674 0.5478778 ## 7 0.2858409 0.2800927 ## 8 0.2211187 0.1759340 ## 9 -0.2858409 -0.3173156 ## 10 0.4887764 0.4693592 Finally, we can plot these against one another to make a normal probability plot: ggplot(all_resids, aes(x = x, y = y)) + geom_point() + geom_abline(intercept = 0, slope = 1) + xlab(&quot;Theoretical Value&quot;) + ylab(&quot;Standardised Residual&quot;) We used geom_abline(intercept = 0, slope = 1) to add the one-to-one (1:1) line. We haven’t used this function before and we won’t need it again. The one-to-one line is just a line with a slope of 1 and an intercept of 0—if an \\(x\\) value and \\(y\\) value are equal their corresponding point will lie on this line. Don’t worry too much if those calculations seem opaque. We said at the beginning of this section that it’s not important to understand how a normal probability plot is constructed. It is important to know how to interpret one. The important feature to look out for is the positioning of the points relative to the 1:1 line. If the residuals really are drawn from a normal distribution they should generally match the theoretical values, i.e. the points should lie on the 1:1 line. In the partridge example that is exactly what we see. A couple of the more extreme values diverge a little, but this isn’t something to worry about. We never expect to see a perfect 1:1 relationship in these kinds of plots. The vast majority of the points are very close to the 1:1 line though, which provides strong evidence that the residuals probably are sampled from a normal distribution. What does a normal probability plot look like when residuals are not consistent with the normality assumption? Deviations from a straight line suggest departures from normality. How do right skew (‘positive skew’) and left skew (‘negative skew’) manifest themselves in a normal probability plot? Here is the normal probability plot produced using data from the left-skewed distribution above: Rather than a straight line, we see a decelerating curved line. This is the signature of residuals that are non-normal, and left-skewed. We see the opposite sort of curvature when the residuals are right-skewed: You should always use normal probability plots to assess normality assumptions in your own analyses. They work with every kind of model fitted by the lm function. What is more, they also work reasonably well when we only have a few residuals to play with. Seven is probably the lowest number we might accept—with fewer points it becomes hard to distinguish between random noise and a real deviation from normality. That’s enough discussion of normal probability plots. Let’s move on to the constant variance evaluation… 21.2.6 Checking the constant variance assumption How do we evaluate the constant variance assumption of a linear regression? That is, how do we assess whether or not the variability of the residuals is constant or not? This assumption can be evaluated in a similar way to the linearity assumption, by producing something called a ‘scale-location plot’. We construct this by plotting residuals against the fitted values, but instead of plotting raw residuals, we transform them first using the following ‘recipe’: Standardise the residuals by dividing them by their standard deviation. Remember, this ensures the new residuals have a standard deviation of 1. Find the absolute value of the residuals produced in step 1. If they are negative, make them positive, otherwise, leave them alone. Take the square root of the residuals produced in step 2. These calculations are simple enough in R. We’ll demonstrate them using the partridge data set again: # extract the residuals sqrt_abs_resids &lt;- resid(partridge_model) # step 1. standardise them sqrt_abs_resids &lt;- sqrt_abs_resids / sd(sqrt_abs_resids) # step 2. find their absolute value sqrt_abs_resids &lt;- abs(sqrt_abs_resids) # step 3. square root these sqrt_abs_resids &lt;- sqrt(sqrt_abs_resids) Now we use the fitted function to extract the fitted values from the model and place these in a data frame with the transformed residuals: plt_data &lt;- data.frame(Fitted = fitted(partridge_model), Resids = sqrt_abs_resids) We called the data frame plt_data. Once we have made this data frame, we use ggplot2 to plot the transformed residuals against the fitted values: ggplot(plt_data, aes(x = Fitted, y = Resids)) + geom_point() + xlab(&quot;Fitted values&quot;) + ylab(&quot;Square root of absolute residuals&quot;) This is a scale-location plot. Why is this useful? We are looking to see if there is any kind of relationship between the transformed residuals and the fitted values, i.e., we want to assess whether or not the size of these new residuals increase or decrease as the fitted values get larger. If they do not—the relationship is essentially flat—then we can conclude that the variability in the residuals is constant. Otherwise, we have to conclude that the constant variance assumption is violated. Although the pattern is not exactly clear cut, in this example there seems to be a bit of an upward trend with respect to the fitted values. This suggests that the variability (more formally, the ‘variance’) of the residuals increases with the fitted values. Larger partridge counts seem to be associated with more variability. This is a very common feature of count data. Poor model fit complicates scale-location plots It is worth reflecting on the ambiguity in this pattern. It is suggestive, but it certainly isn’t as clear as the U-shape in the residuals vs. fitted values plot used earlier. There is one potentially important reason for this ambiguity. The model we have used to describe the relationship between partridge counts and hedgerow density is not a very good model for these data. There is curvature in the relationship that we failed to take account of, and consequently, this lack of fit is impacting the scale-location plot. When a model does not fit the data well, the scale-location plot does not only describe the variability in the residuals. It also reflects the lack of fit. The take-home message is that it is a good idea to fix a lack of fit problem before trying to evaluate the constant variance assumption. Non-constant variance can be a problem because it affects the validity of p-values associated with a model. You should aim to use scale-location plots to assess the constant variance assumption in your own analyses, but keep in mind that a scale-location plot may also reflect non-linearity. 21.3 Regression diagnostics the easy way It turns out that we didn’t have to do all that work to construct the diagnostic plots. R has a built in facility to plot these. It works via a function called plot. For example, to produce a residuals vs fitted values plot, we use: plot(partridge_model, add.smooth = FALSE, which = 1) The first argument is the name of fitted model object. The second argument controls the output of the plot function: which = 1 argument tells it to produce a residuals vs. fitted values plot. The add.smooth = FALSE is telling the function not to add a line called a ‘loess smooth’. This line is supposed to help us pick out patterns, but it tends to over fit the relationship and leads people to see problems that aren’t there. If we can’t see a clear pattern without the line, it probably isn’t there, so it’s better not to include it at all. The other two plots are produced in the same way. We use the plot function to plot a normal probability diagnostic by setting which = 2: plot(partridge_model, which = 2) This produces essentially the same kind of plot we made above, with one small difference. Rather than drawing a 1:1 line, the ‘plot’ function shows us a line of best fit. This just allows us to pick out the curvature a little more easily. Finally, we can produce a scale-location diagnostic plot using the plot function with which = 3, suppressing the line to avoid finding spurious patterns: plot(partridge_model, add.smooth = FALSE, which = 3) 21.4 Diagnostics for other kinds of models We have focussed on simple linear regression in this session because regression diagnostics are most easily understood in the context of a regression model. However, the term ‘regression diagnostic’ is a bit of a misnomer. A more accurate term might be ‘linear model diagnostic’ but no one really uses this. Regression diagnostics can be used with many different kinds of models. In fact, the diagnostic plots we have introduced here can be applied to any model fitted by the lm function. This includes things like the ANOVA models we’ll study in later chapters. Don’t panic if your diagnostics aren’t perfect! The good news about regression is that it is quite a robust technique. It will often give us reasonable answers even when the assumptions are not perfectly fulfilled. We should be aware of the assumptions but should not become too obsessed by them. If the violations are modest, it is often fine to proceed. We just need to interpret results with care. Of course, we have to know what constitutes a ‘modest’ violation. There are no hard and fast rules. The ability to make that judgement is something that comes with experience. Even so-called ‘non-parametric’ models have underpinning assumptions; these are just not as restrictive as their parametric counterparts.↩ This is the situation we face with multiple regression. A multiple regression is a type of regression with more than one independent variable—we don’t study them in this course, but they are often used in biology.↩ "],
["correlation.html", "Chapter 22 Correlation 22.1 Introduction 22.2 Correlation and regression 22.3 Pearson’s product-moment correlation coefficient 22.4 Spearman’s rank correlation", " Chapter 22 Correlation 22.1 Introduction Correlations are statistical measures that quantify an association between two variables in a sample. An association is any relationship between the variables that makes them dependent in some way: knowing the value of one variable gives you information about the possible values of the second variable. The terms association and correlation are often used interchangeably, but strictly speaking correlation has a narrower definition. A correlation quantifies, via a correlation coefficient, the degree to which an association tends to a certain pattern. There are a variety of methods for quantifying correlation, but these all share common properties: If there is no relationship between the variables then the correlation coefficient will be zero. The closer to 0 the value, the weaker the relationship. A perfect correlation will be either -1 or +1, depending on the direction. This is illustrated below… The value of a correlation coefficient indicates the direction and strength of the association, but says nothing about the steepness of the relationship. A correlation coefficient is just a number, so it can not tell us exactly how one variable depends on the other. A correlation coefficient doesn’t tell us whether an apparent association is likely to be real or not. It is possible to construct a statistical test to evaluate whether a correlation is different from zero. Like any statistical test, this requires certain assumptions about the variables to be met. There are several different measures of correlation between two variables. We will consider the two most widely used methods40: The first is called Pearson’s product-moment correlation (\\(r\\)), though for convenience it is often called Pearson’s correlation. Pearson’s correlation is considered to be a parametric method, because the associated significance test makes strong assumptions about the distribution of the variables (but see our comment below). The second is Spearman’s rank correlation (\\(\\rho\\)), which as the name suggests is a method based on the rank order of observations. Spearman’s rank correlation is considered to be a non-parametric method, because the associated significance test relies on fairly loose assumptions. We’ll look at each of these in turn, but first, we need to clear up a source of potential confusion… 22.2 Correlation and regression Correlation and regression are both concerned with associations between variables, but they are different techniques, and each is appropriate under different circumstances. This is a frequent source of confusion. Which technique is required for a particular analysis depends on the way the data were collected and the purpose of the analysis. There are two broad questions to consider: Where do the data come from? Think about how the data have been collected. If the data are from an experimental study where one of the variables has been manipulated, then choosing the best analysis is easy. We should use a regression analysis, in which the independent variable is the experimentally manipulated variable and the dependent variable is the measured outcome. The fitted line from a regression analysis describes how the outcome variable depends on the manipulated variable—it describes the causal relationship between them. It is generally inappropriate to use correlation to analyse data from an experimental setting. A correlation analysis examines association but does not imply the dependence of one variable on another. Since there is no distinction of dependent or independent variables, it doesn’t matter which way round we do a correlation. (The phrase ‘which way round’ doesn’t even make sense in the context of a correlation.) If the data are from an observational study, either method may be appropriate. Time to ask another question… What is the goal of the analysis? Think about what question is being addressed. A correlation coefficient only quantifies the strength and direction of an association between two variables. It will be close to zero if there is no association between the variables; a strong association is implied if the coefficient is near -1 or +1. A correlation coefficient tells us nothing about the form of a relationship. Nor does it allow us to make predictions about the value of one variable from the value of a second variable. A regression does allow this because it involves fitting a line through the data—i.e. there’s a model for the relationship. This means that if the goal of an analysis is to understand the form of a relationship between two variables, or to use a fitted model to make predictions, we have to use regression. If we just want to know whether two variables are associated or not, the direction of the association, and whether the association is strong or weak, then a correlation analysis is sufficient. It is better to use a correlation analysis when the extra information produced by a regression is not needed, because the former will be simpler and potentially more robust (if we use Spearman’s correlation). 22.3 Pearson’s product-moment correlation coefficient Pearson’s correlation, being a parametric technique, makes some reasonably strong assumptions: The data are on an interval or ratio scale. The relationship between the variables is linear. Both variables are normally distributed in the population. The requirements are fairly simple and shouldn’t need any further explanation. It is worth making one comment though. Strictly speaking, only the linearity assumption needs to be met for Pearson’s correlation coefficient (\\(r\\)) to be a valid measure of association. As long as the relationship between two variables is linear, \\(r\\) produces a sensible measure. However, if the first two assumptions are not met, it is not possible to construct a valid significance test via the standard ‘parametric’ approach. In this course we will only consider the Pearson’s correlation coefficient in situations where it is appropriate to rely on this approach to calculating p-values. This means the first two assumptions need to be met. We’ll work through an example to learn about Pearson’s correlation. 22.3.1 Pearson’s product-moment correlation coefficient in R Bracken fern (Pteridium aquilinum) is a common plant in many upland areas. One concern is whether there is any association between bracken and heather (Calluna vulgaris) in these areas. To determine whether the two species are associated, an investigator sampled 22 plots at random and estimated the density of bracken and heather in each plot. The data are the mean Calluna standing crop (g m-2) and the number of bracken fronds per m2. The data are in the file BRACKEN.CSV. Read these data into a data frame, calling it bracken: bracken &lt;- read.csv(&quot;BRACKEN.CSV&quot;) glimpse(bracken) ## Observations: 22 ## Variables: 2 ## $ Calluna &lt;int&gt; 980, 760, 613, 489, 498, 416, 589, 510, 459, 680, 471,... ## $ Bracken &lt;dbl&gt; 2.3, 1.4, 4.0, 3.6, 4.3, 4.0, 6.3, 6.5, 8.3, 8.2, 8.1,... There are only two variables in this data set: Calluna and Bracken. The first thing we should do is summarise the distribution of each variable: ggplot(bracken, aes(x = Calluna)) + geom_dotplot(binwidth = 100) ggplot(bracken, aes(x = Bracken)) + geom_dotplot(binwidth = 2) It looks like we’re dealing with numeric variables (ratio scale), each of which could be normally distributed. What we really want to assess is the association. A scatter plot is obviously the best way to visualise this: ggplot(bracken, aes(x = Calluna, y = Bracken)) + geom_point() It seems clear that the two plants are negatively associated, but we should confirm this with a statistical test. We’ll base this on Pearson’s correlation. How do we know to use a correlation analysis with these data? We didn’t set out with a directional relationship in mind of the form “X causes Y”. There may be an association between the two species, but it is not obvious which should be the dependent and independent variables. Neither variable is controlled by the investigator, and we’re not interested in using one variable to predict the values of the other. Taken together, these observations indicate that correlation analysis is the appropriate method to use to evaluate the significance of the association. Why are we using Pearson’s correlation? A test based on Pearson’s correlation will be more powerful than one using Spearman’s correlation. We need to be confident that all the assumptions are met though. The scatter plot indicates that the relationship between the variables is linear, so Pearson’s correlation is a valid measure of association. Is it appropriate to carry out a significance test though? The data are of the right type—both variables are measured on a ratio scale—and the two dot plots above suggest the normality assumption is reasonable. Let’s proceed with the analysis… Carrying out a correlation analysis in R is straightforward. We use the cor.test function to do this: cor.test(~ Calluna + Bracken, method = &quot;pearson&quot;, data = bracken) ## ## Pearson&#39;s product-moment correlation ## ## data: Calluna and Bracken ## t = -5.2706, df = 20, p-value = 3.701e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.8960509 -0.5024069 ## sample estimates: ## cor ## -0.7625028 We use method = &quot;pearson&quot; to control which kind of correlation coefficient was calculated. There are three options, and although the default method is Pearson’s correlation, it is a good idea to be explicit. We use R’s formula system to determine which pair of variables are analysed. However, instead of placing a variable on the left hand side and a variable on the right hand side (e.g. Calluna ~ Bracken), both two variables appear to the right of the ~ separated by a + symbol. This convention makes good sense if you think about where we use correlation: a correlation analysis examines association, but it does not imply the existence of dependent and independent variables. To emphasise the fact that neither variable has a special status, the cor.test function expects both variables to appear to the right of the ~, with nothing on the left. The output from the cor.test is very similar to that produced by the t.test function. We won’t step through most of this output, as its meaning should be clear. The t = -5.2706, df = 20, p-value = 3.701e-05 line is the one we care about. Here are the key points: The first part says that the test statistic associated with a Pearson’s correlation coefficient is a type of t-statistic. We’re not going to spend time worrying about where this came from, other than to note that it is interpreted in exactly the same way as any other t-statistic. Next we see the degrees of freedom for the test. Can you see where this comes from? It is \\(n-2\\), where \\(n\\) is the sample size. Together, the degrees of freedom and the t-statistic determine the p-value… The t-statistic and associated p-value are generated under the null hypothesis of zero correlation (\\(r = 0\\)). Since p &lt; 0.05, we conclude that there is a statistically significant correlation between bracken and heather. What is the actual correlation between bracken and heather densities? That’s given at the bottom of the test output: \\(-0.76\\). As expected from the scatter plot, there is quite a strong negative association between bracken and heather densities. 22.3.2 Reporting the result When using Pearson’s method we report the value of the correlation coefficient, the sample size, and the p-value41. Here’s how to report the results of this analysis: There is a negative correlation between bracken and heather among the study plots (r=-0.76, n=22, p &lt; 0.001). Notice that we did not say that bracken is having a negative effect on the heather, or vice versa. 22.4 Spearman’s rank correlation The assumptions of Pearson’s correlation are not too restrictive, but if the data do not match them a non-parametric method such as Spearman’s rank correlation (\\(\\rho\\)) is the best approach. The advantages of using Spearman’s rank correlation are: 1) the two variables do not need to be normally distributed, and 2) ordinal data can be used. This means Spearman’s rank correlation can be used with data having skewed (or other odd) distributions, or with data originally collected on a rank/ordinal scale. This latter feature makes it very useful for many studies in, for example, behaviour and psychology, where the original data may have been collected on such a scale. The key assumptions of Spearman’s rank correlation are: Both variables are measured on ordinal, interval or ratio scales. There is a monotonic relationship between the two variables. A monotonic relationship occurs when, in general, the variables increase in value together, or when the values of one variable increase, the other variable tends to decrease. What this means in practice is that we should not use Spearman’s rank correlation if a scatter plot of the data forms a clear ‘hill’ or ‘valley’ shape. Spearman’s rank correlation is somewhat less powerful (roughly 91% in some evaluations) than Pearson’s method when the data are suitable for the latter. Otherwise it may even be more powerful. We’ll work through an example to learn about Spearman’s correlation… 22.4.1 Spearman’s rank correlation coefficient in R Some bird species, at a particular point in the spring, form ‘leks’—gatherings of birds, with males each defending a small area of ground, displaying, and each mating with such females as he is successful in attracting. In general, in leks, a few birds secure many matings and most birds secure rather few. In a study of lekking in black grouse, a biologist is interested in whether birds that secure many matings in one season also do better the next year. Using a population with many colour-ringed birds he is able to get data for a reasonable number of males from two leks in successive years. The data are in the file GROUSE.CSV. Read these data into a data frame, calling it grouse. grouse &lt;- read.csv(&quot;GROUSE.CSV&quot;) glimpse(grouse) ## Observations: 20 ## Variables: 2 ## $ Year1 &lt;int&gt; 3, 15, 4, 3, 4, 9, 1, 0, 2, 7, 3, 2, 6, 0, 1, 12, 2, 1, ... ## $ Year2 &lt;int&gt; 6, 8, 0, 0, 2, 10, 5, 4, 1, 4, 3, 7, 6, 2, 5, 17, 0, 0, ... Each row of the data is the number of matings for a male in the two successive leks: Year1 (year 1) and Year2 (year 2). The first thing we should do is summarise the distribution of each variable: ggplot(grouse, aes(x = Year1)) + geom_dotplot(binwidth = 2) ggplot(grouse, aes(x = Year2)) + geom_dotplot(binwidth = 2) Notice that the data are integer-valued (i.e. they are counts). These distributions seems to tie in with the biological observation that the distribution of matings is right-skewed: in both years there are only a few males that have high mating success, with most males securing only a handful of matings. Next we need to visualise the association: The data are integers, which means there is a risk of over-plotting (points will lie on top of one another). We made the points semi-transparent alpha = 0.5 to pick this up where it occurs. It seems clear that mating success is positively associated, but we should confirm this with a statistical test. We’ll base this on Spearman’s correlation. How do we know to use a correlation analysis with these data? Although there seems to be an association between the counts, it is not obvious that success in one year ‘causes’ success in another year and neither variable is controlled by the investigator. We’re also not interested in using the success measure in year 1 to predict success in year 2. This indicates that correlation analysis is the appropriate method to evaluate the significance of the association. Why are we using Spearman’s correlation? The relationship appears roughly linear, so in that regard Pearson’s correlation might be appropriate. However, the distribution of each count variable is right-skewed, which means the normality assumption is probably suspect in this instance. We’re left with no choice but to use Spearman’s correlation. Carrying out a correlation analysis using Spearman’s rank correlation in R is simple. Again, we use the cor.test function to do this: cor.test(~ Year1 + Year2, method = &quot;spearman&quot;, data = grouse) ## Warning in cor.test.default(x = c(3L, 15L, 4L, 3L, 4L, 9L, 1L, 0L, 2L, ## 7L, : Cannot compute exact p-value with ties ## ## Spearman&#39;s rank correlation rho ## ## data: Year1 and Year2 ## S = 592.12, p-value = 0.01112 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.5547952 The only other thing we had to change, compared to the Pearson’s correlation example, was to set method = &quot;spearman&quot; to specify the use of Spearman’s rank correlation. Notice the warning message: ‘Cannot compute exact p-value with ties’. This is generally not something we need to worry about for this particular test. The output is very similar to that produced by cor.test when using Pearson’s correlation. Once again, the S = 592.12, p-value = 0.01112 line is the one we care about. The main difference is that instead of a t-statistic, we end up working with a different kind of test statistic (‘S’). We aren’t going to explain where this comes from because it’s quite technical. Next we see the p-value. This is generated under the null hypothesis of zero correlation (\\(\\rho = 0\\)). Since p &lt; 0.05, we conclude that there is a statistically significant correlation between mating success in successive years. Wait, where are the degrees of freedom? Simple—there aren’t any for a Spearman’s correlation test. What is the correlation between mating success? That’s given at the bottom of the test output again: \\(+0.55\\). This says that there is a moderate, positive association between mating success in successive years, which is what we expect from the scatter plot. 22.4.2 Reporting the result When using the Spearman method it is fine to report just the value of the correlation coefficient, the sample size, and the p-value (there is no need to report the test statistic). Here’s how to report the results of this analysis: There is a positive association between the number of matings achieved by a particular male in one lek and the number the same male achieves in a subsequent lek (Spearman’s rho=0.55, n=20, p &lt; 0.05). People sometimes just refer to ‘the correlation coefficient’ without stating which one they are using. When this happens, they probably used the most common method: Pearson’s product-moment correlation.↩ People occasionally report the value of the correlation coefficient, the t-statistic, the degrees of freedom, and the p-value. We won’t do this.↩ "],
["principles-of-experimental-design.html", "Chapter 23 Principles of experimental design 23.1 Introduction 23.2 Jargon busting 23.3 Replication 23.4 Controls 23.5 Confounded and noisy experiments 23.6 Dealing with confounding effects and noise 23.7 Ethics and practicality 23.8 Further reading", " Chapter 23 Principles of experimental design Hiawatha to convince them, Organised a shooting contest. Laid out in the proper manner Of designs experimental Recommended in the textbooks Mainly used for tasting tea (but sometimes used in other cases) Still they couldn’t understand it, so they couldn’t raise objections. (Which is what so often happens with analysis of variance.) Maurice Kendal (after Longfellow) from Hiawatha Designs an Experiment 23.1 Introduction The data we use to test hypotheses may be generated by recording information from natural systems (‘observational studies’), or by carrying out some sort of experiment in which the system under study is manipulated in some way (‘experimental studies’). There is often considerable scope for deliberately arranging the system to generate data in the best way to test a particular effect when conducting experiments. For this reason we tend to use the term ‘design’ primarily in the context of experiments. However, collection of data in both situations requires thought and planning, and many of the considerations of what is termed experimental design apply equally to observational and experimental studies42. The underlying principle of experimental design is: to extract data from a system in such a way that differences or variation in the data can be unambiguously attributed to the particular process we are investigating. In order to do this we need to know how to maximise the statistical power of an experiment or data collection protocol. Statistical power is the likelihood that a study will detect an effect when there really is an effect present. Broadly speaking, statistical power is influenced by: (1) the size of the effect and (2) the size of the sample used to detect it. Bigger effects are easier to detect than smaller effects, while large samples present greater test sensitivity than small samples. A second consideration is that the less variable the material we are using the smaller the effects we will be able to detect. Given these facts, there are obviously two things to do when designing an experiment: Use the maximum feasible sample sizes. Take steps to control the variability of the organisms/material being used; of the experimental conditions; and of the methods of measurement. Exactly what combination of these is appropriate will depend on the subject area. In a physiological experiment using complex apparatus and monitoring equipment the scope for replication may be very limited. Obviously here maximum effort should be put into experimentally controlling extraneous sources of variation. With the subject material this may mean using animals of the same age, reared under the same conditions, of the same stock; it may involve using clones of plant material. It will involve running the experiment under controlled conditions of light, temperature, and require that the measurement methods are as precise as possible. On the other hand an ecologist studying an organism in the field may have relatively little scope for experimental control of either the material studied or the environmental conditions, and may be forced to make relatively crude measurements. In this case the best approach is to control what can be controlled and then try and maximise the sample size. 23.2 Jargon busting Before we delve any further into experimental design concepts we need to introduce a little bit of statistical jargon. We’ll define the important terms, and then run through an example to better understand them: An experimental unit is the physical entity which can be assigned, at random, to a treatment (see next definition). Examples of possible experimental units are individual animals, plots or quadrats, populations, or even whole communities. A treatment is any kind of manipulation applied to experimental units. A group of experimental units that all receive the same treatment is called a treatment group. Most experiments include one or more complementary groups, called control groups. The experimental units in a control group receive either no treatment or some kind of standard treatment. A factor is just a collection of related treatments and controls, and the different treatments/controls are called the levels of that factor. An experimental factor can be viewed as a variable whose levels are set by the experimenter. Here’s an example. Suppose we wanted to compare the weight loss of people on 4 different diets to determine which diet is the most effective for losing weight. We conduct an experiment in which groups of 8 volunteers follow each of the diets for one month. A fifth group of 8 volunteers serves as the control group—they follow NHS diet guidelines. At the end of the experiment we measure how much weight each person has lost over the month. In this example the volunteers are the experimental units, diets are the treatments, and the NHS group are the control group. Together, the four ‘diet types’ and the ‘no diet’ (i.e. normal eating) control constitute the five levels of the ‘weight loss group’ factor. A word of warning: it is common to lump control groups and treatment groups together and just call them ‘treatments’. This is fine, but be aware of the distinction between the two. 23.3 Replication We cannot do any statistics without understanding the idea of replication—assigning several experimental units to the same treatment or combination of treatments. Why does replication matter? Replication affects the power of a statistical test—by increasingly the replication in a study we increase the sample size available to detect specific effects. Replication is fundamental to many of the statistical methods we use, and is particularly important in biology because the material we work with is often inherently variable and hard to make precise measurements on. It seems like a simple idea: increased replication = more statistical power. We have to be careful about how we replicate though… 23.3.1 Independence and pseudoreplication An assumption of most statistical tests is that the data are independent. Independence means that the value of a measurement from one object is not affected by the values of other objects. Common sources of non-independence in biology include: genetics - e.g. if a set of mice are taken from the litter of a single female, they are more likely to be similar to each other than mice taken from the litters of several different females. geography - e.g. samples from sites close together will experience similar microclimate, have similar soil type etc. sampling within biological ‘units’ - e.g. leaves on a tree will be more similar to each other than to leaves from other trees. experimental arrangements in the lab - e.g. plants grown together in a pot, or fish kept in one aquarium will all be affected by the conditions in that pot/aquarium. Non-independence occurs at many levels in biological data, and in statistical testing the common consequence of non-independence is pseudoreplication. Pseudoreplication is an artificial increase in the sample size (and hence degrees of freedom) caused by using non-independent data. It may be easiest to see what this means by example. Imagine we are interested in whether plants of a particular species produce flowers with different numbers of petals when grown in two different soil types. We have three plants in each soil type and each plant produces 4 flowers. As it turns out, the 4 flowers within each individual plant have identical numbers of petals. If we count the petals in a single flower from each plant, and then test the difference using a t-test we get the following result: Soil type Num. Petals Mean (Plant 1) (Plant 2) (Plant 3) Soil type A 3 4 5 4 (Plant 1) (Plant 2) (Plant 3) Soil type B 4 5 6 5 p = 0.29 The difference is not significant. Now instead of sampling a single flower from each plant we count the petals of all four flowers on each plant and (incorrectly) use all the values in the analysis (giving an apparent sample size of 12 in each treatment): Soil type Num. Petals Mean (Plant 1) (Plant 2) (Plant 3) Soil type A 3, 2, 3, 4 4, 4, 3, 5 3, 6, 7, 4 4 (Plant 1) (Plant 2) (Plant 3) Soil type B 4, 5, 4, 3 5, 7, 3, 5 6, 5, 7, 6 5 p = 0.009 Even with that proviso that the data might be a bit suspect with regard to normality, the same difference in the means now appears to be highly significant! The problem here is that the flowers within each plant are not independent - there is variation among plants in petal numbers, but within the plant (perhaps for genetic reasons) the number of petals produced are similar. Because of this non-independence the apparent significance in the final result is spurious. There are only three independent entities in each soil type treatment—the plants—so the first of the two tests here is correct, the second is pseudoreplicated. To illustrate the effect in a still more obvious way, consider if we were interested in the heights of plants in the two soil types, but we actually only had one plant in Soil A and one in Soil B. If we measure the plants and find they differ somewhat in height, we cannot tell whether this is due to the soil, or just because no two plants are identical. With one plant in each soil we cannot carry out a statistical test to compare the heights. Now, if it was suggested that we measure the height of each plant 20 times and then used those numbers to do a statistical test to compare the plant heights in the two soils we would realise that this was an entirely pointless exercise. There is no more information about the effect of soil type in the two sets of 20 measurements than there was in the single measurement (except we now know how variable our measuring technique is). And why stop at 20? Why not just keep remeasuring until we have enough numbers to get a significant difference?! Clearly this is nonsense. Put this way, the pitfall of pseudoreplication seems obvious. However, it can creep into biological studies in quite subtle ways and occurs in a significant number of published studies. One very common problem occurs in ecological studies where different habitats, or experimental plots, are being compared. Say we are looking at zooplankton abundance in two lakes, one with fish and one without. We would normally take a number of samples from each lake and could obviously compare the zooplankton numbers between these two sets of samples. It would be tempting to attribute any differences we observe to the effect of fish. However this would not be correct. We have measured the difference in zooplankton between the two lakes (and this is quite a valid thing to do) but the lakes may differ in any number of ways, not just the presence of fish, so it is not correct to interpret our result in relation to the effect of fish. To do this, we would really need data on zooplankton abundance in several lakes with fish, and several without. In other words, for testing the effect of fish, our replicates should be whole lakes with and without the relevant factor (fish), not samples from within a single lake. But surely it is still better to take lots of samples from each site than just one; it must give a more accurate picture? This is true. Taking several measurements or samples from each object guards against the possibility of the results being influenced by a single, possibly unusual, sample; so the accuracy of the information about the object is increased. It would be much more reliable to have twenty zooplankton samples from a lake than just one. This is important, but it is not the same as having measurements from more objects (lakes)—true replication—which increases the power of the statistical test to detect differences among objects with respect to the particular factor (e.g. fish / no fish) we are interested in. So in cases such as those above, the best strategy would be to measure petal number on all the flowers on each plant, but then calculate a mean for each plant and use those means in the statistical test. The same idea applies in the lake situation—several plankton samples could be taken from each of a number of lakes, then combined to give one estimate of plankton density for each lake. Though of course, we couldn’t do much in the way of statistical analysis on the two means. So, in summary, when carrying out an investigation the key question to ask is: What is the biological unit of replication relevant to the effect we trying to test? As this implies, the appropriate unit of replication may vary depending on what we are investigating. If we want to test for a difference in the plankton density between two lakes, then taking 10 samples from each lake and comparing them would be the correct approach. But if, as above, we wanted to assess the effect of fish on plankton density, it would be inappropriate—the correct unit of replication in this case is the whole lake and we would therefore want to sample several lakes with and without fish. 23.4 Controls We are told repeatedly, probably starting at primary school, that every experiment must have a control—a reference treatment against which the other treatments can be compared. The idea does, however, sometimes generate confusion since it is not always clear what is being controlled for, and some experiments do not require a control while others require more than one. In some cases the appropriate control is obvious. In a toxicity test we are interested in the mortality due to the toxicant, and clearly we want the control to tell us what the background mortality rate (without toxicant) would be under those experimental conditions. However, if we are measuring the movement rates of slugs on surfaces of differing moisture content there is no control required — indeed none possible. Slugs encounter many different moisture conditions in their daily lives and there isn’t a ‘control’ moisture level. So the first message is that there may not be a control for all experiments. More tricky is the situation where the objects we are investigating are affected not just by the treatment we are administering, but also by other effects of applying that treatment. This too can sometimes be addressed by the use of control treatments, but these are now not simply the ‘natural’ situation, they may have to be quite specifically designed to mimic certain aspects of the experiment, and not others. These sorts of controls are discussed in more detail below. 23.5 Confounded and noisy experiments Unwanted variation comes in two forms. The first is confounding variation. This occurs when there are one or more other sources of variation that work in parallel to the factor we are investigating and make it hard, or impossible, to unambiguously attribute any effects we see to a single cause. Confounding variation is particularly problematic in observational studies because, by definition, we don’t manipulate the factors we’re interested in. The second is noise. This describes variation that is unrelated to the factor we are investigating but adds variability to the results so that it is harder to see, and detect statistically, any effect of that factor. As noted above, much of experimental design is about improving our ability to account for noise in a statistical analysis. We will consider these together, as some of the techniques for dealing with them are be applicable to both. 23.5.1 Confounding The potential for confounding effects may sometimes be easy to recognise. If we measure growth rates in plants growing at sites of differing altitude, there are several factors which all change systematically with altitude (temperature, ultraviolet radiation, precipitation, wind speed etc.) and it may be hard to use such data to examine effects of any one of these factors alone. The important thing to remember is that observing a relationship between two variables (e.g. a negative relationship between plant growth and increased precipitation up a mountain) does not necessarily indicate a causal link (plant growth may be determined by one or more of the other factors that vary with altitude). Confounding effects can also be much more subtle. We may find that eagle owls take more large Norway rats at a particular time of year—but the factor we are interested in (rat size) is related to sex (males are larger) and the males spend more time moving around (hence out of cover and exposed to predation) at that time of year. So what seems to be a size effect, may actually be produced by sex-specific behaviour and not due to eagle owls selecting larger prey at all. Confounding doesn’t just occur in observational studies. Confounding occurs when administration of a treatment itself generates other unwanted effects where the treatment is applied. An example might be in the administration of nutrients to plants. Changing the supply of nitrogen may be done by supplying different levels of a nitrate (NO3) salt (e.g. Mg(NO3)2 or Ca(NO3)2), but how can we be sure that the effects we see are a consequence of nitrogen addition, rather than effects of the magnesium or calcium cations? 23.5.2 Noise Noise in the data can be generated by the same processes that generate confounding. The difference is that noise is generated even when the confounding factors don’t align with the treatments. So, going back to measuring growth rates in plants, if we were looking at growth rates of different subspecies of plant on a mountain then we might find that we can get five samples from each different subspecies, but the samples are scattered across very different altitudes on the mountain. This will add variation to the estimates of growth rate due to effects of altitude—this variation is unwanted noise. On the other hand, we might find that the subspecies each grow predominantly at different altitudes and in this situation the variation due to altitude is confounded with the variation due to subspecies—we cannot tell whether the subspecies are inherently different, or the differences are just down to altitude. 23.6 Dealing with confounding effects and noise Confounding effects occur often in biological work and noise of some sort is always present. Techniques for dealing with such effects include: randomisation blocking experimental control additional treatments. We’ll consider each of these in turn… 23.6.1 Randomisation Randomisation is fundamental to experimental design. Although there may be specific confounding factors we can identify and explicitly counter using experimental techniques, we can never anticipate all such factors. Randomisation provides an ‘insurance’ against the unpredictable confounding effects encountered in experiments. The basic principle is that each experimental unit should be selected, or allocated to a particular treatment, ‘at random’. This may involve selecting which patients to give a drug and which a placebo at random or it may involve setting out experimental plots at random locations in a field. The important thing is that of all the possible patients or plots, the ones that get a particular treatment are randomly selected. Randomisation guards against a variety of possible biases and confounding effects, including the inadvertent biases that might be introduced simply in the process of setting up an experiment. For example, if in a toxicological experiment with freshwater invertebrates the chemical treatment is set up first and then the control, it may be that the animals caught most easily from the stock tank (the largest? the weakest?) will all end up in the chemical treatment and the remainder in the control, with consequent bias in the death rates observed in the subsequent experiment. Randomisation is a critical method for guarding against confounding effects. It is the best insurance we have against unwittingly getting some other factor working in parallel to a treatment. It does not, of course, do anything to reduce noise in the data, in fact if randomisation removes confounding effectively, it can appear to increase that variation—but it is a necessary cost to pay for being able to interpret treatment effects correctly. What does ‘at random’ mean in practise? The random bit of the word randomisation has a specific meaning: objects chosen ‘at random’ are chosen independently with equal probabilities. How do we achieve this in practice? First we need a set of random numbers. For example, if we need to assign 10 experimental units to treatments we might start with a set of random integers: 4, 3, 5, 8, 7, 1, 10, 9, 6, 2. Attaining a set of random numbers is easy enough. Tables of random numbers are published in most statistics books expressly for use in setting up experiments, or R can also be used to find a set of random numbers (e.g. sample(1:10)). Exactly how these numbers are used in setting up the experiment will depend on what is practical. In the toxicological experiment the best thing to do would be to place animals in each of the test containers to be used for the experiment, number each container and then use the first half of the set of random numbers to randomly select half the containers to be the test and use the remainder as the controls. In a field experiment, a grid could be mapped out and pairs of random numbers used to select co-ordinates at random for each plot—in this case we would generate random co-ordinate values instead of using integers. 23.6.2 Blocking Another way of tackling potential confounding effects, and the general heterogeneity of biological material leading to noise, is organise experimental material into ‘blocks’. This technique, called blocking, is arguably the most important experimental design concept after replication. It works as follows: Group the objects being studied into blocks such that variation among objects within blocks is small; variation between blocks may be larger. Each treatment should occur at least once within each block43. For example, in an experiment in which mice are reared on three different diets (I, II, III), we might expect the responses of mice from within a particular litter to be fairly similar to each other, but they might be rather different to the responses of mice from different litters. If we have five litters of mice (A … E) it would be sensible to select three mice from each litter (at random) to be allocated to each treatment. I \\(A_{1}\\) \\(B_{1}\\) \\(C_{1}\\) \\(D_{1}\\) \\(E_{1}\\) II \\(A_{2}\\) \\(B_{2}\\) \\(C_{2}\\) \\(D_{2}\\) \\(E_{2}\\) II \\(A_{3}\\) \\(B_{3}\\) \\(C_{3}\\) \\(D_{3}\\) \\(E_{3}\\) (Where \\(A_{1}\\) is the first randomly chosen animal from litter \\(A\\), \\(A_{2}\\) the second, etc..). This type of blocking should, if there are differences between litters, increase the power of the experiment to detect effects of the treatment and guards against the possibility that we might by chance end up with one diet having mice from, say, only two litters. In the case of only two treatments (e.g. if we just had diets I and II), this type of blocking is simply the pairing of treatments we have encountered in the paired-sample t-test. Blocked designs with more than two blocks are typically analysed using Analysis of Variance (ANOVA). We will learn how to apply ANOVA to a blocked experimental design this in later chapters. Note that randomisation is important here also. Mice were selected at random from each litter to be allocated to each treatment and litters are essentially ‘random’ in the sense that they are not deliberately chosen to be different in any particular way, we just anticipate that they are likely to be different in some ways. Blocking crops up in all sorts of experimental (and non-experimental) study designs. Some examples are given below. If plants in an experiment on soil water levels are being grown in pots on greenhouse benches, there may be differences in light or temperature at differing distances from the glass. Treatments could be blocked along the gradient—at each position on the bench we have one pot from each treatment. This way, every treatment is represented at each position along the gradient. If a field experiment involving several treatments is set up in an environment known to have some spatial variation (e.g., different parts of a field, sections of a river, etc.) setting up one replicate of each treatment in blocks at different locations ensures that no one treatment ends up confounded by some environmental difference, and helps remove noise due to environmental effects in the final analysis. An immunity response is being tested using insects kept in a parallel set of laboratory cultures. There are insufficient insects from a single culture to run the whole experiment, so we could set up one replicate of each treatment using insects from each culture. The cultures would be the blocks—we are not particularly interested in the differences between cultures, but we want to be able to control and remove any variation due to differences between cultures, so as to stand the best chance of detecting a treatment effect. In a comparison of three new diagnostic techniques for measuring the frequency of abnormalities in tissue samples the techniques could be dependent on the person who carries them out (experience, standard of working etc.). The same workers could carry out all three techniques (in random order) and the results compared using individual workers as blocks to increase the power of the analysis to detect differences. If the process of collecting and analysing samples from an experiment is very time consuming (relative to the rate at which things might change) then we could block the experiment in time. Set up one replicate of each treatment on each of a sequence of days, and then collect the samples after a particular time, again over the same sequence of days. Each replicate has then been run for the same length of time (we would randomize the order in which treatments were sampled each day), and we could then include ‘days’ as a block within the analysis to control for any unknown differences resulting from the different setup, or sample days. It’s worth saying again: blocking is one of the most important experimental design concepts. Most experimental settings lend themselves to some kind of blocking scheme. If there is a way to block an experiment, we should do it. Why? Because a blocked experiment is more powerful, in the statistical sense, than the equivalent non-blocked version. That is, a study is more likely to detect an effect if it uses a blocked design. We will see how to analyse a blocked experiment in a later chapter. 23.6.3 Experimental control It is obvious that some unwanted variation in data will arise if there is poor measurement, or careless implementation of the treatments (imprecise administration of doses, sloppy timing of trial periods, etc.). In every study we do we should look at the ‘protocol’ issues and see if they can be made tighter. This means considering the precision of the measurements we are making, etc. in relation to the sizes of effects we are interested in, and the resources available to carry out the work. There would be no point in timing measurement intervals over which seedling growth was determined to the millisecond, or determining the soil pH to 5 decimal places, but it would be good to measure seedling height using a standard approach (natural growth form, or stretched out to maximum length? Starting from where?) and to the nearest millimetre, rather than centimetre. A second form of experimental control is where we can use experimental manipulation of some sort to control for factors that might vary among replicates or treatments. At its simplest obviously this involves controlling the other conditions (for example temperature) so that all treatments experience identical conditions (though note that it may not always be necessary for the conditions to be constant—it may be sufficient that whatever variation occurs is the same for all treatments). More complex problems arise where the unwanted variation is directly produced as a by product of the treatment we are administering (confounding again). So for example, if we were interested in the effect of decomposition of leaf litter on the microbial communities in soils we might have an experimental treatment that involves varying the amount of leaf litter placed on the soil surface in the test plots. The problem is that this will vary not just the amount of decomposing material entering the soil, but also the physical presence of the leaf litter layer will affect the microclimate at the soil surface (so for example the dryness in the surface of the soil). So we might create some sort of artificial litter which can be mixed in with the real litter , but which does not decompose, so that each plot has a constant volume of ‘litter’ on the surface, but different amounts of decomposing material entering the soil. Other situations in which this type of experimental ‘adjustment’ can be used include experiments in which different nutrient solutions have to be adjusted so that they have the same pH or where different temperature treatments have to have humidity adjusted to ensure that it remains constant. In general this type of approach can be very useful but it depends on the necessary adjustment being known, and sometimes requires continuous monitoring to keep the adjustments correct. 23.6.4 Additional treatments: ‘designing in’ unwanted variation Often we are faced by situations in which the unwanted variation — in particular confounding effects — cannot be removed by manipulating the treatments themselves, but has to be tackled by creating additional treatments whose function is to measure the extent of the unwanted variation, and then allow us to remove it statistically, from the data after the experiment is done. In other words, instead of just designing the experiment with the factor we are interested in, we ‘design in’ the sources of unwanted variation. 23.6.4.1 Transplants and cross-factoring Imagine we had an investigation that involved looking at effects of air pollution on the ability of trees to defend themselves chemically against attack by leaf-mining insects. The obvious thing to do would be to look at trees along a gradient of air pollution and monitor leaf damage by the insects. We might find that the trees in polluted areas are more attacked by the insects. However the problem here is that the trees growing in areas of high air pollution might be attacked more because they are stressed and less able to invest resources in defending themselves (as we hypothesised), or because the insects are more abundant there because their own natural enemies (birds and parasitoids) are less abundant in areas of high air pollution and so cannot control the abundance of the leaf-miners. One way of escaping this confounding effect would be to take tree saplings from polluted and unpolluted areas and do reciprocal transplants — moving trees from polluted areas into clean areas, and vice versa. This then enables us to separate out to a large extent the effect of tree quality from the effect of insect abundance as we can compare trees that have grown with and without air pollution, in both polluted and unpolluted areas. It is also possible that by careful choice of location, or other elements of design, we can include the unwanted variation as an additional factor in the design without necessarily physically manipulating the subjects, but by sampling material systematically with regard to both the thing we are interested in and the additional unwanted factor(s), so that we can cross-factor the two. For example, if we were interested in how habitat use determines gut parasite load in dogfish, then we might sample dogfish from different habitats, but also record the sex, and age, or size, of the fish. It would then be possible to separate out the effects of sex, or age, from those of where the fish were living. If we didn’t do this, then both factors would probably contribute unwanted variation, either noise, or possibly confounding effects (for example male and female dogfish have somewhat different habitat preferences). 23.6.4.2 Procedural controls Confounding effects are not only a problem along natural gradients, they can often be introduced by the experimental procedures. For example, a marine biologist investigating the effect of crab predation on the density of bivalve molluscs in an estuarine ecosystem might have cages on the mud flats from which crabs are removed, and in which any change in bivalve settlement and survival can be monitored. The obvious control for this would be equivalent plots on the adjacent mudflats with normal crab numbers. Obviously if the experiment just compares the bivalve density in cages with reduced crab numbers and in the adjacent mud flat any effects observed could be attributable to crab density, environmental changes brought about by the cages, or disturbance due to the repeated netting to remove crabs. To address this problem there are several additional controls that might be useful here. In addition to the proper treatment, bivalve density could be monitored in: a ‘no cage / no disturbance control’—open mud flat adjacent to the experiment (so no cage effects, no added disturbance). a ‘cage control’—crabs at normal density but with a cage (usually done as cage with openings to allow crabs to enter and leave). a ‘disturbance control’—crabs at normal densities, but subject to the same disturbance as the reduced density treatments (cages netted to remove crabs, but all crabs returned to the cages) The latter two could be combined if it wasn’t important to separate disturbance and cage effects, but even so in some circumstances it is quite possible for an experiment to have as many controls as there are actual treatments. The additional treatments in this sort of situation are effectively additional controls—in fact they be termed procedural controls—but they are not simply the natural ‘background’ conditions. A classic example of this type of control is the use of placebo treatments in medical trials. For example if we are investigating the effect of a drug then there may be a confounding effect due to psychological, behavioural or even physiological changes in patients resulting simply from the process of being treated, rather than any active compound in the drug. It is common, therefore, to give the drug to one group of patients and a ‘placebo’ (equivalent treatment process, but with no active component in the substance administered) to another group. The placebo is a secondary manipulation designed to equalise the effect of simply ‘being treated’. There are many other examples of similar experimental controls: a treatment involving surgical implantation of some sort of device, may require a control group who have the surgery, but without the implantation itself, or even with implantation of an inactive device, to allow us to factor out the confounding effect of surgical trauma, or the body’s reaction to the implant itself. 23.7 Ethics and practicality Although experimental design is often fairly straight forward in principle, the ideal design to test an hypothesis may turn out to be impractical, unaffordable or unethical. All experiments are constrained by practicality, most by finance and a rather smaller, but important set, by ethical considerations. Ethical factors obviously constrain experiments in subjects such as psychology and animal physiology and even in ecology where experiments in studies of rare species, species introductions, or environmental damage may be technically possible, but ethically unacceptable. However, nowhere is the problem more pronounced than in medicine. Drug testing presents the classic difficulty. Effective testing of the efficacy of a drug depends on the comparison of patients receiving the drug with closely equivalent patients not doing so, or receiving some alternative treatment. Since it is highly likely that one of the treatments will be better than another, then by definition, at least one group of people are having an available and better treatment withheld from them (e.g., Aspinal and Goodman 1995). Thus, as soon as the experimental evidence gives some indication of which treatment is best, it is very hard to justify withholding it from all patients, even if the experimenter feels that further work is necessary. Good experimental design and appropriate analysis cannot remove ethical, practical or financial problems, but they can help to ensure that where time and money are invested in investigating a problem, the maximum useful information is returned. 23.8 Further reading Barnard, C., Gilbert, F. and McGregor, P. (2007) Asking questions in biology. Longman. Ruxton, G. D. and Colegrave, N. (2010) Experimental design for the life sciences. Oxford Univ. Press. It is worth noting that in reports experiment and observation should always be distinguished. If we have carried out observations on a natural system of any sort, but where there has been no experimental manipulation of any aspect of the system, that is not an experiment. It would be inappropriate to write in a report: “This experiment consisted of measuring mean stomatal density from thirty trees growing at a range of altitudes.” Instead, we might write: “We conducted an observational study measuring mean stomatal density from thirty trees growing at a range of altitudes.”↩ Actually, there are special types of experimental design that use blocking, but where each treatment does not appear in every block. These are much more advanced than anything we will cover in this book.↩ "],
["introduction-to-one-way-anova.html", "Chapter 24 Introduction to one-way ANOVA 24.1 Introduction 24.2 Why do we need ANOVA models? 24.3 How does ANOVA work? 24.4 Different kinds of ANOVA model 24.5 Some common questions about ANOVA", " Chapter 24 Introduction to one-way ANOVA 24.1 Introduction The two-sample t-tests evaluate whether or not the mean of a numeric variable changes among two groups or experimental conditions. At the beginning of the Relationships and regression chapter we pointed out that the different groups/conditions can be encoded by a categorical variable. We pointed out that we could conceptualise these t-tests as evaluating a relationship between between the numeric and categorical variable. The obvious question is, what happens if we need to evaluate differences among means of more than two groups? The ‘obvious’ thing to do might seem to be to test each pair of means using a t-test. However this procedure is tedious and, most importantly, statistically flawed. In this chapter we will introduce an alternative method that allows us to assess the statistical significance of differences among several means at the same time. This method is called Analysis of Variance (abbreviated to ANOVA). ANOVA is one of those statistical terms that unfortunately has two slightly different meanings: In its most general sense ANOVA refers to a methodology for evaluating statistical significance. It appears when working with a statistical model known as the ‘general linear model’. Simple linear regression is a special case of the general linear model. Essentially, whenever we see an F-ratio in a statistical test we’re carrying out an Analysis of Variance of some kind. We saw this crop up when we tested the significance of the regression slope. In its more narrow sense the term ANOVA is used to describe a particular type of statistical model. When used like this ANOVA refers to models that compare means among two or more groups (ANOVA models are also examples of general linear models). The ANOVA-as-a-model is the focus of this chapter. ANOVA models underpin the analysis of many different kinds of experimental data; they are one of the main ‘work horses’ of basic data analysis. As with many statistical models we can use ANOVA without really understanding the details of how it works. However, when it comes to interpreting the results of statistical tests associated with ANOVA, it is important to at least have a basic conceptual understanding of how it works. The goal of this chapter is to provide this basic understanding. We’ll do this by exploring the simplest type of ANOVA model: a one-way Analysis of Variance. 24.2 Why do we need ANOVA models? Let’s consider the diet example from the previous chapter. We wanted to compare the weight loss of people on 4 different diets to determine which diet is the most effective for losing weight, so we conducted an experiment in which groups of 8 volunteers follow one of the diets for a month. A fifth group of 8 volunteers served as the control group—they follow NHS eating guidelines. At the end of the experiment we measured how much weight each person had lost over the month. Once the data have been collected we need to understand the results. The weight loss of 8 volunteers on each of the diets could be plotted (this is the raw data), along with the means of each diet group, the standard error of the mean, and the sample mean of all the data: The grey points are the raw data, the means and standard error of each group are in blue, and the overall sample mean is shown by the dashed red line. We can see that there seem to be differences among the means: people in each of the different groups often deviate quite a lot from the overall average for all the people in the study (the dashed line). This would seem likely to be an effect of the diet they are on. At the same time, there is still a lot of variation within each of the groups: not everyone on the same diet has the same weight loss. Perhaps all of this could be explained away as sampling variation—i.e. the diet plans make no difference at all to weight loss. Obviously we need to apply a statistical test to decide whether these differences are ‘real’. It might be tempting to use t-tests to compare each mean value with every other. However, this would it would involve 10 t-tests. Remember, if there is no effect of diet, each time we do a t-test there is a chance that we will get a false significant result. If we use the conventional p = 0.05 significance level, there is a 1 in 20 chance of getting such ‘false positives’. Doing a large number of such tests increases the overall risk of finding a false positive. In fact doing ten t-tests on all possible comparisons of the 5 different diets gives about 40% chance of at least one test giving a false significant difference, even though each individual test is conducted with p = 0.05. That doesn’t sound like a very good way to do science. We need a reliable way to determine whether there is a significance of differences between several means without increasing the chance of getting a spurious result. That’s the job of Analysis of Variance (ANOVA). Just as a two sample t-test compares means between two groups, ANOVA compares means among two or more groups. The fundamental job of an ANOVA model is to compares means. So why is it called Analysis of Variance? Let’s find out… 24.3 How does ANOVA work? The key to understanding ANOVA is to realise that it works by examining the magnitudes of different sources of variation in the data. We start with the total variation—the variation among all the units in the study—and then partition this into two sources: Variation due to the effect of experimental treatments or control groups. This is called the ‘between-group’ variation. This describes that variability that can be attributed to the different groups in the data (e.g. the diet groups). This is the same as the ‘explained variation’ described in the Relationships and regression chapter. It quantifies the variation that is ‘explained’ by the different means. Variation due to other sources. This second source of variation is usually referred to as the ‘within-group’ variation because it applies to experimental units within each group. This quantifies the variation due to everything else that isn’t accounted for by the treatments. Within-group variation is also called the ‘error variation’. We’ll mostly use this latter term because it is a bit more general. ANOVA does compare means, but it does this by looking at changes in variation. That might seem odd, but it works! If the amount of variation among treatments is sufficiently large compared to the within-group variation, this suggests that the treatments are probably having an effect. This means that in order to understand ANOVA we have to keep three sources of variation in mind: the total variation, the between-group variation, and the error variation. We’ll get a sense of how this works by carrying on with the diet example. We’ll look at how to quantify the different sources of variation, and then move on to evaluate statistical significance using these quantities. The thing to keep in mind is that the logic of these calculations is no different from that used to carry out a regression analysis. The only real difference is that instead of fitting a line through the data, we fit means to different groups when working with an ANOVA model. Total variation The figure below shows the weight loss of each individual in the study and the grand mean (i.e. we have not plotted the group-specific means). The vertical lines show the distance between each observation and the grand mean—we have ordered the data within each group so that the plot is a little tidier. A positive deviation occurs when a point is above the line, and a negative deviation corresponds to a case where the point is below the line. We’re not interested in the direction of these deviations. What we need to quantify is the variability of the deviations, which is a feature of their magnitude (the length of the lines). What measure of variability should we use? We can’t add up the deviations because they add to zero. Instead, we apply the same idea introduced in the regression chapter: the measure of variability we need is based on the ‘sum of squares’ (abbreviated SS) of the deviations. A sum of squares is calculated by taking each deviation in turn, squaring it, and adding up the squared values. Here are the numeric values of the deviations shown graphically above: ## [1] -0.505 0.195 -0.105 -0.205 -0.305 -0.405 -0.805 0.195 -0.305 -0.005 ## [11] -0.805 0.395 0.495 0.095 0.695 0.195 0.195 0.595 -0.105 -0.205 ## [21] 0.195 0.195 -0.305 -0.205 0.395 -1.305 -0.805 -0.205 -0.705 -0.105 ## [31] 0.195 -0.505 -0.105 0.695 0.595 0.695 0.695 0.095 0.895 0.295 The sum of squares of these numbers is 9.68. This is called the total sum of squares, because this measure of variability completely ignores the information about treatment groups. It is a measure of the total variability in the data, calculated relative to the grand mean. Residual variation The next component of variability we need relates to the within-group variation. Let’s replot the original figure showing the weight loss of each individual (points), the mean of each diet group (horizontal blue lines), and the grand mean: The vertical lines show something new this time. They display the distance between each observation and the group-specific means, which means they summarise the variation among individuals within treatment groups. Here are the numeric values of these deviations: ## [1] -0.9250 -0.5625 -0.9000 -0.4250 -0.3250 -0.2625 -0.1250 -0.1625 ## [9] -0.0625 -0.4000 -0.3500 0.0375 -0.2500 -0.2500 0.1750 0.1375 ## [17] -0.1500 0.2750 -0.5875 -0.1000 0.0000 -0.3875 0.4375 0.4375 ## [25] 0.1000 0.1500 0.1500 0.1500 0.5750 -0.1875 0.3000 0.7750 ## [33] 0.4000 0.5500 0.1125 0.6000 0.2125 0.2125 0.2125 0.4125 These values are a type of residual: they quantify the variation that is ‘left over’ after accounting for differences due to treatment groups. Once again, we can summarise this variability as a single number by calculating the associated sum of squares, calculated by taking each deviation in turn, squaring it, and adding up the squared values. The sum of squares of these numbers is (6.1). This is called the residual sum of squares44. It is a measure of the variability that may be attributed to differences among individuals after controlling for the effect of different groups. Between-group variation The last component of variability we need relates to the between group variation. We’ll replot the figure one more time, but this time we’ll show just the group-specific means (blue points), the overall grand mean (dashed red line), and the deviations of each group mean from the grand mean: Now the vertical lines show the distance between each group-specific mean and the grand mean. We have five different treatment groups, so there are only five lines. These lines show the variation due to differences among treatment groups. Here are the numeric values of these deviations: ## [1] -0.2425 0.0950 0.0450 -0.3800 0.4825 These values quantify the variation that can be attributed to differences among treatments. Once again, we can summarise this variability as a single number by calculating the associated sum of squares—this number is called the treatment sum of squares. This is the same as the ‘explained sum of squares’ discussed in the context of regression. It is a measure of the variability attributed to differences among treatments. This is 0.45 in the diets example. Notice that this is much smaller than the total sum of squares and the residual sum of squares. This isn’t all that surprising as it is based on five numbers, whereas the other two measures of variability are based on all the observations. 24.3.1 Degrees of freedom The problem with the raw sums of squares in ANOVA is that they are a function of sample size and the number of groups. In order to be useful, we need to convert them into measures of variability that don’t scale with sample size. We use degrees of freedom (written as df, or d.f.) to do this. We came across the concept of degrees of freedom when we studied regression: the degrees of freedom associated with a sum of squares is a measure of how much ‘information’ it is based on. Each of the three sums of squares we just calculated has a different degrees of freedom calculation associated with it: Total d.f. = (Number of observations - 1) Treatment d.f. = (Number of treatment groups - 1) Error d.f. = (Number of observations - Number of treatment groups) The way to think about these is as follows. We start out with a degrees of freedom that is equal to the total number of deviations associated with a sum of squares. We ‘lose’ one degree of freedom for every mean we have to calculate to work out the deviations. Here is how this works in the diet example: Total d.f. = 40 - 1 = 39 — The total sum of squares was calculated using all 40 observations in the data, and the deviations were calculated relative to 1 mean (the grand mean). Treatment d.f. = 5 - 1 = 4 — The treatment sum of squares was calculated using the 5 treatment group means, and the deviations were calculated relative to 1 mean (the grand mean). Error d.f. = 40 - 5 = 35 — The error sum of squares was calculated using all 40 observations in the data, and the deviations were calculated relative to 5 means (the treatment group means). Don’t worry too much if that seems confusing. We generally don’t have to carry out degrees of freedom calculations by hand because R will do them for us. We have explained them because knowing where they come from helps us understand the output of an ANOVA significance test. 24.3.2 Mean squares, variance ratios, and F-tests Once we know how to calculate the degrees of freedom we can use them to standardise each of the sums of squares. The calculations are very simple. We take each sum of squares and divide it by its associated degrees of freedom. The resulting quantity is called a mean square (abbreviated as MS): \\[ \\text{Mean square} = \\frac{\\text{Sum of squares}}{\\text{Degrees of freedom}} \\] We stated what a mean square represents when discussing regression: it is an estimate of a variance. The mean squares from an ANOVA quantify the variability of the whole sample (total MS), the variability explained by treatment group (treatment MS), and the unexplained residual variation (residual MS). ANOVA quantifies how strong the treatment effect is by comparing the treatment mean square to the residual mean square. When the treatment MS is large relative to the residual MS this suggests that the treatments are more likely to be having an effect. In reality, they are compared by calculating the ratio between them (designated by the letter F): \\[F = \\mbox{Variance ratio} = \\frac{\\mbox{Variance due to treatments}}{\\mbox{Error variance}}\\] This is the same as the F-ratio mentioned in the context of regression. When the variation among treatment means (treatment MS) is large compared to the variation due to other factors (residual MS) then the F-ratio will be large too. If the variation among treatment means is small relative to the residual variation then the F-ratio will be small. How do we decide when the F-ratio is large enough? That is, how do we judge a result to be statistically significant? We play out the usual ‘gambit’: We assume that there is actually no difference between the population means of each treatment group. That is, we hypothesise that the data in each group are sampled from a single population with one mean. Next, we use information in the sample to help us work out what would happen if we were to repeatedly take samples in this hypothetical situation. The ‘information’ in this case are the mean squares. We then ask, ‘if there is no difference between the groups, what is the probability that we would observe a variance ratio that is the same as, or more extreme than, the one we actually observed in the sample?’ If the observed variance ratio is sufficiently improbable, then we conclude that we have found a ‘statistically significant’ result, i.e. one that is inconsistent with the hypothesis of no difference. In order to work through these calculations we make one key assumption about the population from which the data in each treatment group has been sampled. We assume that the residuals are normally distributed. Once we make this assumption the distribution of the F-ratio under the null hypothesis (the ‘null distribution’) has a particular form: it follows an F distribution. This means we assess the statistical significance of differences between means by comparing the F-ratio calculated from a sample of data to the theoretical F distribution. This procedure is a type of F-test—it is really no different from the significance testing methodology outlined for regression models. The important message is that ANOVA works by making just one comparison: the treatment variation and the error variation, rather than the ten t-tests that would have been required to compare all the pairs. 24.3.3 Assumptions of ANOVA We just discussed the main assumption that makes ANOVA work. This is the normality of residuals assumption. There are other assumptions that must be met for an ANOVA analysis to be valid: Independence. If the experimental units of the data are not independent, then the p-values generated by an F-test in an ANOVA will not be reliable. This one is important. Even mild non-independence can be a serious problem. Measurement scale. The variable that you are working with should be measured on an interval or ratio scale. Equal variance. The validity of F-tests associated with ANOVA depends on an assumption of equal variance in the treatment groups. If this assumption is not supported by the data, then it needs to be addressed. If you ignore it, the p-values that you generate cannot be trusted. There is a version of one-way ANOVA that can work with unequal variances, but we won’t study it in this course. Normality. The validity of F-tests associated with ANOVA also depends on the assumption of normality. ANOVA is reasonably robust to small departures from normality, but larger departures can start to matter. Unlike the t-test, having a large number of samples doesn’t make this assumption less important. Strictly speaking, assumptions 3 and 4 really apply to the (unobserved) population from which the experimental samples are derived, i.e., the equal variance and normality assumptions are with respect to the variable of interest in the population. However, we often just informally refer to ‘the data’ when discussing the assumptions of ANOVA. 24.4 Different kinds of ANOVA model There are many different flavours of ANOVA model. The one we’ve just been learning about is called a one-way ANOVA. It’s called one-way ANOVA because it involves only one factor: diet type (this includes the control). If we had considered two factors—e.g. diet type and exercise regime—we would have to use something called a two-way ANOVA. A degin with three factors is called a three-way ANOVA, and… you get the idea. There are many other ANOVA models, each of which is used to analyse a specific type of experimental design. We are only going to consider three different types of ANOVA in this book: one-way ANOVA, two-way ANOVA, and ANOVA for one-way, blocked design experiments. 24.5 Some common questions about ANOVA To finish off with, three common questions that often arise: 24.5.1 Can ANOVA only be applied to experimental data? We have been discussing ANOVA in the context of a designed experiment (i.e. we talked about treatments and control groups). Although ANOVA was developed to analyse experimental data—and that is where it is most powerful—it can be used in an observational setting. As long as we’re careful about how we sample different kinds of groups (i.e. at random), we can use ANOVA to analyse differences between them. The main difference between ANOVA for experimental and observational studies arises in the interpretation of the results. If the data aren’t experimental, we can’t say anything concrete about the causal nature of the among-group differences we observe. 24.5.2 Do we need equal replication? So far we have only considered the use of ANOVA with data in which each treatment has equal replication. One of the frequent problems with biological data is we often don’t have equal replication, even if we started with equal replication in our design. Plants and animals have a habit of dying before we have gathered all our data; a pot may get dropped, a culture contaminated, all sorts of things conspire to upset even the best designed experiments. Fortunately one-way ANOVA does not require equal replication, it will work even where sample sizes differ between treatments. 24.5.3 Can ANOVA be done with only two treatments? Although the t-test provides a convenient way of testing means from two treatments, there is nothing to stop you doing an ANOVA on two treatments. A t-test (assuming equal variances) and ANOVA on the same data should give the same p-value (in fact the F-statistic from the ANOVA will be the square of the t-value from the t-test). One advantage to the t-test, however, is that you can do the version of the test that allows for unequal variances—something a standard ANOVA does not do. There is a version of Welch’s test for one-way ANOVA, but we won’t study it in this course (look at the oneway.test function if you are interested). You will sometimes see something called error sum of squares, or possibly, the within-group sum of squares. These are just different names for the residual sum of squares.↩ "],
["one-way-anova-in-r.html", "Chapter 25 One-way ANOVA in R 25.1 Introduction 25.2 Factors in R 25.3 Roughly checking the assumptions 25.4 Fitting the ANOVA model 25.5 Diagnostics 25.6 Interpreting the results 25.7 Summarising and presenting the results of ANOVA", " Chapter 25 One-way ANOVA in R 25.1 Introduction Our goal in this chapter is to learn how to work with one-way ANOVA models in R. Just as we did with regression, we’ll do this by working through an example—the diets example from the last chapter. We’ll start with the problem and the data, and then work through model fitting, evaluating assumptions, significance testing, and finally, presenting the results. Before we can do this though, we need to side track a bit and learn about ‘factors’ in R… Walk through You should begin working through the diet example from this point. You will need to download the DIET_EFFECTS.CSV file from MOLE and place it in your working directory. 25.2 Factors in R Remember factors? An experimental factor is a controlled variable whose levels (‘values’) are set by the experimenter. R is primarily designed to carry out data analysis so we shouldn’t be surprised that it has a special type of vector to represent factors. This kind of vector in R is called, rather sensibly, a factor. We have largely ignored factors in R up until now because we haven’t needed to use them45. However, we now need to understand how they work because much of R’s plotting and statistical modelling facilities rely on factors. We’ll look at the diet data (stored in diet_effects) to begin getting a sense of how they work: diet_effects &lt;- read.csv(file = &quot;DIET_EFFECTS.CSV&quot;) First, we need to be able to actually recognise a factor when we see one. Here is the result of using glimpse with the diet data: glimpse(diet_effects) ## Observations: 40 ## Variables: 2 ## $ Plan &lt;fctr&gt; None, None, None, None, None, None, None, None, Y-... ## $ WeightLoss &lt;dbl&gt; 1.3, 2.0, 1.7, 1.6, 1.5, 1.4, 1.0, 2.0, 1.5, 1.8, 1... This tells us that there are 40 observations in the data set and 2 variables (columns), called Plan and WeightLoss. The text next to $ Plan says &lt;fctr&gt;. We can guess what that stands for…. it is telling us that the Plan vector inside diet_effects is a factor. The Plan factor was created automatically when we read the data stored in DIET_EFFECTS.CSV into R. When we read in a column of data that is non-numeric, read.csv will often decide to turn it into a factor for us46. R is generally fairly good at alerting us to the fact that a variable is stored as a factor. For example, look what happens if we extract the Plan column from diet_effects and print it to the screen: diet_effects$Plan ## [1] None None None None None None None None ## [9] Y-plan Y-plan Y-plan Y-plan Y-plan Y-plan Y-plan Y-plan ## [17] F-plan F-plan F-plan F-plan F-plan F-plan F-plan F-plan ## [25] Slimaid Slimaid Slimaid Slimaid Slimaid Slimaid Slimaid Slimaid ## [33] Waisted Waisted Waisted Waisted Waisted Waisted Waisted Waisted ## Levels: F-plan None Slimaid Waisted Y-plan This obviously prints out the values of each element of the vector, but look at the last line: ## Levels: F-plan None Slimaid Waisted Y-plan This alerts us to the fact that Plan is a factor, with 5 levels (the different diets). Look at the order of the levels: these are alphabetical by default. Remember that! The order of the levels in a factor can be important as it controls the order of plotting in ggplot2 and it affects the way R presents the summaries of statistics. This is why we are introducing factors now—we are going to need to manipulate the levels of factors to alter the way our data are plotted and analysed. 25.3 Roughly checking the assumptions We listed the assumptions of ANOVA at the end of the last chapter, but here they are again: Independence. The experimental units must be independent. Measurement scale. The focal variable should be measured on an interval or ratio scale. Equal variance. The variance of the focal variable within each treatment combination is assumed to be constant. Normality. The focal variable is assumed to be normally distributed within each treatment combination. The first two assumptions are about independence and the type of variable. The independence assumption is an aspect of the experimental design, so we’ll just have to assume this assumption is satisfied without knowing more about how the data were collected. The second assumption states that the variable being analysed (WeightLoss) should be an interval or ratio scale numeric variable. Change in weight is clearly a ratio scale numeric variable. The last two assumptions are about the distribution of the variables in each group. The thing to bear in mind here is that there are five different groups to consider—one for each treatment. We are interested in whether the variance within each group is similar, and whether the WeightLoss variable within each group is likely to have been drawn from a normal distribution. A simple plot to roughly assess the distributional assumptions of ANOVA is the box and whiskers plot: ggplot(data = diet_effects, aes(x = Plan, y = WeightLoss)) + geom_boxplot() Simple! We just set the x and y axis aesthetics to the Plan and Weightloss variables, respectively, and use geom_boxplot to add the required layer. The resulting box and whiskers plot summarises the distribution of weight loss in each diet group: 1) the median diet loss in each group (horizontal lines) shows us the central tendency; 2) the interquartile range (the boxes) summarises the dispersion / spread; and 3) roughly speaking, the ‘whiskers’ summarise the range of the data. The plot indicates that that the variability within each group is quite similar, the distributions are fairly symmetric (i.e. the lower whisker, the box and the upper whisker are all about equally large), and there are no ‘outliers’. These features are consistent with a normal distribution. Dot plots or histograms might also be used to make a more direct assessment of the distributional assumptions. However, these aren’t very useful when there are only a few replicates in each group. A much better way to assess the distributional assumptions is via regression diagnostics. That’s right, we use ‘regression’ diagnostics to evaluate the assumptions of an ANOVA model. We’ll come back to this after we’ve fitted the model… 25.4 Fitting the ANOVA model Carrying out ANOVA in R is quite simple, but as with regression, there is more than one step. The first involves a process known as fitting the model (or just model fitting). This is the step where R calculates the relevant means, along with the additional information needed to generate the results in step two. We call this step model fitting because an ANOVA is a type of model for our data: it is a model that allows the mean of a variable to vary among groups. How do we fit an ANOVA model in R? We will do it using the lm function again. Remember what the letters ‘lm’ stand for? They stand for ‘(general) linear model’. So… an ANOVA model is just another special case of the general linear model. Here is how we fit a one-way ANOVA in R, using the diet data: diets_model &lt;- lm(WeightLoss ~ Plan, data = diet_effects) Hopefully by now this kind of thing is starting to look familiar. We have to assign two arguments: The first argument is a formula. We know this because it includes a ‘tilde’ symbol: ~. The variable name on the left of the ~ must be the numeric variable whose means we want to compare among groups. The variable on the right should be the indicator variable that says which group each observation belongs to. These are WeightLoss and Plan, respectively. The second argument is the name of the data frame that contains the two variables listed in the formula. Why does R carry out ANOVA? How does R know we want to use an ANOVA model? After all, we didn’t specify this anywhere. The answer is that R looks at what type of vector Plan is. It is a factor, and so R automatically carries out an ANOVA. It would do the same if Plan had been a character vector. However, if the levels of Plan had been stored as numbers (1, 2, 3, …) then R would not have fitted an ANOVA. We’ve already seen what would have happened. It would have assumed we meant to fit a regression model. This is why we don’t store categorical variables as numbers in R. Avoid using numbers to encode the levels of a factor, or any kind of categorical variable, if you want to avoid making mistakes in R. Notice that we did not print the results to the console. Instead, we assigned the result a name (diets_model)—diets_model now refers to a model object. What happens if we print this to the console? diets_model ## ## Call: ## lm(formula = WeightLoss ~ Plan, data = diet_effects) ## ## Coefficients: ## (Intercept) PlanNone PlanSlimaid PlanWaisted PlanY-plan ## 1.8500 -0.2875 -0.4250 0.4375 0.0500 Not a great deal. Printing a fitted model object to the console is not very useful when working with ANOVA. We just see a summary of the model we fitted and some information about the coefficients of the model. Yes, an ANOVA model has coefficients, just like a regression does. What we really want to extract from the model is a significance test. However, before we dive into p-values, we should revisit the assumptions of the ANOVA model using… regression diagnostics. Yes, regression diagnostics. Remember, we can use the diagnostic tools developed for regression to evaluate the assumptions of any model produced by lm. Now that we have a fitted model, we can go ahead and examine these. It makes sense to do this before doing ‘the stats’ because if there is a problem we need to fix it before proceeding. 25.5 Diagnostics The first diagnostic plot we might produce is the residuals vs. fitted values plot. In a regression this is used to evaluate the linearity assumption. What does it do in a one-way ANOVA? Not much of use to be honest. There isn’t much point making a residuals vs. fitted values plot for a one-way ANOVA. Why? Because the residuals will never show a trend with respect to the ‘fitted values’, which are just the group-specific means. That’s one thing less to worry about. The normal probability plot is used to identify departures from normality. This plot allows us to check whether the deviations from the group means (the residuals) are likely to have been drawn from a normal distribution. This is definitely something we’d like to assess for an ANOVA model. Here’s the normal probability plot for the diet example: plot(diets_model, which = 2, add.smooth = FALSE) This looks very good. The points don’t deviate from the line in a systematic way (except for a couple at the lower end—this is nothing to worry about) so it looks like the normality assumption is satisfied. The scale-location plot allows us to evaluate the constant variance assumption of ANOVA. This allows us to see whether or not the variability of the residuals is roughly constant within each group. Here’s the scale-location plot for the diet example: plot(diets_model, which = 3, add.smooth = FALSE) The warning sign we’re looking for here is a systematic pattern. We want to see if the magnitude of the residuals tends to increase or decrease with the fitted values. If such a pattern is apparent then it suggests that variance changes with the mean. There is no such pattern in the above plot so it looks like the constant variance assumption is satisfied. So it looks like our assumptions are fine and we can move onto the hypothesis testing step. A word of warning first. It may be tempting to evaluate the assumptions of an ANOVA by plotting the distribution of residuals in each group. Don’t do this—use the regression diagnostics instead. They are a much more powerful diagnostic tool than dot plots or histograms of residuals. 25.5.1 Aside: formal test of equality of variance It is not critical that you learn the material in this short section. It is provided so that you know how to test for equality of variance. You won’t be asked to do this in an assessment. Looking back over the scale-location plot, it seems like three of the treatment groups exhibit similar variability, while the remaining two are more variable. They aren’t wildly different, so it is reasonable to assume the differences are due to sampling variation. People are sometimes uncomfortable using this sort of visual assessment. They want to see a p-value… A number of statistical tests have been designed to evaluate the equality of variance assumption. The most widely used is the Bartlett test (the bartlett.test function in R). Here is how to use it: bartlett.test(WeightLoss ~ Plan, data = diet_effects) ## ## Bartlett test of homogeneity of variances ## ## data: WeightLoss by Plan ## Bartlett&#39;s K-squared = 3.6444, df = 4, p-value = 0.4563 This looks just like the t-test specification. We use a ‘formula’ (WeightLoss ~ Plan) to specify the variable of interest (WeightLoss) and the grouping variable (Plan), and use the data argument to tell the bartlett.test function where to look for these variables. The null hypothesis of a Bartlett test is that the variances are equal, so a non-significant p-value (&gt;0.05) indicates that the data are consistent with the equal variance assumption. That’s what we find here. Generally speaking, we don’t recommend that you carry out a statistical test to evaluate the equality of variance assumption. We have shown it because some people seem to think they are needed. Here is why we think they are wrong: formal tests of equality of variance are not very powerful (in the statistical sense). This means that in order to detect a difference, we either need a lot of data, or the differences need to be so large that they would be easy to spot using a graphical approach. 25.6 Interpreting the results What we really want is a p-value to help us determine whether there is statistical support for a difference among the group means. That is, we need to calculate things like degrees of freedom, sums of squares, mean squares, and the F-ratio. This is step 2. We use the anova function to do this: anova(diets_model) ## Analysis of Variance Table ## ## Response: WeightLoss ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plan 4 3.5765 0.89412 5.1281 0.002331 ** ## Residuals 35 6.1025 0.17436 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that all we did was pass the anova function one argument: the name of the fitted model object. Let’s step through the output to see what it means. The first line just informs us that we are looking at an ANOVA table, i.e. a table of statistical results from an analysis of variance. The second line just reminds us what variable we analysed. The important information is in the table that follows: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Plan 4 3.5765 0.89412 5.1281 0.002331 ** ## Residuals 35 6.1025 0.17436 This is an Analysis of Variance Table. It summarises the parts of the ANOVA calculation: Df – degrees of freedom, Sum Sq – the sum of squares, Mean Sq – the mean square, F value – the F-ratio (i.e. variance ratio), Pr(&gt;F) – the p-value. The F-ratio (variance ratio) is the key term. This is the test statistic. It provides a measure of how large and consistent the differences between the means of the five different treatments are. Larger values indicate clearer differences between means, in just the same way that large values of Student’s t indicate clearer differences between means in the two sample situation. The p-value gives the probability that the observed differences between the means, or a more extreme difference, could have arisen through sampling variation under the null hypothesis. What is the null hypothesis: it is one of no effect of treatment, i.e. the null hypothesis is that all the means are the same. As always, the p-value of 0.05 is used as the significance threshold, and we take p &lt; 0.05 as evidence that at least one of the treatments is having an effect. For the diet data, the value is 5.1, and the probability (p) of getting an F-ratio this large is given by R as 0.0023, i.e. less than 0.05. This provides good evidence that there are differences in weight loss between at least some of the treatments. So far so good. The test that we have just carried out is called the global test of significance. It goes by this name because it doesn’t tell us anything about which means are different. The analyses suggest that there is an effect of diet on weight loss, but some uncertainty remains because we have only established that there are differences among at least some diets. A global test doesn’t say which diets are better or worse. This could be very important. If the significant result is generated by all diets being equally effective (hence differing from the control but not from each other) we would draw very different conclusions than if the result was a consequence of one diet being very effective and all the others being useless. Our result could even be produced by the diets all being less effective than the control! So having got a significant result in the ANOVA, we should always look at the means of the treatments to understand where the differences actually lie. We did this in the previous chapter but here is the figure again anyway: What looking at the means tells us is that the effect of the diets is generally to increase weight loss (with one exception, ‘Slimaid’) relative to the control group (‘None’), and that it looks as though ‘Waisted’ is the most effective, followed by ‘Y-plan’ and ‘F-plan’. Often inspection of the means in this way will tell us all we need to know and no further work will be required. However, sometimes it is desirable to have a more rigorous way of testing where the significant differences between treatments occur. A number of tests exist as ‘add ons’ to ANOVA which enable you to do this. These are called post hoc multiple comparison tests (sometimes just ‘multiple comparison tests’). We’ll see how to conduct multiple comparison tests in the next chapter. 25.7 Summarising and presenting the results of ANOVA As with all tests it will usually be necessary to summarise the result from the test in a written form. With an ANOVA on several treatments, we always need to at least summarise the results of the global test of significance. Here is again: There was a significant effect of diet on the weight losses of subjects (ANOVA: F=5.1; d.f.= 4,35; p&lt;0.01). There are several things to notice here: The degrees of freedom are always quoted as part of the result, and…there are two values for the degrees of freedom to report in ANOVA because it involves F-ratios. These are obtained from the ANOVA table and should be given as the treatment degrees of freedom first, followed by the error degrees of freedom. Order matters. Don’t mix it up. The degrees of freedom are important because, like a t-statistic, the significance of an F-ratio depends on the degrees of freedom, and giving them helps the reader to judge the result you are presenting. A large value may not be very significant if the sample size is small, a smaller may be highly significant if the sample sizes are large. The F-ratio rarely needs to be quoted to more than one decimal place. When it comes to presenting the results in a report, it helps to present the means, as the statement above cannot entirely capture the results. We could use a table to do this, but tables are ugly and difficult to interpret. A good figure is much better. You won’t be assessed on your ability to produce summary plots such as those below. Nonetheless, you should try to learn how to make them because you will need to produce these kinds of figures in your own projects. Box and whiskers plots and multi-panel dot plots / histograms are exploratory data analysis tools. We use them at the beginning of an analysis to understand the data, but we don’t tend to present them in project reports or scientific papers. Since ANOVA is designed to compare means, a minimal plot needs to show the point estimates of each group-specific mean, along with a measure of their uncertainty. We often use the standard error of the means to summarise this uncertainty. In order to be able to plot these quantities we first have to calculate them. We can do this using dplyr. Here’s a reminder of the equation for the standard error of a mean: \\[ SE = \\frac{\\text{Standard deviation of the sample}}{\\sqrt{\\text{Sample size}}} = \\frac{SD}{\\sqrt{n}} \\] So, the required dplyr code is: # get the mean and the SE for each diet plan diet_stats &lt;- diet_effects %&gt;% group_by(Plan) %&gt;% summarise(Mean = mean(WeightLoss), SE = sd(WeightLoss)/sqrt(n())) # print to the console diet_stats ## # A tibble: 5 × 3 ## Plan Mean SE ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 F-plan 1.8500 0.1069045 ## 2 None 1.5625 0.1209154 ## 3 Slimaid 1.4250 0.1979809 ## 4 Waisted 2.2875 0.1231107 ## 5 Y-plan 1.9000 0.1690309 Notice that we used the n function to get the sample size. The rest of this R code should be quite familiar by now. We gave the data frame containing the group-specific means and standard errors the name diet_stats. We have a couple of different options for making a good summary figure. The first plots a point for each mean and places error bars around this to show ±1 SE. In order to do this using ggplot2 we have to add two layers—the first specifies the points (the means) and the second specifies the error bar (the SE). Here is how to do this: ggplot(data = diet_stats, aes(x = Plan, y = Mean, ymin = Mean - SE, ymax = Mean + SE)) + # this adds the means geom_point(colour = &quot;blue&quot;, size = 3) + # this adds the error bars geom_errorbar(width = 0.1, colour = &quot;blue&quot;) + # controlling the appearance scale_y_continuous(limits = c(0, 3)) + # use sensible labels xlab(&quot;Diet treatment&quot;) + ylab(&quot;Weight loss (kg)&quot;) + # flip x and y axes coord_flip() + # use a more professional theme theme_bw() First, notice that we set the data argument in ggplot to be the data frame containing the summary statistics (not the original raw data). Second, we set up four aesthetic mappings: x, y, ymin and ymax. Third, we added one layer using geom_point. This adds the individual points based on the x and y mappings. Fourth, we added a second layer using geom_errorbar. This adds the error bars based on the x, ymin and ymax mappings. Finally we adjusted the y limits and the labels (this last step is optional). Ask a demonstrator to step through this with you if you are confused by it. Take a close look at that last figure. Is there anything wrong with it? The control group in this study is the no diet group (‘None’). Conventionally, we display the control groups first. R hasn’t done this because the levels of Plan are in alphabetical order by default. If we want to change the order of plotting, we have to change the way the levels are organised. Here is how to to this using dplyr and a function called factor: diet_stats &lt;- diet_stats %&gt;% mutate(Plan = factor(Plan, levels = c(&quot;None&quot;, &quot;Y-plan&quot;, &quot;F-plan&quot;, &quot;Slimaid&quot;, &quot;Waisted&quot;))) We use mutate to update Plan, using the factor function to redefine the levels of Plan and overwrite the original. Now, when we rerun the ggplot2 code we end up with a figure like this: The treatments are presented in the order specified with the levels argument. Problem solved! A bar plot is another popular visualisation for summarising the results of an ANOVA. We only have to change one thing about the last chunk of ggplot2 code to make a bar plot. Instead of using a geom_point, we use geom_bar (we’ll drop the coord_flip bit too): ggplot(data = diet_stats, aes(x = Plan, y = Mean, ymin = Mean - SE, ymax = Mean + SE)) + # this adds the means geom_bar(stat = &quot;identity&quot;, fill = &quot;lightgrey&quot;, colour = &quot;grey&quot;) + # this adds the error bars geom_errorbar(width = 0.1, colour = &quot;black&quot;) + # controlling the appearance scale_y_continuous(limits = c(0, 3)) + xlab(&quot;Diet treatment&quot;) + ylab(&quot;Weight loss (kg)&quot;) We have to set stat argument of geom_bar to &quot;identity&quot; to ensure it plots the y variable ‘as is’. If we forget to do this ggplot2 will just show you the number of cases in each group. We have worked with factors, but only when reordering the labels of a categorical variable plotted in ggplot2.↩ This behaviour isn’t really all that helpful. It would be better if R let us, the users, decide whether or not to turn something into a factor vector. However, in the early days of R computers had limited memory, which meant storing a categorical variable as a factor made sense; it saved memory. This benefit of factors has long since disappeared, but we’re stuck with the turn-everything-into-a-factor behaviour.↩ "],
["multiple-comparison-tests.html", "Chapter 26 Multiple comparison tests 26.1 Introduction 26.2 Tukey’s HSD in R 26.3 How to summarise multiple-comparison results 26.4 Doing it the easy way… 26.5 Summarising and presenting the results of a Tukey test 26.6 Significant ANOVA but no differences in a Tukey test?", " Chapter 26 Multiple comparison tests 26.1 Introduction In the previous chapter we learned how to use ANOVA to examine the global hypothesis of no difference between means—we did not learn how to evaluate which means might be driving such a significant result. For example, we found evidence for a significant difference between the means in the diet example, but we were not able to say which diets are better or worse, and did not make any statements about which diets differ significantly from each other. The purpose of this chapter is to examine one method for assessing where the differences actually lie. The general method we will use is called a post hoc multiple comparisons test. The phrase ‘post hoc’ refers to the fact that these tests are conducted without any particular prior comparisons in mind. The words ‘multiple comparisons’ refer to the fact that they consider many different pairwise comparisons. There are quite a few multiple comparison tests—Scheffé’s test, the Student-Newman-Keuls test, Duncan’s new multiple range test, Dunnett’s test, … (the list goes on and on). Each one is applicable to particular circumstances, and none is universally accepted as ‘the best’. We are going to work with the most widely used test: the Tukey multiple comparison test. This test is also known as Tukey’s Honestly Significant Difference (Tukey HSD) test47. People tend to favour Tukey’s HSD test because it is ‘conservative’: the test has a low false positive rate compared to the alernatives. A false positive occurs when a test turns up a statistically significant result for an effect that is not really there. A low false positive rate is a good thing. It means that if we find a significant difference we can be more confident it is ‘real’. The cost of using the Tukey HSD test is that it isn’t as powerful as alternatives: the test turns up a lot of false negatives. A false negative occurs when a test fails to produce a statistically significant result for an effect when it is really present. A test with a high false negative rate tends to miss effects. There is one line of thinking that says post hoc multiple comparisons tests of any kind should never be undertaken. We shouldn’t carry out an experiment without a prior prediction of what will happen—we should know which comparisons need to be made and should only undertake those particular comparisons rather than making every possible comparison. Nonetheless, post hoc multiple comparisons test are easy to apply and widely used, so there is value in knowing how to use them. The Tukey HSD test at least tends to guard against picking up non-existent effects. 26.2 Tukey’s HSD in R Walk through example We are going to work with the diets example again. If you kept the DIET_EFFECTS.CSV file from the last chapter all you need to do is make sure your working directory is set to this location. Otherwise you’ll need to download it again. Let’s work through the diet example again. Read the data into R we and fit an ANOVA model using the lm function. # 1. read in data diet_effects &lt;- read.csv(file = &quot;.DIET_EFFECTS.CSV&quot;) # 2. fit anova model diets_model &lt;- lm(WeightLoss ~ Plan, data = diet_effects) We stored the model object in diets_model. In the previous chapter we saw how to evaluate whether the means differ using anova on this object. We use a couple of new functions to carry out a Tukey HSD test. First, we have to convert the linear model object into a different kind of model object using the aov function: diets_aov &lt;- aov(diets_model) We don’t really need to understand what this is doing—aov prepares the model so that we can perform a Tukey HSD test. Notice that we gave the new object its own name (diets_aov) because we need to use it in the next step. It is easy to perform a Tukey HSD test once we have the ‘aov’ version of our model. There are a few different options. Here is how to do this using the TukeyHSD function: TukeyHSD(diets_aov, ordered = TRUE) Pay attention! We applied the TukeyHSD function to the ‘aov’ object, not the original lm object. We have suppressed the output for now. Before we review it we need to get an idea of what it is going to show us. The ordered = TRUE tells TukeyHSD that we want to order the treatment means from smallest to largest, and then apply every pairwise comparison, starting with the smallest mean (‘Slimaid’) and working up through the order. Here are the means ordered from smallest to largest, working left to right: Diet Slimaid None F-Plan Y-Plan Waisted Mean 1.42 1.56 1.85 1.90 2.29 So the TukeyHSD with ordered = TRUE will first compare ‘Slimaid’ to ‘None’, then ‘Slimaid’ to ‘F-Plan’, then ‘Slimaid’ to ‘Y-Plan’, then ‘Slimaid’ to ‘Waisted’, then ‘None’ to ‘F-Plan’, then ‘None’ to ‘Y-Plan’, … and so on, until we get to ‘Y-Plan’ vs. ‘Waisted’. Let’s look at the output: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## factor levels have been ordered ## ## Fit: aov(formula = diets_model) ## ## $Plan ## diff lwr upr p adj ## None-Slimaid 0.1375 -0.4627565 0.7377565 0.9638453 ## F-plan-Slimaid 0.4250 -0.1752565 1.0252565 0.2707790 ## Y-plan-Slimaid 0.4750 -0.1252565 1.0752565 0.1771593 ## Waisted-Slimaid 0.8625 0.2622435 1.4627565 0.0018764 ## F-plan-None 0.2875 -0.3127565 0.8877565 0.6459410 ## Y-plan-None 0.3375 -0.2627565 0.9377565 0.4971994 ## Waisted-None 0.7250 0.1247435 1.3252565 0.0113786 ## Y-plan-F-plan 0.0500 -0.5502565 0.6502565 0.9992352 ## Waisted-F-plan 0.4375 -0.1627565 1.0377565 0.2447264 ## Waisted-Y-plan 0.3875 -0.2127565 0.9877565 0.3592201 This table look confusing at first glance. It enables you to look up every pair of treatments, and see whether they are significantly different from each other. Lets see how this works… The first four lines compare the Slimaid treatment (‘Slimaid’) with each of the other treatments in turn: ## diff lwr upr p adj ## None-Slimaid 0.1375 -0.4627565 0.7377565 0.9638453 ## F-plan-Slimaid 0.4250 -0.1752565 1.0252565 0.2707790 ## Y-plan-Slimaid 0.4750 -0.1252565 1.0752565 0.1771593 ## Waisted-Slimaid 0.8625 0.2622435 1.4627565 0.0018764 So to look up the difference between the control treatment and the ‘Slimaid’ treatment, we read the first results row in the table. This says the means differ by 0.1375, the confidence interval associated with this difference is [-0.46, 0.74], and that the comparison has a p-value of 0.96. So in this case we would conclude that there was a no significant difference between the control treatment and the ‘Slimaid’ treatment. We could look up any comparison of the ‘Slimaid’ treatment with a different treatment in the next three lines of this portion of the table. This basic logic extends to the rest of the table. If we want to know whether the ‘Waisted’ treatment is different from the control, we look up the ‘Waisted-None’ row: ## diff lwr upr p adj ## Waisted-None 0.7250 0.1247435 1.3252565 0.0113786 It looks like the means of the ‘Waisted’ and ‘None’ levels are significantly different at the p &lt; 0.05 level. Now we know how to look up any set of comparisons we need to see whether the difference is significant. The next question is: How should we summarise such a table? 26.3 How to summarise multiple-comparison results Summarising the results of multiple comparison tests can be a tricky business. The first rule is: don’t present the results like the TukeyHSD function does! A clear summary of the results will help us to interpret them correctly and makes it easier to explain them to others. How should we do this? Let’s list the means in order from smallest to largest again (2 decimal places is sufficient precision for these data): Diet Slimaid None F-Plan Y-Plan Waisted Mean 1.42 1.56 1.85 1.90 2.29 Now, using the table of Tukey results we can perform a sequence of pair-wise comparisons between the diets starting with the smallest pair… ‘Slimaid’ and ‘None’. The appropriate test is in the first table: ## diff lwr upr p adj ## 0.1375000 -0.4627565 0.7377565 0.9638453 The last column gives the p-value, which in this case is certainly not significant (it is much greater than 0.05), so we conclude there is no difference between the ‘Slimaid’ and ‘None’ treatments. So now we continue with ‘Slimaid’, but compare it to the next larger mean (‘F-Plan’). In this case the values are: ## diff lwr upr p adj ## 0.4250000 -0.1752565 1.0252565 0.2707790 The last column gives the p-value, which again is not significant, so we conclude there is no difference between the ‘Slimaid’ and ‘F-Plan’ treatments. So now we continue with ‘Slimaid’, but compare it to the next larger mean (‘Y-Plan’). In this case the values are: ## diff lwr upr p adj ## 0.4750000 -0.1252565 1.0752565 0.1771593 Once again, this difference is not significant, so we conclude there is no difference between the ‘Slimaid’ and ‘Y-Plan’ treatments either. So again, we continue with ‘Slimaid’, which we compare to the next larger mean (‘Waisted’). ## diff lwr upr p adj ## 0.8625000 0.2622435 1.4627565 0.0018764 This time the p-value is clearly less than 0.05 so we conclude that this pair of treatments are significantly different. We record this by marking ‘Slimaid’, ‘None’, ‘F-plan’ and ‘Y-Plan’ to indicate that they don’t differ from each other. We’ll use the letter ‘b’ to do this. Diet Slimaid None F-Plan Y-Plan Waisted Mean 1.42 1.56 1.85 1.90 2.29 b b b b The ‘b’ defines a group of treatment means—‘Slimaid’, ‘None’, ‘F-plan’ and ‘Y-Plan’—that are not significantly different from one another. It doesn’t matter which letter we use by the way (the reason for using ‘b’ here will become apparent in a moment). The means are ordered from smallest to largest which means we can forget about ‘None’, ‘F-Plan’ and ‘Y-Plan’ treatments for a moment—if they are not significantly different from ‘Slimaid’ they can’t be significantly different from one another. We move on to ‘Waisted’, but now, we work back down the treatments to see if we can define another overlapping group of means that are not significantly different from one another. When we do this, we find that ‘Waisted’ is not significantly different from ‘F-Plan’ and ‘Y-Plan’, but that it is significantly different from ‘None’. This forms a second ‘not significantly different’ group. We will denote this with a new letter (‘a’) in our table: Diet Slimaid None F-Plan Y-Plan Waisted Mean 1.42 1.56 1.85 1.90 2.29 b b b b a a a If there were additional treatments with a mean that was greater than ‘Waisted’ we would have to carry on this process, working back up from ‘Waisted’. Thankfully, there are no more treatments, so we are finished. This leaves us with a concise and complete summary of where the differences between treatments are, which greatly simplifies the task of interpreting the results. Treatments that share a letter form groups within which means are not different from each other. Treatments that are not linked are significantly different. 26.4 Doing it the easy way… The results table we just produced is concise and complete, but no reasonable person would say they were easy to arrive at. As you might expect, someone has written an R function to do this for us. It isn’t part of ‘base R’ though, so we have to install a package to use it. The package we need is called agricolae: install.packages(&quot;agricolae&quot;) Once this has been installed we use the library function to tell R that we want to use the package in the current session: library(&quot;agricolae&quot;) Now that the package is loaded we can carry out the Tukey HSD test and find the ‘not significantly different’ groups using the HSD.test function: HSD.test(diets_aov, &quot;Plan&quot;, console=TRUE) ## ## Study: diets_aov ~ &quot;Plan&quot; ## ## HSD Test for WeightLoss ## ## Mean Square Error: 0.1743571 ## ## Plan, means ## ## WeightLoss std r Min Max ## F-plan 1.8500 0.3023716 8 1.5 2.4 ## None 1.5625 0.3420004 8 1.0 2.0 ## Slimaid 1.4250 0.5599745 8 0.5 2.2 ## Waisted 2.2875 0.3482097 8 1.7 2.7 ## Y-plan 1.9000 0.4780914 8 1.0 2.5 ## ## alpha: 0.05 ; Df Error: 35 ## Critical Value of Studentized Range: 4.065949 ## ## Honestly Significant Difference: 0.6002565 ## ## Means with the same letter are not significantly different. ## ## Groups, Treatments and means ## a Waisted 2.288 ## ab Y-plan 1.9 ## ab F-plan 1.85 ## b None 1.562 ## b Slimaid 1.425 The console = TRUE argument tells the function to print the results for us. That’s a lot of output, but we can ignore most of it. The part that matters most is the table at the very end. This shows the group identities as letters, the treatment names, and the treatment means. If we compare that table with the one we just made, we can see they convey the same information. The package labels each group with a letter. For example, we can see that ‘Y-plan’ and ‘F-plan’ are both members of the ‘a’ and ‘b’ group. Hopefully it is obvious why we used ‘b’ and then ‘a’ when building the table manually above. So, there is no need to build these Tukey HSD tables by hand. We just use the HSD.test function in the agricolae package. So why did we do it the long way? Well, the usual reasoning applies: it is important to know how to do this because it improves our understanding of what the ‘letters notation’ actually means. 26.5 Summarising and presenting the results of a Tukey test As with any statistical test it will usually be necessary to summarise the result from the Tukey HSD test in a written form. With an ANOVA test of global significance and multiple comparisons of the means, this can become quite complex and hard to follow. In most cases it is best to summarise the ANOVA results and either the main differences between means, or concentrate on those comparisons which relate to the original hypothesis we were interested in, and then refer to a table or figure for the additional detail. For example… There was a significant effect of diet on the weight losses of subjects (ANOVA: F=5.1; d.f.= 4,35; p&lt;0.01) (Figure 1). The only diet plan that led to a significantly higher rate of weight loss than the control group was Waisted (Tukey multiple comparisons test, p &lt; 0.05). When it comes to presenting the results in a report, we really need some way of presenting the means, and the results of the multiple comparison test, as the statement above cannot entirely capture the form of the results. The information can often be conveniently incorporated into a table or figure, using more or less the same format as the output from the HSD.test function in the agricolae package. An example table might be: Diet Mean weight loss (kg) Waisted 2.29a Y-plan 1.90ab F-plan 1.85ab None 1.56b Slimaid 1.43b Note that, however we present it, we need to provide some explanation saying: (a) what test we did, (b) what the letter codes mean, and (c) the critical threshold we used to judge significance. In this case the information could be presented in a table legend: Table 1: Mean weight loss of subjects in the five diet treatments. Means followed by the same letter did not differ significantly (Tukey test, p&gt;0.05). Letter coding can also be used effectively in a figure. Again, we must ensure all the relevant information is given in the figure legend. 26.6 Significant ANOVA but no differences in a Tukey test? It is possible to get a significant result from the global ANOVA test but find no significant differences in a Tukey test, though it doesn’t happen often. ANOVA and the Tukey HSD test (or indeed other multiple comparison tests) are different tests, with different null hypotheses. Because of this it is possible to end up with a significant result from ANOVA, indicating at least one difference between means, but fail to get any differences detected by the Tukey test. This usually happens when the ANOVA result is marginal (close to p = 0.05). If it happens then aside from running the experiment again with more replication, there isn’t much we can do except make the best interpretation we can from inspecting the data, and be suitably cautious in the conclusions we draw48. If in doubt seek some expert advice! N.B. Try to avoid a common mistake: it is Tukey, after its originator, the statistician Prof. John Tukey, not Turkey, a large domesticated bird which has made no useful contributions to statistical theory or practice.↩ It might be tempting to run a new post hoc analysis using a different kind of test. Don’t do this. It is a terrible strategy for doing statistics because this kind of practise is guaranteed to increase the overall false positive rate.↩ "],
["introduction-to-two-way-anova.html", "Chapter 27 Introduction to two-way ANOVA 27.1 Introduction 27.2 Degrees of freedom, mean squares, and F-statistics 27.3 Multiple comparison tests 27.4 Beyond two-way ANOVA", " Chapter 27 Introduction to two-way ANOVA The experimenter who believes that only one factor at a time should be varied is amply provided for by using a factorial experiment. Box et al. (2005) 27.1 Introduction One-way ANOVA allows us to determine whether there are significant differences between the effects of two or more treatments. The treatments we are interested in comparing are the different levels of a factor. These levels may represent quantitative variations of a general treatment (e.g. the effect of different concentrations of slug poison on slugs), or qualitatively different varieties of a class of treatments (e.g. the effect of different diets on weight loss). Fairly obviously, we are less likely to be interested in questions which involve comparing completely different sorts of treatment. For example, it is hard to see the value of an experiment comparing the movement of slugs where the three different treatments are: (i) half-strength Slugit (ii) moist wood substrate (iii) darkness. None of the treatments are comparable and so it will be very difficult to interpret differences between them. Although the above experiment would not be very useful, we might well be interested in whether the moisture level of the substrate has an effect on movement rate, and similarly, whether movement is affected by the slug being in the light or dark. To address these questions we could obviously design two separate experiments—one where the treatments are wooden boards with two or more different moisture levels, and another where the treatments are ‘light’ or ‘dark’. Although this is a perfectly valid procedure, it still leaves us lacking some information. With the moisture experiment, we have to decide whether to run it in the light or the dark, and with the light and dark experiment we have to decide how moist the wooden boards we use should be. If we decide to run our moisture level experiment in the dark, then we end up (we hope) knowing something about the effect of moisture on slug movement, but only in dark conditions—we can’t say whether the effect of moisture would have been different had the slugs been in the light (and obviously we can’t say anything about the effect of light and dark more generally since that is the subject of our other experiment). One obvious solution might be to run two moisture experiments… one in the dark and one in the light, and similarly three light/dark experiments, one at each different moisture level. This is indeed what we want to do, but instead of running each combination separately, it is more powerful, and experimentally less problematic, to run all the combinations together. If we used four slugs in each combination, we would require: 4 replicates \\(\\times\\) 3 moisture levels \\(\\times\\) 2 light levels \\(=\\) 24 slugs. We would end up with 4 measurements of movement rate in each combination of treatments (figures are cm per min), e.g. Moisture level Light level &lt;5% 50% 100% Dark 2, 3, 5, 0 3, 9, 5, 10 15, 8, 11, 12 Light 4, 2, 7, 1 10, 7, 4, 13 13, 17, 12, 9 An experiment of this sort, where measurements are made under each combination of several levels of two or more different kinds of experimental treatments, is called a fully factorial experiment. This type of experimental design gets its name from the fact it involves every combination of treatments among two or more factors. The example here is a two-factor experiment because it has two different kinds of treatment (illumination and moisture). It should be straightforward to see that different factors can be combined in a single experiment, and that this seems to yield the maximum amount of information, but to get at that information we need to be able to analyse the data. Fortunately the principles of ANOVA that you have seen already can be extended to provide a powerful and elegant way of analysing data from factorial designs. With two different sets of treatment (as here) this approach is referred to as two-way ANOVA (also known as two-factor ANOVA). A two-way ANOVA on data from the slug experiment would tell us whether slug movement was affected by (1) moisture and (2) illumination, and (3) whether the effect of illumination depends on moisture levels (and vice versa). So instead of just one result (as we get from a one-way ANOVA) there are now three to consider. The effect of moisture and of illumination are termed main effects and the effect of each moisture level / illumination combination is termed the interaction. What are these? The main effects are fairly obvious: The moisture effect …tells you whether there is a significant difference between the mean movement of slugs among the three moisture levels (i.e., the means of the data in each of the three columns in the table above, across both light levels). The illumination effect …tells you whether there is any difference between slug movement in the light and dark (i.e., the means of the data in each of the two rows in the table above, across all moisture levels). The interaction is a bit more tricky: The interaction between moisture and illumination …tells you whether there are differences between slug movement rates which are due to specific combinations of different moisture and illumination levels, which cannot be accounted for just by combining the mean effects of moisture level and of illumination level (i.e. are there differences between the means of the data from each cell in the table, having taken account of the overall effects of moisture and illumination?). Another way of looking at the interaction is that it indicates whether slug movement responds differently to moisture depending on whether it is in the light or dark. All this will probably make more sense when we have an example to work with, so we’ll carry out a two-way ANOVA and then come back to how the results should be interpreted. Treatments When writing about factorial experiments, the word ‘treatment’ tends to be used in two subtly different ways: Some people enumerate different treatments at the level of combinations of factor levels. For example, if we were carrying out an experiment with two factors, each of which has two levels (‘A’ vs. ‘B’ and ‘X’ vs. ‘Y’), we would say that the experiment has four treatments. Others delineate treatments at the level of individual factors, and then refer to ‘treatment combinations’ to distinguish unique experimental conditions. In our example we would say that each factor involves two treatments and overall, the experiment involves four treatment combinations. Notice that ‘factor levels’ and ‘treatments’ are synonymous when using the second naming convention (this suggests the first definition is probably the more useful one). We will adopt this second convention in this course because it is so widely used. 27.2 Degrees of freedom, mean squares, and F-statistics We are not going to step through the logic underpinning the calculations of the degrees of freedom, sum of squares, mean squares, and F-statistics. The logic is no different than that used in one-way ANOVA. It is a bit trickier to explain and visualise though. Ultimately, an F-statistic is calculated for each term, which is the ratio of the term’s mean square and the error mean square. A higher F-statistic is more likely to be significant, and the p-value is calculated by comparing the F-statistic to the theoretical F distribution. 27.3 Multiple comparison tests Having established that there are significant differences, we might wish to go further and specify between which means these differences occur. With one-way ANOVA obviously there was only one set of means to compare with the multiple comparison tests. Now, however, there are three possible sets of means: the two main effects and the interaction. 27.4 Beyond two-way ANOVA It is possible to have more complex designs using 3 or more factors (‘multi-way fully factorial’)—for example we could add to our experiment considered earlier by running our existing treatment combinations at each of three different temperatures—but as the experiment becomes more complex, so does the analysis and interpretation (and also the work involved in running it: adding three temperature treatments would mean we needed 72 slugs!). A multi-way factorial design isn’t really a ‘different’ kind of design from the two-way case we have examined. The principle of the 2-way fully factorial design can be directly extended to multi-way fully factorial designs. In a three-way design (factors A, B, C) there are three main effects (A, B, C), three pairwise interactions (A x B, A x C, B x C) and one new kind of interaction: a three-way interaction (A x B x C). The challenge posed by such designs is that the results can be tricky to interpret (what is a three-way interaction?). Analysis of Variance is a large and complex subject—we are only scratching the surface in this book. Most intermediate level biostatistics texts deal with the more involved designs of ANOVA. As with many aspects of statistics and experimental design there is much to be said for doing experiments and analyses you are confident you understand and can interpret, even if more complex forms of analysis are technically possible (providing of course the simpler approach is appropriate!). When contemplating a design that looks like it might require more than two factors, it is a good idea to talk to someone who knows about these things to ensure that is indeed necessary. "],
["two-way-anova-in-r.html", "Chapter 28 Two-way ANOVA in R 28.1 Introduction 28.2 Competition between Calluna and Festuca 28.3 Visualising the data 28.4 Fitting the ANOVA model and interpreting the results 28.5 Diagnostics 28.6 Interpreting the results 28.7 Multiple comparison tests 28.8 Drawing conclusions and presenting results 28.9 Balanced or orthogonal designs", " Chapter 28 Two-way ANOVA in R 28.1 Introduction Our goal in this chapter is to learn how to work with two-way ANOVA models in R. As always, we’ll do this by working through an example—using data from a plant competition experiment. The work flow is very similar to one-way ANOVA in R. We’ll start with the problem and the data, and then work through model fitting, evaluating assumptions, significance testing, and finally, presenting the results. 28.2 Competition between Calluna and Festuca Plants have an optimal soil pH for growth, and this varies between species. Consequently we would expect that if we grow two plants in competition with each other at different pH values the effect of competition might vary according to the soil pH. In a recent study the growth of the grass Festuca ovina (Sheep’s Fescue) in competition with the heather Calluna vulgaris (Ling) was investigated in soils with different pH. Calluna is well adapted to grow on very acidic soils such as on the Millstone grit and blanket bogs around Sheffield. Festuca grows on soils with a much wider range of pH. We might hypothesise that Calluna will be a better competitor of Festuca in very acid soils than in moderately acid soils. To test this hypothesis an experiment was designed in which Festuca seedlings were grown in pots at all combinations of two levels of two different kinds of treatment: Factor 1: Soil pH at 3.5 or 5.5 Factor 2: Presence or absence of Calluna. This is a fully factorial, two-way design. The total number of treatments was thus \\(2 \\times 2 = 4\\). For each of the treatments there were 5 replicate pots, giving a total of \\(2 \\times 2 \\times 5 = 20\\) pots. The following data are the yields of Festuca from each treatment (dry weight in g) from the two pH levels and in the presence or absence of Calluna. pH 3.5 pH 5.5 Calluna Present 2.76, 2.39, 3.54, 3.71, 2.49 3.21, 4.10, 3.04, 4.13, 5.21 Calluna Absent 4.10, 2.72, 2.28, 4.43, 3.31 5.92, 7.31, 6.10, 5.25, 7.45 Walk through You should begin working through the Festuca example from this point. The data are in a CSV file called FESTUCA.CSV. We’ll read the data into an R data frame, giving it the name festuca, and then print the whole data frame to the Console (View is another option here): festuca &lt;- read.csv(file = &quot;FESTUCA.CSV&quot;) ## Weight pH Calluna ## 1 2.76 pH3.5 Present ## 2 2.39 pH3.5 Present ## 3 3.54 pH3.5 Present ## 4 3.71 pH3.5 Present ## 5 2.49 pH3.5 Present ## 6 4.10 pH3.5 Absent ## 7 2.72 pH3.5 Absent ## 8 2.28 pH3.5 Absent ## 9 4.43 pH3.5 Absent ## 10 3.31 pH3.5 Absent ## 11 3.21 pH5.5 Present ## 12 4.10 pH5.5 Present ## 13 3.04 pH5.5 Present ## 14 4.13 pH5.5 Present ## 15 5.21 pH5.5 Present ## 16 5.92 pH5.5 Absent ## 17 7.31 pH5.5 Absent ## 18 6.10 pH5.5 Absent ## 19 5.25 pH5.5 Absent ## 20 7.45 pH5.5 Absent Notice that the data for a two-factor experiment can be laid out in a very similar manner to those from a one factor experiment, with the actual data in one column, but now there are two additional columns containing the codes for the treatments, one for pH and one for Calluna. The first column (Weight) contains the Festuca dry weights, the second column (pH) contains the codes for the pH treatment (levels: pH3.5, pH5.5), the third column (Calluna) contains the codes for the presence or absence of Calluna (levels: Present, Absent). A couple of points are worth noting at this point: As is always the case in this book, the data are ‘tidy’. Each experimental factor is in one column and each observation is in a single row. Last year we pointed out that data need to be in this format to be used effectively with dplyr. The same applies to the majority of R’s statistical modelling tools—they expect data to be supplied in this format. We avoided using numbers to encode the levels of the pH treatment. This is important, as it ensures that the pH variable will be converted into a factor rather than a number when we read the data into R. We have said it before, but it is worth saying one more time: fewer mistakes will occur if we use words to encode the levels of a factor. 28.3 Visualising the data We should take a look at the data before doing anything with it. We only have five replicates per treatment combination, so any figure we produce is going to provide only limited information. Five replicates is just about sufficient for a box plot: ggplot(data = festuca, aes(x = Calluna, y = Weight, colour = pH)) + geom_boxplot() The main purpose of a plot like this is to help us understand what the treatments are doing. We want to quickly assess things like: How big are the main effects? What direction do they work in? Is there likely to be an interaction? It looks like the higher pH conditions tends to increase Festuca growth, and the presence of Calluna tends to reduce it (yes, plants compete!). The more interesting observation is that the effect of Calluna seems to be greater at higher pH. It looks like we might have an interaction. At this point it may be tempting to start evaluating the assumptions of our model. If we have a large number of replicates then it sometimes makes sense to do this with a box and whiskers plot, or separate dot plots for each treatment combination. We don’t have that many replicates though, and even if we did, it is much better to rely on regression diagnostics. We need to fit the model first to get at these. 28.4 Fitting the ANOVA model and interpreting the results Carrying out a two-way ANOVA in R is really no different from one-way ANOVA. It still involves two steps. First we have to fit the model using the lm function, remembering to store the fitted model object. This is the step where R calculates the relevant means, along with the additional information needed to generate the results in step two. The second step uses the anova function to calculate F-statistics, degrees of freedom, and p-values. Here is the R code needed to carry out the model-fitting step with lm: festuca_model &lt;- lm(Weight ~ pH + Calluna + pH : Calluna, data = festuca) This is very similar to the R code used to fit a one-way ANOVA model. The first argument is a formula (notice the ‘tilde’ symbol: ~) and the second argument is the name of the data frame that contains all the variables listed in the formula. That’s all we need. The specific model fitted by lm is a result of 1) the type of variables referenced in the formula, and 2) the symbols used to define the terms in the formula. To ensure that we have fitted an ANOVA model, the variables which appear to the right of the ~ must be factors or character vectors—an ANOVA only involves factors. The variable name to the left of the ~ is the numeric variable we are analysing. We know that Calluna and pH are factors, so we can be certain that lm has fitted some kind of ANOVA model. What kind of ANOVA have we fitted, i.e., what are the terms on the right hand side of the formula doing? Here is the formula we used: Weight ~ pH + Calluna + pH:Calluna There are three terms, each separated by a + symbol: pH, Calluna and pH:Calluna. This tells R that we want to fit a model that accounts for the main effects of pH and Calluna, and that we also wish to include the interaction between these two factors. The specification of the main effects is fairly self-explanatory—we just include the name of each factor variable in the formula. The interaction term is less obvious. It is specified by a colon (the : symbol) with the two interacting variables either side of it. In summary… 1) the ~ symbol specifies a formula in R, where the name on the left hand side is the variable we are analysing, and the names on the right denote the terms in the model; 2) we place a + between terms to delineate them (we are not adding anything up when the + is used in a formula); 3) each main effect is specified by the corresponding factor name; and 4) an interaction between factors is specified by the : symbol. Notice that we assigned the result a name (festuca_model) which now refers to the model object produced by lm. Just as with a one-way ANOVA, we can’t extract p-values by printing this object to the console, because all this gives us is a limited summary of the fitted model’s coefficients: festuca_model ## ## Call: ## lm(formula = Weight ~ pH + Calluna + pH:Calluna, data = festuca) ## ## Coefficients: ## (Intercept) pHpH5.5 CallunaPresent ## 3.368 3.038 -0.390 ## pHpH5.5:CallunaPresent ## -2.078 We’re not going to worry about what those mean for a two-way ANOVA. We’re going to use anova to calculate things like degrees of freedom, sum of squares, mean squares, F-statistics, and finally, the p-values. But… before we do that, let’s check our assumptions now that we have a fitted model object. 28.5 Diagnostics We’re going to produce two diagnostic plots: a normal probability plot to evaluate the normality assumption, and a scale-location plot allows us to evaluate the constant variance assumption. Here’s the normal probability plot: plot(festuca_model, which = 2, add.smooth = FALSE) This plot allows us to check whether the deviations from the group means (the residuals) are likely to have been drawn from a normal distribution. This looks… not so great. The points deviate from the line in a systematic way so it looks like the normality assumption may not be satisfied. The left tail is above the line and the right tail is below it. This tells us that the tails of the residual distribution do not extend out as far as they should—the distribution is ‘squashed’ toward its middle. The scale-location plot allows us to see whether or not the variability of the residuals is roughly constant within each group. Here’s the plot: plot(festuca_model, which = 3, add.smooth = FALSE) We’re on the lookout for a systematic pattern in the size of the residuals and the fitted values—does the variability go up or down with the fitted values? There is no such pattern so it looks like the constant variance assumption is at least satisfied here. We’ve identified one potential problem. We’ll ignore it for now and press on. The goal here is to learn the work flow for two-way ANOVA in R. However, keep in mind that if we were serious about the analysis we should find a way to ‘fix’ it. We’re going to learn some useful tricks for doing this in later chapters. 28.6 Interpreting the results Now we’re ready to calculate the degrees of freedom, sums of squares, mean squares, the F-ratio, and p-values for the main effects and the interaction terms: anova(festuca_model) ## Analysis of Variance Table ## ## Response: Weight ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pH 1 19.9800 19.9800 28.1792 7.065e-05 *** ## Calluna 1 10.2102 10.2102 14.4001 0.00159 ** ## pH:Calluna 1 5.3976 5.3976 7.6126 0.01397 * ## Residuals 16 11.3446 0.7090 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What does all this mean? We interpret each line of the ANOVA table in exactly the same way as we do for a one-way ANOVA. The first part tells us what kind of output we are looking at: ## Analysis of Variance Table ## ## Response: Weight This reminds us that we are looking at an ANOVA table where we analysed a variable called Weight. The table contains the key information: ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## pH 1 19.9800 19.9800 28.1792 7.065e-05 *** ## Calluna 1 10.2102 10.2102 14.4001 0.00159 ** ## pH:Calluna 1 5.3976 5.3976 7.6126 0.01397 * ## Residuals 16 11.3446 0.7090 This ANOVA table is similar to the ones we have already seen, except that now we have to consider three lines—one for each term in the model. The first is for the main effect of pH, the second for the main effect of Calluna, the third for the interaction between pH and Calluna. The F-ratio is the test statistic for each term. These provides a measure of how large and consistent the effects associated with each term are. Each F-ratio has a pair of degrees of freedom associated with it: one belonging to the term itself, the other due to the error (residual). Together, the F-ratio and its degrees of freedom determines the p-value. The p-value gives the probability that the differences between the set of means for each term in the model, or a more extreme difference, could have arisen through sampling variation under the null hypothesis of no difference. We take p &lt; 0.05 as evidence that at least one of the treatments is having an effect. Here, p &lt; 0.05 for three effects, so we conclude both main effects and the interaction are significant (though at different significance levels). The ANOVA table tells us nothing about the direction of the effects. We have to delve a little further into the fitted model or plot the data to be able to do this. The presence of an interaction between treatments indicates that the impact of one factor depends on the levels of the other factor. This means that if there is a significant interaction in a two-way ANOVA, then the main effects should be interpreted with care. An ‘interaction diagram’ provides a good way to think about these issues… 28.6.1 Understanding the model graphically How should we go about interpreting the significant effects? To reiterate, the interaction tells us that the magnitude, or even direction, of the effect of one factor is dependent upon the levels of the other factor. In other words the treatment effects are contingent on one another. This contingency can arise in a number of ways, giving rise to different mixtures of main effects and interactions. This is illustrated most easily by considering some hypothetical results from a pH/Calluna experiment of this sort, in schematic form (the Calluna bars are linked by a dotted line): Diagrams such as these are sometimes called ‘interaction diagrams’ and they are often the best way of looking at the results from this sort of experiment to try and interpret what is happening. You will notice that the lines linking the treatments are parallel when there is no interaction, but become non-parallel when an interaction is present. An interaction may just mildly change the way the main effects work (4th plot) or it might completely reverse the effects (5th plot). We can use R to produce an interaction diagram for a two-way design. We’ll use dplyr and ggplot2 to construct this for the example. First we have to calculate the mean weight of Festuca in each treatment combination: # step 1. calculate means for each treatment combination festuca_means &lt;- festuca %&gt;% group_by(Calluna, pH) %&gt;% # &lt;- remember to group by *both* factors summarise(Means = mean(Weight)) festuca_means ## Source: local data frame [4 x 3] ## Groups: Calluna [?] ## ## Calluna pH Means ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 Absent pH3.5 3.368 ## 2 Absent pH5.5 6.406 ## 3 Present pH3.5 2.978 ## 4 Present pH5.5 3.938 The key to making the plot is to specify four aesthetic mappings, and then add two layers, one showing points and the other showing lines: # step 2. plot these as an interaction plot ggplot(festuca_means, aes(x = Calluna, y = Means, colour = pH, group = pH)) + geom_point(size = 4) + geom_line() Notice that we mapped pH to two aesthetics: colour and group. This ‘trick’ makes ggplot2 link the levels of pH with lines, each of which gets its own colour. This clearly reveals how the different effects are working. Our interaction plot resembles the 4th hypothetical outcome. It is possible to make some interpretation of the main effects—namely that increase in pH, and removal of Calluna increase Festuca yield. However, the magnitude of these effects is dependent on the other (interaction)—the effect of Calluna is increased at higher pH. 28.7 Multiple comparison tests Obviously, since the main treatments only have two levels there is no need for any multiple comparison tests on the main effects — if there is a difference it must be between the two levels. However, the interaction is significant, so it may be desirable to know which particular treatment combinations differ. Predictably, the work flow is very similar to that applied to a one-way ANOVA model. We could use the TukeyHSD function to do this. We start by converting the model object produced by lm into an aov object… festuca_aov &lt;- aov(festuca_model) …and then we perform a Tukey HSD test: TukeyHSD(festuca_aov, which = &#39;pH:Calluna&#39;) We have suppressed the output for now. The only new tweak that we have to learn is the which argument. Assigning this the value 'pH:Calluna' makes the TukeyHSD function carry out all pairwise comparisons among the means of each treatment combination, i.e., we are considering the full set of interactions. Here is the output: ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = festuca_model) ## ## $`pH:Calluna` ## diff lwr upr p adj ## pH5.5:Absent-pH3.5:Absent 3.038 1.5143518 4.5616482 0.0001731 ## pH3.5:Present-pH3.5:Absent -0.390 -1.9136482 1.1336482 0.8826936 ## pH5.5:Present-pH3.5:Absent 0.570 -0.9536482 2.0936482 0.7117913 ## pH3.5:Present-pH5.5:Absent -3.428 -4.9516482 -1.9043518 0.0000443 ## pH5.5:Present-pH5.5:Absent -2.468 -3.9916482 -0.9443518 0.0014155 ## pH5.5:Present-pH3.5:Present 0.960 -0.5636482 2.4836482 0.3079685 You extract information from this table just as you did before. The table present a series of pair-wise comparisons between mean values tested by the Tukey procedure. For example, the first 3 lines show the significance of differences between the mean of the treatment combination pH 3.5 without Calluna and the 3 other mean values. All we need from this table is to note the codes for the treatment means which are being compared (listed in the first column), and the p-value in each case listed in the final column. If we list the mean values in sequence from the lowest to the highest we can then use the results presented in these tables of pair-wise comparisons to derive letter codes to indicate which means differ significantly (at p&lt;0.05) in exactly the same way as you did last week. There are three significant differences, all of which involve the treatment combinations pH 5.5 with Calluna absent. This implies that there are two ‘not significantly different’ groups: one defined by pH 5.5 with Calluna absent, and then everything else. As you might expect, we don’t have to step through the results of the TukeyHSD function to define the ‘not significantly different’ groups. We used the agricolae package to do this for a one-way ANOVA. We can use this again here. We need to load and attach the package first (it may also need to be installed if you are on a university computer): library(agricolae) Once the package is ready for use, we can carry out the Tukey HSD test to find the ‘not significantly different’ groups using the HSD.test function: HSD.test(festuca_aov, trt = c(&quot;pH&quot;, &quot;Calluna&quot;), console = TRUE) ## ## Study: festuca_aov ~ c(&quot;pH&quot;, &quot;Calluna&quot;) ## ## HSD Test for Weight ## ## Mean Square Error: 0.709035 ## ## pH:Calluna, means ## ## Weight std r Min Max ## pH3.5:Absent 3.368 0.9042511 5 2.28 4.43 ## pH3.5:Present 2.978 0.6089089 5 2.39 3.71 ## pH5.5:Absent 6.406 0.9451614 5 5.25 7.45 ## pH5.5:Present 3.938 0.8685448 5 3.04 5.21 ## ## alpha: 0.05 ; Df Error: 16 ## Critical Value of Studentized Range: 4.046093 ## ## Honestly Significant Difference: 1.523648 ## ## Means with the same letter are not significantly different. ## ## Groups, Treatments and means ## a pH5.5:Absent 6.406 ## b pH5.5:Present 3.938 ## b pH3.5:Absent 3.368 ## b pH3.5:Present 2.978 Setting the trt argument to c(&quot;pH&quot;, &quot;Calluna&quot;) makes the function carry out all pair-wise comparisons among the mean values defined by each treatment combination. The output that matters is the table at the very end, which shows the group identities as letters, the treatment names, and the treatment means. This just reiterates what we already knew—there are two ‘not significantly different’ groups, defined by pH 5.5 with Calluna absent, and ‘everything else’. Multiple comparison tests for main effects As mentioned above, in this experiment there is no point in trying to make further comparisons between the means from the main treatments (pH 3.5 and 5.5, or with and without Calluna) since (a) there is a significant interaction, so detailed comparisons of the main effects are hard to interpret, and (b) even if that was not the case there are only two levels in each treatment so any difference must be between those two levels! However, it is quite common to have experiments with more than two levels in one or both factors. If the ANOVA indicates that there is a significant effect of one, or both, of the associated effects, and there is no interaction to worry about (don’t forget this caveat), then you may wish to carry out multiple comparisons for the means associated with the main effects. This can be done using a Tukey test just as we did for the interaction in this example. The only difference is that we have to specify the name of the main effect you are interested in. For example, if we wanted to use TukeyHSD function to evaluate the significance of the pH main effects, we would use: TukeyHSD(festuca_aov, which = ‘pH’) 28.8 Drawing conclusions and presenting results In the results section of the report we will need to provide a succinct factual summary of the analysis: There were significant effects of soil pH (ANOVA: F=28.18, df=1,16, p&lt;0.001), competition with Calluna (F=14.4, df=1,16, p=0.002) and the interaction between these treatments (F=7.61, df=1,16, p=0.014) on the dry weight yield of Festuca. Festuca grew much better in the absence of Calluna at high pH than in any other treatment combination (Tukey multiple comparison test p&lt;0.05) (Figure 1). For presentation we could tabulate the results, or better still present them as a figure rather like the interaction diagrams we saw earlier. We can of course produce such a figure, though we should include the standard errors of each mean to use it in a report or presentation. We’ll round off this section by looking at how to produce these publication-ready figures. You won’t be assessed on your ability to produce summary plots such as those below. But yes, you should learn how to make them because you will need to produce these kinds of figures in your own projects. The good news is that the figures below are as complicated as things will get in this book. We want to plot some sample statistics (means and standard errors) so we first have to calculate these using dplyr: # step 1. calculate means for each treatment combination festuca_stats &lt;- festuca %&gt;% group_by(Calluna, pH) %&gt;% # &lt;- remember to group by the two factors summarise(Means = mean(Weight), SEs = sd(Weight)/sqrt(n())) festuca_stats ## Source: local data frame [4 x 4] ## Groups: Calluna [?] ## ## Calluna pH Means SEs ## &lt;fctr&gt; &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Absent pH3.5 3.368 0.4043934 ## 2 Absent pH5.5 6.406 0.4226890 ## 3 Present pH3.5 2.978 0.2723123 ## 4 Present pH5.5 3.938 0.3884250 Once we’ve constructed a data frame containing the descriptive statistics we can make a plot: # step 2. plot these as an interaction plot ggplot(festuca_stats, aes(x = Calluna, y = Means, colour = pH, ymin = Means - SEs, ymax = Means + SEs)) + # this adds the mean geom_point(size = 3) + # this adds the error bars geom_errorbar(width = 0.1) + # controlling the appearance scale_y_continuous(limits = c(2, 7)) + xlab(&quot;Calluna&quot;) + ylab(&quot;Festuca yield (g dry weight)&quot;) This is very similar to the ggplot2 code used to make the summary figure in the one-way ANOVA example. We set the data argument in ggplot to be the data frame containing the statistics (not the original raw data), and this time, we set up five aesthetic mappings: x, y, colour, ymin and ymax. We use the colour aesthetic to delineate the levels of pH. We added two layers: one layer is added with geom_point to include the individual points based on the x and y mappings; the second layer is added with geom_errorbar to include the error bars based on the x, ymin and ymax mappings. 28.8.1 A little more customisation It is not uncommon to find that two or more means are quite close to one another, and as a result, plotted points and/or standard errors overlap. We can tweak a figure to avoid this by moving the plotted points a little to one side. The trick is to use the position_dodge function to define a ‘position adjustment’ object, and then associated this with position arguments in geom_errorbar and geom_point: # define a position adjustment pos &lt;- position_dodge(0.15) # make the plot ggplot(festuca_stats, aes(x = Calluna, y = Means, colour = pH, ymin = Means - SEs, ymax = Means + SEs)) + # this adds the mean (shift positions with &#39;position =&#39;) geom_point(size = 3, position = pos) + # this adds the error bars (shift positions with &#39;position =&#39;) geom_errorbar(width = 0.1, position = pos) + # controlling the appearance scale_y_continuous(limits = c(2, 7)) + xlab(&quot;Calluna&quot;) + ylab(&quot;Festuca yield (g dry weight)&quot;) Look at the new positions of the points and error bars at each level of Calluna—they have shifted very slightly to the left and right. We don’t need to do this in the Calluna example of course, because there is no overlap to deal with. If the points/SEs had overlapped, this would now be avoided because the data would be plotted side-by-side. We could also use a bar chart with error bars to summarise the data. You only have to change one thing about the last chunk of ggplot2 code to make this. You can probably guess how to do this—instead of using a geom_point, we use geom_bar… ggplot(festuca_stats, aes(x = Calluna, y = Means, fill = pH, ymin = Means - SEs, ymax = Means + SEs)) + # this adds the mean geom_bar(stat = &quot;identity&quot;, position = position_dodge()) + # this adds the error bars geom_errorbar(position = position_dodge(0.9), width=.2) + # controlling the appearance xlab(&quot;Calluna&quot;) + ylab(&quot;Festuca yield (g dry weight)&quot;) Remember (one more time) that we have to set the stat argument of geom_bar to &quot;identity&quot; to ensure it plots the y variable ‘as is’. The only other other trick we need to apply is to set the position arguments of geom_errorbar and geom_bar using position_dodge. If we don’t do this, ggplot2 will produce a stacked bar chart and the error bars will end up in the wrong place (the value of 0.9 used in position_dodge(0.9) ensures the error bars appear in the centre of each bar). 28.9 Balanced or orthogonal designs We’re going to finish this chapter with a small warning. In an ideal world, for ANOVA with two or more factors the experiment should be designed such that we have: 1) every possible combination of treatments represented, e.g. pH and Calluna factors each have two levels, so the experiment should have four treatment combinations; 2) equal numbers of replicates in each combination of treatments, e.g. all the cells in the data table for pH and Calluna treatments have equal numbers of data values. This leads to what is called a balanced, orthogonal experiment. The word balanced refers to the ‘equal numbers of replicates in each combination’ aspect of the experiment. The word orthogonal refers to the ‘every possible combination of treatments’ aspect of the experiment. The analysis workflow that we learn in this book assumes a balanced, orthogonal experimental design. We put that in bold because it is a really important point to remember. If at all possible, aim for a balanced, orthogonal experimental design. This makes life much easier when the time comes to analyse the data. The workflow we’re learning is only appropriate when using balanced, orthogonal design. If our experimental design does not meet these conditions, it is not necessarily a problem, but we need to consider its limitations. What are these? Here’s the key one: we can’t just take the fitted model object and pass it to the anova function to carry out the significance tests. We have to be a lot more careful than that. "],
["anova-for-randomised-block-designs.html", "Chapter 29 ANOVA for randomised block designs 29.1 Randomized Complete Block Designs 29.2 Designs without replication 29.3 Analysing an RCBD experiment 29.4 Carrying out the analysis with R 29.5 Are there disadvantages to randomised block designs? 29.6 Multiple blocking factors", " Chapter 29 ANOVA for randomised block designs Block what you can; randomize what you cannot. George Box 29.1 Randomized Complete Block Designs We have only considered one type of experimental ANOVA design up until now: the Completely Randomised Design (CRD). The defining feature of a CRD is that treatments are assigned completely at random to experimental units. This is the simplest type of experimental design. The randomisation is a good thing because it prevents systematic biases creeping in. A CRD approach is often ‘good enough’ in many situations. However, it isn’t necessarily the most powerful design—if at all possible, we should design experiments to account for nuisance factors. A nuisance factor is one that has an effect on the response—it creates variation—but is of no interest to the experimenter. To end up with the most powerful experiment possible, the variability induced by nuisance factors should be accounted for at the design stage of an experiment. We can do this by blocking the experiment with respect to nuisance factors. The idea behind blocking was considered in the Principles of experimental design chapter. Let’s consider a hypothetical experiment to remind ourselves how blocking works… Imagine we’re evaluating the effect of three different fertilizer application rates on wheat yields (t/ha). We suspect that the soil type and management histories of our experimental fields are quite different, leading to significant ‘field effects’ on yield. We don’t care about these field effects—they are a nuisance—but we’d like to account for them. There are two factors in play in this setting: the first is the set of treatments that are the subject of the experiment (fertilizer application rate); the second is the source of nuisance variation (field). Fertilizer application rate is the ‘treatment factor’ and field is the ‘blocking factor’. Here is one way to block the experiment. The essence of the design is that a set of fields are chosen, which may differ in various unknown conditions (soil water, aspect, disturbance, etc.) and within each field, three plots are set up. Each plot receives one of the three fertilizer rate treatments at random. If we chose to work with eight fields the resulting data might look like this: Fertilizer Control Absent High Block Field 1 9.3 8.7 10.0 Field 2 8.7 7.1 9.1 Field 3 9.3 8.2 10.4 Field 4 9.5 8.9 10.0 Field 5 9.9 9.1 10.8 Field 6 8.9 8.0 9.0 Field 7 8.3 6.2 8.9 Field 8 9.1 7.0 8.1 Each treatment level is represented in each field (block), but only once. The experiment is ‘blocked by field’. Now consider these two questions: Why is this design useful? Blocking allows us to partition out the environmental variation due to different field conditions. For example, the three treatments in field 5 produced a high yield relative to the yields within each treatment, while the yields in field 7 are consistently below average within each treatment. This among field variation is real, and if we hadn’t blocked the experiment and used a CRD it would manifest itself in the ‘noise’ component of our analysis. But since we blocked the experiment, and every treatment is present in every block, we can ‘remove’ the block variation from the noise. Less noise means more statistical power. Why is each treatment level is represented only once within blocks? This gives us the best chance of generalising our results. If we are interested in the overall effect of fertilizer, we should prefer to put our effort into including a range of possible environmental conditions. If we only did the experiment in one field the results might turn out to be rather unusual. We are not interested in the environmental variation as such, we just want to know for a range of conditions, whatever they might be, whether there are consistent differences between fertilizer application rates. There are many different ways to introduce blocking into an experiment. The most commonly used design—and the one that is easiest to analyse—is called a Randomized Complete Block Design. The defining feature of this design is that each block sees each treatment exactly once. The fertiliser study is an example of a Randomized Complete Block Design (RCBD). The obvious question is: How do we analyse an RCBD? We’ll explore that after a small detour…. Remember the paired sample design? Conceptually a Completely Randomised Block Design can be viewed as a generalisation of the paired sample design we studied in the t-test section. The pairs in a paired sample design are the blocks. Both designs are more powerful than their equivalent completely randomised version because they allow us to remove the noise created by nuisance factors (individual differences in health, spatial environmental variation, etc). Both designs work by effectively looking at changes within blocks (pairs). 29.2 Designs without replication The two-way designs we have seen up until now follow a similar pattern: two factors, each having two or more levels, with replicate measurements within each combination of levels: Factor A Level 1 Level 2 Level 3 Factor B Level 1 1,2,3 1,5,9 2,6,8 Level 2 3,4,2 … … Level 3 4,7,9 … … Level 4 4,6,5 … … As we have just seen, it is possible to have a two-way design with only a single measurement within each combination of levels: Factor A Level 1 Level 2 Level 3 Factor B Level 1 1 1 2 Level 2 3 … … Level 3 4 … … Level 4 2 … … What’s this… no replication? Isn’t that a problem? In fact there is replication of a sort for each level of the factors. For example there are 3 values for each level of Factor B, it’s just that each value is at a different level of Factor A. This means we can still compare the means for the different levels of each treatment using an ANOVA (that is, we can analyse the main effects). What we can’t do is analyse the interaction because this is derived from differences between the means and variances of the cases in each individual cell in the table (= each combination of the two factor levels). If there is only one value in a cell then clearly means and variances cannot be calculated. So… it is possible to carry out a two-way ANOVA without replication, though we can’t learn anything about the interaction with this sort of design. Is this ever useful? There are two situations where this sort of unreplicated two-way arrangement is used: One of the factors is a blocking factor. We could have guessed this one, given what this chapter is about. If we have a single blocking factor and one treatment factor, and we adopt an RCBD, then this leads to an unreplicated two-way arrangement. The fact that we can’t estimate the interaction is not a problem. In fact, “it’s a feature, not a bug.” We only care about the treatment factor. There might well be an interaction between the blocks and the experimental treatments, but we only need the average effect of treatments across blocks to be able to generalise our results. We are considering two treatment factors (no blocks), but we are only interested in the main effects. We have three choices in this situation: 1) conduct two separate one-way experiments, 2) construct a single two-way experiment without replication, or 3) construct a single two-way experiment with replication. The third option would be best, but sometimes we just don’t have the resources to do it49. What is the next best choice: option 1 or option 2? The two-way experiment without replication is the better option because a two-way design—with or without replication—is always more powerful than a pair of one-way experiments that include an equivalent number of replicates between them. So there is nothing “dodgy” about a two-way design without replication. Indeed, it is the best design to use in some situations. Let’s see how to the analyse a Randomized Complete Block Design experiment. Interactions are not a feature of the experimental design It’s important to realise that just because we can’t test for the interaction, it does not mean there is no interaction effect between two factors. We simply can’t detect it even if it is there. However, as long as we have a balanced, orthogonal design, a significance test of main effects is still meaningful. If the balanced and orthogonal criteria are not met, then we do have to be very careful about how we interpret a significant main effect in the absence of replication, because the presence of an interaction may generate a spuriously significant main effect. 29.3 Analysing an RCBD experiment Let’s consider a new example to really drive home how an RCBD works. We want to assess whether there is a difference in the impact that the predatory larvae of three damselfly species (Enallagma, Lestes and Pyrrhosoma) have on the abundance of midge larvae in a pond. We plan to conduct an experiment in which small (1 m2) nylon mesh cages are set up in the pond. All damselfly larvae will be removed from the cages and each cage will then be stocked with 20 individuals of one of the species. After 3 weeks we will sample the cages and count the density of midge larvae in each. We have 12 cages altogether, so four replicates of each of the three species can be established. On the face of it this looks like a straightforward one-way design, with each species as a treatment. The only problem to resolve is how to distribute the enclosures in the pond. Obviously the pond is unlikely to be uniform in depth, substrate, temperature, shade, etc… Some of the variation will be obvious, some will not. We have two options: 1) use a CRD and distribute the cages at random, or 2) adopt an RCBD by grouping the cages into clusters of three, placing each cluster at a randomly chosen location, and assigning the three treatments to cages at random within each cluster. These are illustrated below (left = CRD, right = RCBD): What are the consequences of the two alternatives? If the cages are distributed at random (CRD) then they will cover a wide range of variation in these various factors. These sources of variation will almost certainly cause the density of midge larvae to vary around the pond in an unpredictable way, increasing the noise in the data. If we group sets of treatments into clusters we are creating ‘spatial blocks’. There may be considerable differences between blocks, but these won’t obscure differences between the treatments because all three treatments are present in every block. 29.4 Carrying out the analysis with R Walk through example You should work through the example from here. Carrying out an analysis of an RCBD in R is straightforward. If we have only one blocking factor and one treatment factor it is only slightly different from a standard two-way ANOVA. The data from the damselfly experiment are in a file called DAMSELS.CSV. damsels &lt;- read.csv(&quot;./DAMSELS.CSV&quot;) glimpse(damsels) ## Observations: 12 ## Variables: 3 ## $ Midge &lt;int&gt; 304, 464, 320, 578, 509, 458, 680, 740, 630, 356, 390,... ## $ Species &lt;fctr&gt; Enallagma, Lestes, Pyrrhosoma, Enallagma, Lestes, Pyr... ## $ Block &lt;fctr&gt; A, A, A, B, B, B, C, C, C, D, D, D The density of midge larvae in each enclosure, after running the experiment for 3 weeks, are in the Midge variable (number m\\(^{-2}\\)); codes for species in the Species variable (levels: Enallagma, Lestes and Pyrrhosoma), and the block identities (A, B, C, D) in the third column. The process of analysing an RCBD is essentially the same as any other type of ANOVA. First we fit the model using the lm function and then we use anova to calculate F-statistics, degrees of freedom, and p-values: damsels.model &lt;- lm(Midge ~ Species + Block, data = damsels) anova(damsels.model) We suppressed the output for now. Notice that we only specified the two main effects (species and blocks)—we cannot test for the interaction. If we put an interaction term in the model, lm will fit it but the results from anova will be useless: ## Warning in anova.lm(wrong.model): ANOVA F-tests on an essentially perfect ## fit are unreliable ## Analysis of Variance Table ## ## Response: Midge ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452 ## Block 3 208425 69475 ## Species:Block 6 14878 2480 ## Residuals 0 0 Look at the residual degrees of freedom (the error degrees of freedom). The model we just fitted is called a saturated model—there are zero degrees of freedom left over after fitting the three terms. We can’t calculate an error sum of squares, which means we can’t calculate mean squares or F-ratios. This demonstrates that there really is no way to estimate an interaction in ANOVA when there is no replication at the level of each combination of factor levels. Here are the results of the global significance tests using the correct ANOVA model for our randomised block experiment: anova(damsels.model) ## Analysis of Variance Table ## ## Response: Midge ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452 3.0053 0.1246687 ## Block 3 208425 69475 28.0182 0.0006306 *** ## Residuals 6 14878 2480 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 There are only two main effect terms in the ANOVA table, as expected. By this point we should be able to interpret these. There is a significant effect of block (p &lt; 0.001), which says that the density of midge larvae varies across the lake. It looks like blocking was a good idea—there is a lot of spatial (nuisance) variation in midge larvae density. Of course what we actually care about is the damselfly species effect. This main effect term is not significant (p &gt; 0.05), so we conclude that there is no difference in the impact of the predatory larvae of three damselfly species. It is worth looking at what happens if we analyse the damselfly data as though they are from a one-way design. We do this by including only the experimental treatment term (Species) in the model: damsels.oneway &lt;- lm(Midge ~ Species, data = damsels) anova(damsels.oneway) ## Analysis of Variance Table ## ## Response: Midge ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Species 2 14904 7452.1 0.3003 0.7477 ## Residuals 9 223303 24811.4 Look at the degrees of freedom and the sums of squares of the residual (error). How do these compare to the previous model that accounted for the block effect? The degrees of freedom is higher. In principle this is a good thing because it means we have more power to detect a significant difference among the treatment means. However, the error sum of squares is also much higher when we ignore the block effect. We have accounted for much less noise by ignoring the block effect. As a result, the error mean square is much lower, and so the F-statistic associated with the treatment effect is also much lower. The take home message is that designing a blocked experiment, and properly accounting for the blocked structure, will (usually) result in a more powerful analysis. 29.4.1 Multiple comparisons anyone? In a randomised block analysis we are not usually interested in investigating significant block effects—the primary role of the blocking is to remove unwanted variation that might obscure the differences between treatments. R automatically gives us a test of the block effect, and if it is significant it tells us that using the block layout has removed a considerable amount of variation (though even if the result isn’t quite what would conventionally be regarded as significant, i.e. if is not as low as 0.05, then the blocking may still have been helpful). For this, and other, technical reasons we never carry out multiple comparisons between the block means. If the treatment effect is significant, multiple comparisons can be done between the treatment means using the Tukey test. 29.5 Are there disadvantages to randomised block designs? The short answer is no, not really. There are instances when a randomised block design might appear to be disadvantageous at first glance, but these don’t really stand up to criticism: What if we were interested in knowing whether there is an interaction between the levels of the block and the treatments? For example, in the damselfly experiment we might be interested to know whether, if the damselfly species have differing effects on the midge densities, these effects are consistent in all habitat areas (e.g. some species may forage more effectively in muddy areas, others where there are more leaves). If this is the question we are trying to answer, then we should really have designed a different experiment. For example, a two-way design (which might also include blocking) in which we consider treatment combinations of different midge species and habitat characteristics might be appropriate. Fundamentally, the goal of blocking is to account for uncontrolled variation. Designing a blocked experiment and then lamenting the fact that we can’t fully evaluate differences among blocks is a good example of trying to “have our cake and eat it too”. If the blocking term is having no effect in accounting for some of the variation, then the analysis may be slightly less powerful than just using a one-way ANOVA. This is because there are fewer error degrees of freedom associated with the blocked analysis (we lose a few to the block effects). This argument only works if the block effect accounts for very little variation. We can never know before we start an experiment whether or not blocking is needed, but we do know that biological systems are inherently noisy, with many sources of uncontrolled variation coming into play. In the majority of experimental settings (in biology at least) we can be fairly sure that blocking will ‘work’. If we choose not to block an experiment there is no way to account for uncontrolled variation and we will almost certainly end up losing statistical power as a result. The advice contained in the quote at the beginning of this chapter is probably the best experimental design advice ever dished out: “Block what you can; randomize what you cannot.” 29.6 Multiple blocking factors You will not be assessed on the material in this section. It is here to demonstrate that there are options beyond the Randomised Complete Block Design, but that they aren’t easy to employ. It is common to find ourselves in a situation whereby we need to account for more than one blocking factor. The simplest option is to combine the nuisance factors into a single factor. However, this isn’t always possible, or even desirable. Consider an instance where there is a single factor of primary interest (the treatment factor) and two nuisance factors. For example, imagine that we want to test three drugs A, B, C for their effect in alleviating the symptoms of a disease. Three patients are available for a trial, and each will be available for three weeks. Testing a single drug requires a week, meaning an experimental unit is a ‘patient-week’. The obvious question is, how should we randomise treatments across ‘patient-weeks’? We have to design an experiment like this with great care, or there is a risk that we will not be able to statistically separate the effects of the treatment (drug) and block effects (week &amp; patient). The most appropriate design for this kind of experiment has the following structure: Week Patient Drug 1 1 A 1 2 B 1 3 C 2 1 C 2 2 A 2 3 B 3 1 B 3 2 C 3 3 A This kind of experimental design is called a Latin square design. It gets its name form the fact that if we organise the treatments into the rows and columns of a grid according to week and patient number, we arrive at something like this50: A B C C A B B C A Notice that each letter appears once in each column and row. Mathematicians call this a Latin square arrangement. Latin square designs (and their more exotic friends, e.g. ‘Hyper-Graeco-Latin square designs!’) have a very useful property: they allow us to unambiguously separate treatment and block effects. The reasoning behind this conclusion is quite technical, so we won’t try to explain it. We just want to demonstrate that it is perfectly possible to block an experiment by more than one factor, but this needs to be done with care (this is a ‘seek advice’ situation). Having replication at each combination of factors will maximise statistical power, but it also makes for a more robust experiment—if we have only one replicate for a combination and we lose it then we have no information at all from that combination! If we lose just one of a number of replicates in a particular combination it may be a nuisance, but unlikely to ruin everything.↩ This example probably isn’t a very good experimental design, for the simple reason that it lacks statistical power. However, we could design a better version of this experiment using the same basic principles if we needed to.↩ "],
["data-transformations.html", "Chapter 30 Data transformations 30.1 Data that violate ANOVA assumptions 30.2 Data transformation: ANOVAs and t-tests 30.3 Carrying on anyway 30.4 Transforming the data 30.5 Types of transformations 30.6 What about other kinds of models? 30.7 Final thoughts", " Chapter 30 Data transformations 30.1 Data that violate ANOVA assumptions Up until now, the data we’ve examined have conformed, at least roughly, to the assumptions of the statistical models we’ve been studying. This is all very handy, but perhaps a little unrealistic. The real world being the messy place it is, biological data often don’t conform to the distributional assumptions of t-tests and ANOVA: The residuals may not be normally distributed. Variances may be unequal among groups. The same kinds of problems with these distributional assumptions can also arise when working in a regression (i.e. non-normal residuals, non-constant variance). Furthermore, we might run into additional problems if there is some kind of non-linearity in the relationship between the dependent and independent (numeric) variables. Most biological data are unlikely to conform perfectly to all the assumptions, but experience has shown (fortunately) that t-tests, ANOVA’s and regressions are generally quite robust—they perform reasonably well with data that deviate to some extent from the assumptions of the tests. However, in some cases residuals are clearly very far from normal, or variances changes a lot across groups. In these cases steps may need to be taken to deal with the problem. This chapter deals with one way of tackling the analysis of data that don’t fit the assumptions: data transformation. We will mostly focus on ANOVA / t-test setting, but keep in mind that the ideas are equally applicable to regression analysis. 30.2 Data transformation: ANOVAs and t-tests 30.2.1 The data: foraging in ants ants &lt;- read.csv(&quot;./data_csv/ANTS1.CSV&quot;) Red wood ants, Formica rufa, forage for food (mainly insects and ‘honeydew’ produced by aphids) both on the ground and in the canopies of trees. Rowan, oak and sycamore support very different communities of insect herbivores (including aphids) and it would be interesting to know whether the foraging efficiency of ant colonies is affected by the type of trees available to them. As part of an investigation of the foraging of Formica rufa observations were made of the prey being carried by ants down trunks of rowan, oak and sycamore trees. The total biomass of prey being transported was measured over a 30 minute sampling period and the data were expressed as the biomass (dry weight in mg) of prey divided by the total number of ants leaving the tree to give the rate of food collection per ant per half hour. Observations were made on 28 rowan, 26 sycamore, and 27 oak trees. Walk through Work through the ants example. The data are the file ANTS1.CSV. The Tree variable contains the tree identities and the Food variable contains the food collection rates: ants &lt;- read.csv(&quot;ANTS1.CSV&quot;) glimpse(ants) ## Observations: 81 ## Variables: 2 ## $ Food &lt;dbl&gt; 11.9, 33.3, 4.6, 5.5, 6.2, 11.0, 24.3, 20.7, 5.7, 12.6, 1... ## $ Tree &lt;fctr&gt; Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, Rowan, ... Let’s examine the data. We could make a dot plot… ggplot(ants, aes(x = Food)) + geom_dotplot(binwidth = 6) + facet_wrap(~ Tree) …or we could construct a box and whiskers plot: ggplot(ants, aes(x = Tree, y = Food)) + geom_boxplot() It doesn’t matter which plot we use. They tell the same story. The food collection rate is generally highest in Oaks and lowest in Rowans (Sycamores are in between). Notice too that the sample distribution of food collection rate is right-skewed. The test we are most likely to want to use with these data is an ANOVA, i.e. we want to assess whether the mean food collection rates are different among the three tree species. Already we have an indication that an ANOVA with the raw food values may be problematic. 30.2.2 Fit the model and checking the assumptions This chapter is about fixing models when the assumptions are not satisfied. What assumptions do we need to check? The test we are most likely to want to use with these data is an ANOVA, so the following assumptions must be evaluated: Independence. The experimental units of the data must be independent. Measurement scale. The dependent variable is measured on an interval or ratio scale. Normality. The residuals are normally distributed in each level of the grouping factor. Equal variance. The variance in each level of the grouping factor is the same. We’ll have to assume the first assumption is satisfied and the food collection rate (second assumption) is obviously measured on a ratio scale. The distributional assumptions (normality and equality of variance) are the ones we can address with a transformation. Let’s fit the ANOVA model and produce regression diagnostics to evaluate these—remember, we make these kinds of plots after we have fitted a statistical model: ant_mod &lt;- lm(Food ~ Tree, data = ants) We need to produce a ‘normal probability plot’ to assess the normality assumption: plot(ant_mod, which = 2) This plot exhibits the accelerating curvature that is indicative of right-skewed residuals. This probably isn’t just sampling variation because there is a systematic departure from the dashed line everywhere along it. So… it looks like there is a problem. This sort of pattern is quite common in biological data, especially when it involves counts. Clearly we might be a bit worried about using an ANOVA with these data since it assumes the residuals to be at least approximately normally distributed. Are the variances significantly different? Look at the box plots above. The data from the three samples seem to have rather different scatter. The sample from the rowan has less variation than that from the sycamore, and the sycamore has less variation than the oak. Does the scale-location plot tell the same story? plot(ant_mod, which = 3) This shows that the variance increases with the fitted values—it looks like there is also a problem with the constant variance assumption. Again, this pattern is very common in biological data. In the light of these evaluations, we have three options … To carry on and carry out an ANOVA anyway—hoping that the violation of the assumptions won’t matter too much. To try and transform the data in some way to make it fit the assumptions better, then carry out an ANOVA. To use a different sort of test which doesn’t require the data to conform to these assumptions. Such tests are known as nonparametric tests. We will consider the first two options below, and return to the third in the Non-parametric tests chapter. 30.3 Carrying on anyway Carrying on and performing an analysis anyway doesn’t sound like a very good idea if we’ve already decided that the assumptions are suspect, but don’t dismiss it straight away. Mild departures from the assumptions often do not make a huge difference to the results of ANOVA (i.e. the p-values). At the very least, it can be instructive to carry out an analysis without ‘fixing’ the apparent problems so that we can get a sense of whether they matter or not. We already fitted the ANOVA model to allow us to make the diagnostic plots. All we have to do is pass the model object to the anova function to get the F-ratio and p-value for the tree effects: anova(ant_mod) ## Analysis of Variance Table ## ## Response: Food ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tree 2 3317.1 1658.53 8.0305 0.0006741 *** ## Residuals 78 16109.2 206.53 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Based on these results, it looks like there is a highly significant difference in food collection rates across the three tree species. We know the data are problematic though, so the question is, does this result stand up when we deal with these problems? t-tests are robust The ‘carry on anyway’ strategy can often be justified if we just need to compare the sample means of two groups because in this situation we can use a two-sample t-test rather than an ANOVA. By default R uses a version of the t-test that allows for unequal sample variances. This at least deals with one potential problem. The t-test is also fairly robust to violations of the normality assumption when the sample sizes are small, and when the sample sizes are large, the normality assumption matters even less. The ability to do a t-test which doesn’t require equal variances is extremely useful. A word of warning though: some people advise carrying out a statistical test of equal variance, and if the variances are deemed not to be significantly different, using the version of a two-sample t-test that assumes equal variances. This is not good advise. Following this procedure leads to less reliable p-values. The reason for this effect is somewhat technical, but trust us, this procedure is not good statistical practise. 30.4 Transforming the data One approach to dealing with difficult data is to apply a mathematical function to it to make the transformed data fits the model assumptions better: a process called data transformation. This may sound a bit dubious, but it is a perfectly valid procedure that will often allow us to use the statistical model we want to even if the data don’t initially fit the assumptions. The key thing to keep in mind is that the transformatoin should be applied to the dependent variable. 30.4.1 The logarithmic transformation let’s try a simple transformation on the food collection rate variabe in the ant data set. Instead of using the original numbers we will convert them to their logarithms. We can use common logs (logs to the base 10, written log\\(_{10}\\)) or natural logs (logs to the base, written log\\(_{e}\\) or ln). It doesn’t matter: they have exactly the same effect on the data in terms of making it meet the assumptions of ANOVA (or not). Applying a log transform is quick and easy in R—there are built in functions to take common logs and natural logs, called log10 and log, respectively. We’ll use mutate to add a new variable, which is the common log of Food: ants &lt;- mutate(ants, logFood = log10(Food)) We stored the transformed variable in a new column called logFood. ant_mod_log &lt;- lm(logFood ~ Tree, data = ants) We need to produce a ‘normal probability plot’ to assess the normality assumption: plot(ant_mod_log, which = 2) The accelerating curvature (indicative of right-skewed residuals) has gone. The new normal probability plot is a bit better than before as now perhaps 60% of the cases are on the dashed line. It’s hardly perfect though—the tails of the distribution are not where we’d like them to be. What about the variances? plot(ant_mod_log, which = 3) The scale location-plot indicates that the constant variance assumption is now OK, i.e. the variance no longer increases with the fitted values. It looks like the log transformation seems to to have improved things quite a lot, but the diagnostics are still not perfect. The assumptions are closer to being satisfied. Let’s carry out ANOVA again using the model with the transformed food variable to see how the results change: anova(ant_mod_log) ## Analysis of Variance Table ## ## Response: logFood ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Tree 2 1.4106 0.70530 7.2867 0.001255 ** ## Residuals 78 7.5498 0.09679 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 What has happened? We still see evidence for a significant effect of tree (p&lt;0.01) with the transformed data, but the p-value is somewhat bigger than when we used the original data. This illustrates why it is important to evaluate assumptions, and to deal with them when they are obviously violated—the output of a statistical test is affected. It did not matter too much here, but in other settings we can end up with misleading or downright spurious results if we ignore problems with the assumptions of a statistical model. Values of 0 One thing to be aware of is that we cannot take the log of zero. If our data contain zeros we have add a small value (conventionally 1) to the variable before taking the logs (so the transformation is actually log(x + 1)). It is worth tying this —it’s easy to do and occasionally works well—but don’t be surprised if this transformation results in a model with poor diagnostics. The log(x + 1) transformation often doesn’t work when there are many zeros. Sadly, dealing with ‘non-normal’ data containing many zeros is tricky. 30.4.2 Presenting results from analyses of transformed data Having compared the transformed means, how should we present the results in a report? There are three alternatives. We’ll illustrate them using the log-transformation, but they applicable to other kinds of transformations. We could present the transformed means (having stated what the transformation was, e.g. \\(log_{e}(x+1)\\)). The disadvantage to this is that the numbers themselves convey little information about the data values on the original scale. This isn’t always a problem. For example, effects given on a log scale act in a ‘multiplicative’ manner, so a model with log-transformed dependent variable can still be interpreted if we know what we’re doing. We could back-transform the means of the log-transformed data by taking the antilogs: \\(10^{x}\\) (for logs to the base 10) and \\(e^{x}\\) (for natural logs)51. When we back-transform data, however, we need to be aware of two things: (1) The back-transformed mean will not be the same as a mean calculated from the original data; (2) We have to be careful when we back-transform standard errors. If we want to display the back-transformed means on a bar plot, with some indication of the variability of the data, we must calculate the standard errors and then back transform the upper and lower limits, which will not then be symmetrical about the mean. We could also present the means calculated from the original data but state clearly that the statistical analysis was carried out on transformed data. This is often the simplest way to proceed. 30.5 Types of transformations Clearly, in the case study above, a log-transformation alters outcome of statistical tests applied to the data. It is not always the case that transforming the data will make the difference between a result being significant and not significant, or that the transformed data will give a less significant result. Never use p-values to judge the success of a transformation! We use diagnostic plots to make that assessment. What we hope is that we can transform the dependent variable so that it conforms, at least approximately, to the assumptions of the statistical model we want to use, making the result from associated tests as reliable as possible. Taking logarithms is only one of many possible transformations. Each type of transformation is appropriate to solving different problems in the data. The following is a summary of the three most commonly used transformations and the sort of situations they are useful for. 30.5.1 Logarithms Log transformation, as we’ve just seen, affects the data in two ways: A log-transformation stretches out the left hand side (smaller values) of the distribution and squashes in the right hand side (larger values). This is obviously useful where the data set has a long tail to the right as in the example above. The ‘squashing’ effect of a log-transformation is more pronounced at higher values. This means a log-transformation may also deal with another common problem in biological data (also seen in the ant data)—samples with larger means having larger variances. If we are carrying out an ANOVA and the scale-location plot exhibits a positive relationship—–i.e. groups with larger means have larger variances—–then a log transformation could be appropriate52. 30.5.2 Square roots Taking the square root of the data is often appropriate where the data are whole number counts (though the log transformation may also work here). This typically occurs where your data are counts of organisms (e.g. algal cells in fields of view under a microscope). The corresponding back-transformation is obviously \\(x^{2}\\). In R the square root of a set of data can be taken using the sqrt function. However, note that there is no square function in the list. Taking squares is done using the ^ operator with the number 2 on the right (e.g. if the variable is called x, use x^2). 30.5.3 Arcsine square root This transformation is generally used where the data are in the form of percentages or proportions. It can be shown in theory (even if not from the data you actually have) that such data are unlikely to be normally distributed. A correction for this effect is to take the inverse sine of the square roots of the original data, i.e. \\(\\arcsin \\sqrt{x}\\). Converting percentages to proportions Although this transformation is usually discussed in the context of percentages we cannot actually take the arcsine of numbers larger than 1. Obviously percentages range between 0 and 100. To get round this, when dealing with percentage data, simply express the percentages as proportions (e.g. 100% = 1, 20% = 0.2, 2% = 0.02, etc.) before doing the transformation. In R the transformation can be achieved by combining the sqrt and asin functions inside mutate. For example, if we need to transform a proportion stored in the x variable use something like… mydata &lt;- mutate(mydata, assqrt.x = asin(sqrt(x))) …where mydata is the name of hypothetical data frame containing the data. Just remember to apply sqrt and asin in the correct order. We used nested functions here, which are applied from the inside to the outside. 30.5.4 Squaring In addition to the standard transformations above there are a variety of others less commonly used. One problem which the above transformations don’t deal with is when data have a negative skew (i.e. a long tail to the left). This problem can sometimes be dealt with, or at least reduced, by squaring the data values. In R the transformation can be achieved by combining the ^ operator inside mutate. For example, we might use something like… mydata &lt;- mutate(mydata, sqr.x = x^2) …where mydata is again the name of hypothetical data frame containing the data. 30.5.5 Situations which cannot be dealt with by transformations There are some situations where no amount of transformation of the data will get round the fact that the data are problematic. Three in particular are worth noting… Multimodal distributions: these may in fact have only one actual mode, but nonetheless have two or more clear ‘peaks’ (N.B. not to be confused with distributions that are ‘spiky’ just because there are few data). Dissimilar distribution shapes: if the two (or more) samples have very different problems, e.g. one is strongly right-skewed and the other strongly left-skewed then no single transformation will be able to help—whatever corrects one sample will distort the other. Samples with many exactly equal values: with results that are small integer numbers (e.g. counts of numbers of eggs in birds’ nests) then there will be many identical values. If the non-normality results from lots of identical values forming a particular peak, for example, this cannot be corrected by transformation since equal values will still be equal even when transformed. Is it ‘fiddling’ the data? Changing the data by transformation can alter the results of a statistical test—so isn’t this a bit dodgy? The key thing here is to realise that the scales on which we measure things are, to some extent, arbitrary. Transforming data to a different scale replaces one arbitrary scale with another. The transformations we have discussed don’t alter the ordering of data points relative to each other—they only alter the size of the gaps between them. In some cases this rescaling can make the data more amenable to study, analysis or interpretation. In fact we often use data transformations, perhaps without realising it, in many situations other than doing statistical tests. For example, when we look at a set of pH readings we are already working with data on a log scale because pH units (0-14) are actually the negative logarithms of the hydrogen ion concentration in the water. Similarly measurements of noise in decibels (dB), and seismic disturbance on the Richter scale, are actually logarithmic scales. In fact, there are subtle ways in which using a transformation can affect what aspect of the biological system it is our measurements are characterising but this is an issue beyond the scope of this course. We’ll already mentioned one example: a logarithmic transformation turns a ‘multiplicative’ process into an ‘additive’ one. One final comment… Obviously we have to apply the same transformation to all the data, e.g. we can’t log transform the observations in one group and leave the other alone—that really would be cheating! 30.6 What about other kinds of models? We have focussed on ANOVA here for the simple reason that the assumptions are a bit easier to evaluate compared to regression. However, exactly the same ideas apply when working with other kinds of models that lm can fit. The workflow is the same in every case: Always check the diagnostic plots after we fit a regression or ANOVA (and do this before worrying about p-values). If there is evidence for a problem with the assumptions, try transforming the dependent variable. Refit the model ing the transformed variable and generate new diagnostic plots to see if the new model is any better. Finally, keep in mind that this is often an iterative process. We might have to go through several rounds of transforming and model checking before we arrive at a good model. 30.7 Final thoughts Evaluating assumptions and picking transformations is as much ‘art’ as science. It takes time and experience to learn how to do it. R makes it very easy to try out different options, so don’t be afraid to do this. Frequently, with real biological data, no straightforward transformation really improves the form of the data or in correcting one problem you generate another. Fortunately, in many cases where the assumptions of a the test are not reasonably well fulfilled, there is an alternative approach—the use of a nonparametric test. These tests are the topic of the next chapter. N.B. if we used log\\((x+1)\\) we must remember to subtract the 1 again after taking the antilog.↩ Log transformations have a variety of other uses in statistics. One is in transforming an independent variable when looking at the form of relationships between two variables.↩ "],
["non-parametric-tests.html", "Chapter 31 Non-parametric tests 31.1 What is a non-parametric test? 31.2 How does a non-parametric test work? 31.3 Non-parametric equivalents to t-tests and ANOVA 31.4 Wilcoxon signed-rank test 31.5 The Mann-Whitney U-test 31.6 The Kruskal-Wallis test 31.7 Why use non-parametric tests … and when?", " Chapter 31 Non-parametric tests 31.1 What is a non-parametric test? The the majority of procedures we have been using to evaluate statistical significance require various assumptions about population distributions to be satisfied. These are referred to as parametric methods because they are underpinned by a mathematical model of the population(s) (we discussed this idea in the Parametric statistics chapter). For this reason, the statistical tests associated with these methods—e.g. global significance tests and multiple comparisons tests in ANOVA—are called parametric tests. This is a class of statistical tests that make much weaker assumptions. These are called non-parametric tests. The advantage of non-parametric tests is that they can be employed with a much wider range of forms of data than their parametric cousins. Although non-parametric tests are less restrictive in their assumptions, they are not, as is sometimes stated, assumption-free. The term non-parametric is just a catch-all term that applies to any test which doesn’t assume the data are drawn from a specific distribution. We have already seen a number of examples of non-parametric tests: The bootstrap and permutation test procedures introduced in the first few chapters are non-parametric techniques. The \\(\\chi^{2}\\) goodness of fit and the \\(\\chi^{2}\\) contingency table tests make weak assumptions about the frequency data. Spearman’s rank correlation test is viewed as a type of non-parametric test of association. In this chapter we are going to look at non-parametric tests which perform analyses equivalent to t-tests and the global significance test in one-way ANOVA. 31.2 How does a non-parametric test work? The key thing about the non-parametric tests we’ll consider here is that the calculations are done using the rank order of the data, whatever the type of the original data-–-ratio, interval, or ordinal. The general principle of such a test can be easily illustrated with the following example. Imagine that we want to compare two samples to determine whether they differ in their central tendency. That is, we want to know if the values in one sample tend to be larger or smaller than the values in a second sample. For example… Sample A: 1.3 2.8 4.1 3.2 Sample B: 7.2 3.0 4.2 6.2 If all the data (i.e. from both samples) are put in ascending order… Sample A: 1.3 2.8 3.2 4.1 Sample B: 3.0 4.2 6.2 7.2 Then each number can be given a rank according to its place in the ordering… Sample A: 1 2 4 5 Sample B: 3 6 7 8 It is easy to see now that if the rank values from each sample are added up Sample A will have a lower value (sum = 12) than Sample B (sum = 24). This suggests that sample B has larger values than sample A. If the samples had been completely non-overlapping the totals would have been… Sample A: 1 + 2 + 3 + 4 = 10 Sample B: 5 + 6 + 7 + 8 = 26 On the other hand, if the samples had been largely overlapping the rank totals would have been equal, or close to it… Sample A: 1 + 4 + 6 + 7 = 18 Sample B: 2 + 3 + 5 + 8 = 18 Obviously, the greater the difference in the rank totals the less likely it is that the two samples could have come from the same distribution of values—i.e. the more likely it is that difference between them may be statistically significant. The important thing to notice in such a procedure is that the original data could have been replaced with quite different values which, provided the ordering was the same, would have given exactly the same result. In other words the outcome of the test is reasonably insensitive to the underlying distribution of the data. 31.3 Non-parametric equivalents to t-tests and ANOVA There are several non-parametric procedures (all available R) which provide similar types of test to the simple parametric tests we have seen already. The actual calculations for the different tests are a little more involved than the general outline above, but they work from the same basic principle: find the rank order of all the data, and then use the ranks in different groups to assess the statistical significance of the observed differences. As always, we have to specify some kind of null hypothesis (e.g. the medians of two samples are the same) to make the significance calculation work. The three tests discussed here are: Wilcoxon signed-rank test: The test is equivalent to a one-sample and paired-sample t-test. This test also goes by the name of the Wilcoxon one-sample test, the Wilcoxon matched-pairs test, the Wilcoxon paired-sample test. It can be used to… compare a sample to a single value, or test for differences between paired samples. (The Wilcoxon signed-rank test is almost always used to evaluate the significance of differences between paired samples) Mann-Whitney U-test: This is equivalent to the two-sample t-test. It tests for differences between two unpaired samples. This test also goes by the name of the Wilcoxon two-sample test, the Mann–Whitney–Wilcoxon, Wilcoxon rank-sum test. Kruskal-Wallis test: tests for differences between several samples. This is equivalent to a one-way analysis of variance. Notice that we haven’t said anything about what kind of differences among samples these evaluate, i.e. we didn’t specify the null hypothesis. We’ll address this question as we discuss each test. 31.4 Wilcoxon signed-rank test The most widely used non-parametric equivalent to the one-sample t-test is the Wilcoxon signed-rank test. The test can be used for any situation requiring a test to compare the median of a sample against a single value. However, it is almost always used to analyse data collected using a paired-sample design, so we’ll focus on that particular application of the test. The Wilcoxon signed-rank test makes less stringent assumptions than the t-test, but it does make some assumptions: The variable being tested is measured on ordinal, interval, or ratio scales The (population) distribution of the variable is approximately symmetric53. The first assumption is simple enough—the variable can be anything but nominal. The second assumption essentially means that it doesn’t matter what form the distribution of the variable looks like as long as it is symmetric—it would not be satisfied if the distribution were strongly right- or left-skewed. As with the t-test, when applied to paired-sample data the Wilcoxon signed-rank test starts by finding the differences between all the pairs, and then tests whether these differences are significantly different form zero. The distribution under consideration is the distribution of differences between the pairs. Remember, this distribution will often be approximately normal even when the connected samples are not themselves drawn from a normal distribution. This means that even if the samples have odd distributions, we may still find that we can use a paired-sample t-test if differences have a perfectly acceptable distribution. However, if the distribution of differences is not normal then the Wilcoxon signed-rank test provides an alternative. The following is an example of a situation where the Wilcoxon signed-rank test would be appropriate. 31.4.1 Leaf damage, plant defences and feeding by winter moth larvae It has been hypothesised that plants respond to physical damage to their leaves from herbivores such as lepidopteran larvae by increasing production of defensive compounds such as phenolics. In an experiment to test this effect, birch saplings were subjected to artificial leaf damage (hole punching) on half the leaves on selected branches. After 24 h undamaged leaves from both the branches with hole-punched leaves and others distant from the damage site were collected and used in a choice test experiment with winter moth larvae. Twenty trials were carried out with a single caterpillar in each trial, offered one of each type of leaf (i.e. one leaf from close to the damage site and one from an undamaged area). The percentage of each leaf consumed was estimated (to the nearest 5%) after 24h. The data are in the file LEAF_DAMAGE.CSV: leaves &lt;- read.csv(file=&quot;LEAF_DAMAGE.CSV&quot;) glimpse(leaves) ## Observations: 40 ## Variables: 3 ## $ ID &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16... ## $ LeafType &lt;fctr&gt; Notdamaged, Notdamaged, Notdamaged, Notdamaged, Notd... ## $ Damage &lt;int&gt; 65, 0, 10, 45, 55, 45, 0, 10, 5, 70, 55, 40, 0, 70, 6... There are three variables: ID contains a unique identifier for each pair of leaves in a trial (1-20), Leaf identifies the type of leaf (‘Damaged’ vs. ‘Undamaged’), and Damage contains the percent damage score. As always, it’s a good idea to visualise the data: jitter_leaves &lt;- mutate(leaves, Damage = jitter(Damage, factor = 4)) ggplot(jitter_leaves, aes(x = LeafType, y = Damage, group = ID)) + geom_point() + geom_line() It’s not critical to learn it, but we used a new trick here: using mutate, we ‘jittered’ the Damage values (with the jitter function) to deal with the overplotting. This adds a bit of random noise to the values so that those with the same values end up being plotted in different highlights two features of the data: 1) There is plenty of variation (some larvae just eat more than others), 2) there is a tendency for larvae to prefer the undamaged leaves, though the pattern is not overwhelming. Obviously we need a statistical test—a paired sample t-test or a Wilcoxon signed-rank test are our two options. We should also visualise this distribution of within larvae differences to determine whether or not we need to use a Wilcoxon signed-rank test: # step 1 -- calculate the differences leaves_diff &lt;- leaves %&gt;% group_by(ID) %&gt;% summarise(DiffDamage = diff(Damage)) # step 2 -- make the plot ggplot(leaves_diff, aes(x = DiffDamage)) + geom_dotplot(binwidth = 10) There are large differences in both directions and the distribution does not seem to be normal (it is very ‘flat’). The problem here is that caterpillars often spend time feeding on the first leaf they encounter, which is a more or less random choice; some may change leaves in quickly in response to the quality of the leaf, others may feed for sometime before leaving the leaf. We might still expect those on better defended leaves to feed less, as the leaves should be less palatable, but they may not make the behavioural decision to leave the leaf during the short period of the experiment (the experiment cannot go on for longer because the leaves change chemically the longer they are detached from the plant) The distribution may not be normal, but it is roughly symmetrical. The symmetry assumption of the Wilcoxon signed-rank test seems to be satisfied, so we can use this test to examine whether there is any difference in caterpillar feeding on leaves from damaged areas and undamaged areas. We carry out the test using the wilcox.test function in R54. This is used in exactly the same way as the t.test function for a paired-sample design: wilcox.test(Damage ~ LeafType, paired = TRUE, data = leaves) ## Warning in wilcox.test.default(x = c(5L, 25L, 30L, 40L, 15L, 5L, 35L, ## 45L, : cannot compute exact p-value with ties ## Warning in wilcox.test.default(x = c(5L, 25L, 30L, 40L, 15L, 5L, 35L, ## 45L, : cannot compute exact p-value with zeroes ## ## Wilcoxon signed rank test with continuity correction ## ## data: Damage by LeafType ## V = 50.5, p-value = 0.07617 ## alternative hypothesis: true location shift is not equal to 0 We must remember to set paired = TRUE. If we forget to do this R will carry out the two-sample version of the test (the Mann-Whitney U-test discussed next). R produces a couple of warning here but we can usually ignore these—we can’t do much about them even if we wanted to. The elements of the output should be fairly easy to understand by now. At the top we see a summary of the test and the data: ## Wilcoxon rank sum test with continuity correction ## data: Damage by LeafType Next, we see the results: a test statistic and p-value. Although the test statistic is given as ‘V’ in the output, it is often quoted as ‘W’ when writing it in a report (we’ll adopt this convention). This seems a bit odd, but it stems from the fact that there is some variation in the literature about what we report, and how to report, non-parametric tests. The p-value is close to p = 0.05, but it doesn’t quite fall below the threshold. It looks like there is no significant difference in feeding rates on the two types of leaf. We might report this as: When given a choice, winter moth larvae did not consume larger amounts of the leaves collected from areas of the tree that are undamaged than those from damaged areas (Wilcoxon matched-pairs test: W = 139.5, n = 20, p = 0.076). 31.5 The Mann-Whitney U-test The Mann-Whitney U-test is the non-parametric equivalent to the independent two-sample t-test. The test can be used for any situation requiring a test to compare the median of two samples. The assumptions of the Mann-Whitney U-test are: The variable being tested is measured on ordinal, interval, or ratio scales The observations from both groups are independent of one another. The first two assumptions are straightforward—data can be anything but nominal, and as with a paired-sample t-test, there must not be any dependence between the observations. Though not strictly necessary for the Mann-Whitney U-test to be valid, we usually add a third assumption: The distribution of the variable in each group is similar (apart than the fact that they have a different central tendency) This assumption essentially means that it doesn’t matter what the distributions of the two samples are like, but they should be at least roughly similar—it would not be satisfied if we plan to compare data from a strongly right-skewed distribution with data from a strongly left-skewed distribution. If this assumption is not satisfied the test can still be used, but a significant result is hard to interpret (so don’t bother!). When all three of the above assumptions are satisfied the Mann-Whitney U-test is used as a way of looking for differences between the central tendency of two distributions. A two-sample t-test evaluates the statistical significance of differences between two means. The null hypothesis of the Mann-Whitney U-test (if all three of the above assumptions) is that the two distributions have the same median . A significant p value therefore indicates that the medians are likely to be different. 31.5.1 Ant foraging Let’s use the Red wood ants example from the chapter on transformations to see how to carry out the Mann-Whitney U-test. The data are the file ANTS1.CSV (not ANTS2.CSV!). Recall that this study measured the total biomass of prey being transported—the rate of food collection per ant per half hour—on three different tree species (rowan, sycamore and oak). The Tree variable contains the identities and the Food variable contains the food collection rates. Since we the Mann-Whitney U-test only compares two samples, we will focus on the oak and sycamore here to illustrate the test55. After reading the data into R we need to use the filter function to remove ‘Rowan’ cases: ants &lt;- read.csv(&quot;ANTS1.CSV&quot;) ants &lt;- filter(ants, Tree != &quot;Rowan&quot;) The Tree != &quot;Rowan&quot; inside the filter function specifies that we want cases where Tree is not equal to (!=) ‘Rowan’. Carrying out a Mann-Whitney U-test in R is simple enough, but somewhat confusingly, we have to use the wilcox.test function again. This is because, as we noted above, the Mann-Whitney U-test is also called a two-sample Wilcoxon test. The wilcox.test function for an independent two-sample design works just like the corresponding analysis with the t.test function. The first argument should be a formula, and the second should be the data frame containing the relevant variables: wilcox.test(Food ~ Tree, data = ants) ## Warning in wilcox.test.default(x = c(20.1, 47.4, 85.6, 17.1, 5.7, 7.8, ## 28.8, : cannot compute exact p-value with ties ## ## Wilcoxon rank sum test with continuity correction ## ## data: Food by Tree ## W = 453.5, p-value = 0.06955 ## alternative hypothesis: true location shift is not equal to 0 The formula (Food ~ Tree) works in the same way as other statistical function in R: the variable containing the values we wish to compare (Food) is on the left hand side of the ~ and the variable containing the group identities (Tree) belongs to the right of it. The output from a Mann-Whitney U-test is similar from that produced by the t.test function. At the top we see a summary of the test and the data: ## Wilcoxon rank sum test with continuity correction ## data: Food by Tree This version of the Wilcoxon test is the same thing as a Mann-Whitney U-test so there is nothing to worry about here. After this line we see a test statistic (’W’ilcoxon) and the associated p-value. Since p=0.07, we conclude the rates of removal of prey biomass were not significantly different between ants foraging in oak and in sycamore. In a report the conclusion from the test can be summarised… The rates of removal of prey biomass were not significantly different between ants foraging in oak and in sycamore (Mann-Whitney U-test: U=453.5, n1=26, n2=27, p=0.07). Notice how the statistics were reported. Because we are describing the test as a Mann-Whitney U-test, it is conventional to quote the test statistic as ‘U’, rather than ‘W’. If we had decided to present the test as a two-sample Wilcoxon test the test can be summarised… The rates of removal of prey biomass were not significantly different between ants foraging in oak and in sycamore (two-sample Wilcoxon test: W=453.5, n1=26, n2=27, p=0.07). …remembering to change the name (‘two-sample Wilcoxon test’) and label (‘W’) used to describe the test statistic. In either case, we also have to provide the sample sizes associated with each tree group so that a reader can judge how powerful the test was. 31.6 The Kruskal-Wallis test The Kruskal-Wallis is the non-parametric equivalent to one-way ANOVA. The Kruskal-Wallis test allows us to test for differences among more than two samples. Like the other rank-based tests we have encountered it has some assumptions, but these are less restrictive than those for ANOVA. The assumptions are essentially the same as those for the Mann-Whitney U-test: The variable being tested is measured on ordinal, interval, or ratio scales The observations from both groups are independent of one another. The distribution of the variable in each group is similar (apart than the fact that they have a different central tendency) The third assumption is important, particularly with respect to the skewness of the distributions. The test is at least reasonably robust to differences in the dispersion, but the Kruskal-Wallis test should not be used if the skewness of the variable is different among groups is very different. The reason for this is—just as with Mann-Whitney U-test—that a significant result is hard to interpret. When all three of the above assumptions are satisfied the Kruskal-Wallis is used as a way of looking for differences in the central tendency of two or more groups groups. A one-way ANOVA evaluates the statistical significance of differences between means of these groups. The null hypothesis of the Kruskal-Wallis test (if all three of the above assumptions) is that the groups have the same median. A significant p value therefore indicates that the medians are likely to be different. 31.6.1 Learning in cuttlefish In a study of the ability of cuttlefish to learn, an experiment was conducted to determine how the length of exposure to a situation influenced the learning process. Cuttlefish feed on prawns. If they are presented with prawns in a glass tube they strike at them but, obviously, fail to capture the prey. Not surprisingly, after a period of this fruitless activity, they give up striking at the prawns. In the experiment, 50 cuttlefish were divided at random into 5 groups of 10 and cuttlefish from each group were presented with prawns in glass tubes for different lengths of time: 2 min., 4 min., 8 min., 15 min., and 30 min. for the 5 groups respectively. After 24 hours the same cuttlefish were presented with prawns again and the number of strikes they made (over a fixed period) were recorded. The data are in the file CUTTLEFISH.CSV. There are two variables: Strikes contains the number of strikes recorded and Time identifies the groups (period of previous exposure): cuttlefish &lt;- read.csv(file=&quot;CUTTLEFISH.CSV&quot;) glimpse(cuttlefish) ## Observations: 50 ## Variables: 2 ## $ Strikes &lt;int&gt; 5, 12, 11, 4, 1, 6, 8, 3, 0, 5, 0, 13, 0, 3, 0, 7, 3, ... ## $ Time &lt;fctr&gt; t02, t02, t02, t02, t02, t02, t02, t02, t02, t02, t04... Take need to understand the data. Dot plots for each treatment will work here: ggplot(cuttlefish, aes(x = Strikes)) + geom_dotplot(binwidth = 2) + facet_wrap(~ Time) The data are clearly very variable. The combination of the apparent skew and the fact that the data are generally small whole numbers with several equal values in each sample, means that we may not be very successful in using a transformation to beat the data into shape. Let’s use the Kruskal-Wallis test instead. Predictably, the R function to carry out a Kruskal-Wallis test is called kruskal.test, and it is used in exactly the same way as every other statistical modelling function we have looked at: kruskal.test(Strikes ~ Time, data = cuttlefish) ## ## Kruskal-Wallis rank sum test ## ## data: Strikes by Time ## Kruskal-Wallis chi-squared = 11.042, df = 4, p-value = 0.0261 And… one more time… the elements of the output should be easy to work out. These are a statement of the test used and the data, followed by the results: a test statistic (another type of \\(\\Chi^2\\) statistic), a degrees of freedom, and the all-important p-value. We report all of these when writing up results of a Kruskal-Wallis test. However, there is some disagreement in the literature how to report a Kruskal-Wallis test—some people report the statistic as a \\(\\Chi^2\\), while others refer to it as an ‘H’ statistic. We will follow the common convention of reporting it as an ‘H’ value. The test (as with ANOVA) tells us that there is at least one difference among the groups, but it doesn’t tell where the difference or differences are. The output does not give the medians so we cannot judge how the samples are ordered. We can use dplyr to calculate the group-specific medians though: cuttlefish %&gt;% group_by(Time) %&gt;% summarise(Median = median(Strikes)) ## # A tibble: 5 × 2 ## Time Median ## &lt;fctr&gt; &lt;dbl&gt; ## 1 t02 5.0 ## 2 t04 3.5 ## 3 t08 2.5 ## 4 t15 2.5 ## 5 t30 0.0 In this case it is fairly clear that longer periods of exposure to the protected prawn do seem to result in fewer strikes in the later trial. Once we understand what is driving the significant result we’re in a position to write a summary: The frequency with which the cuttlefish attacked the prawns was significantly affected by the length of time for which they had been exposed to protected prawns 24h earlier (Kruskal-Wallis test: H=11.04, d.f.=4, p&lt;0.05), with longer prior exposure resulting in lower attack rates. If it was important to know exactly which treatments were significantly different, then some sort of multiple comparison test would be useful. There are no non-parametric multiple comparison tests available in base R, but they are implemented in the package called nparcomp. 31.7 Why use non-parametric tests … and when? 31.7.1 What are the advantages? Since most non-parametric tests make relatively weak assumptions about the distribution of the data, they are obviously useful techniques for many situations where the data we have are not well suited to parametric tests. If in doubt, a non-parametric test may be a safe bet. They also have another important feature which hasn’t been emphasised yet but is of considerable value. Since non-parametric tests work with ranks of the original data, they can be used to analyse data originally collected in ordinal, or rank, form. This is extremely useful in many investigations where the data cannot be collected any other way for example… Subjects in a psychology experiment might be asked to rank a series of photographs of people in order of attractiveness A panel of tasters judging the sweetness of wines may be able to score sweetness on a rank scale Encounters between animals might be scored on the basis of the aggressive behaviours shown which can be put in rank order (retreats, stands ground passively, fights when attacked, initiates attack) The apparatus for making actual measurements might be unavailable—but relative comparisons can be made by direct observation, perhaps in the field—e.g. ‘greenness’ of leaves, turbidity of water, crawling speed of caterpillars, order of dung fly species arrival on a new dung pat, etc. 31.7.2 What are the disadvantages? Given their advantages, there has to be a catch otherwise everyone would use non-parametric tests all the time. In fact, they are usually use as a last resort. One problem with non-parametric tests is that if the data are actually appropriate for a parametric test the equivalent non-parametric test will be less powerful (i.e. less likely to find a difference even if there really is one). For some tests the difference is not enormous—for example, if data are suitable for a t-test the Mann-Whitney U-test is about 95% as powerful as the t-test. For this reason, an appropriate transformation followed by a parametric test will yield a more powerful analysis. A second limitation of non-parametric tests is that by their very nature, they are less informative than parametric tests. For example, if we find a significant differences between group medians using a Kruskal-Wallis test, it can be difficult to understand which differences are driving the significant effect (methods are available, but they are not easy to use). On the other hand, if we can determine a suitable transformation and use an ANOVA model we can deploy tools such as multiple comparison tests to better understand the data. Parametric models make stronger distributional assumptions about the data, but in some ways they are much more flexible than non-parametric tests, i.e. there are non-parametric equivalents to some parametric tests, but there are many parametric tests for which there is no readily available non-parametric equivalent (e.g., the more complex designs of ANOVA). There is a non-parametric equivalent to ANOVA for complete randomized block design with one treatment factor, called Friedman’s test (available via the friedman.test function in R), but beyond that the options are very limited unless we are able to use advanced techniques such as the bootstrap. 31.7.3 Parting words Inappropriately distributed data can result in incorrectly high or low p-values. We should choose the statistical model we use on the basis of what the data look like in relation to the assumptions of the model, and reasons in principle, even if not clearly evident in a small sample, why the population from which the data are drawn might be expected to violate the test assumptions. On the principle that biological data rarely, if ever fully comply with the assumptions of parametric tests, it is sometimes advocated that non-parametric tests should always be used. This is not very good advice. There is more to statistics than just calculating p-values, and where possible, we prefer readily intepretable models to more ‘black box’ approaches. Use both approaches as appropriate and be aware of the strengths and weaknesses of each. There is an alternative to the Wilcoxon test which doesn’t make an assumption about the distribution being symmetrical—called the sign test. This can be done in R, but there isn’t a dedicated function for the sign test, and in any case, it is not very powerful with smaller sample sizes.↩ There are two different ways to use the wilcox.test function for a paired design. These are analogous to the two methods used to carry out a paired sample t-test: the first uses the raw data and makes use of the R’s formula system. The second supplies the function with a vector of differences↩ Of course, if we really were interested in the difference between sycamore, oak and rowan we should use a different test.↩ "],
["exercises.html", "A Exercises A.1 Week 2 A.2 Week 3 A.3 Week 4 A.4 Week 5 A.5 Week 6 A.6 Week 7 A.7 Week 8 A.8 Week 9", " A Exercises A.1 Week 2 You should work through the exercises step-by-step, following the instructions carefully. At various points we will interrupt the flow of instructions with a question. Make a note of your answer so that you can complete the associated MOLE quiz, which is called ‘Statistical Principles (I)’. A.1.1 What kind of variable is it? The following table gives a number of measurements taken in the course of a study of a woodland ecosystem. What type of variable results from the measurements taken in each case? Table A.1: Examples of different kinds of variable. Measurement Units Height of tree Metres Particulate deposits (pollution) on leaves Scale of 1 to 5 (very light to very dark) Age of tree Number of annual rings (one ring formed each year) Month of bud break Month no. 1 (Jan) - 12 (Dec) Leaf shape 1 - oval, 2 - lanceolate, 3 - palmate, 4 - pinnate Number of species of aphid on a tree Number of spp. Average number of aphids per leaf Individuals on 20 leaves Time of highest light intensity under leaves Time of day Occurrence of fungal infection on the leaves Heavy (&gt;50% of leaves), Light (&lt;50% of leaves), Absent (no infection) Tree species Latin name Aspect of ground on which each lime tree occurs Degrees (0-360) Bracken in random quadrats Visual estimate of % coverage There are no answers to this question on MOLE. If you’re not 100% sure what the right answer is in any of these examples, ask a TA or instructor for help. A.1.2 Definitions The figure below is an attempt to represent some of the concepts you’ve been studying this week (e.g. the sampling distribution, the standard error, etc): Figure A.1: What do the letters refer to? MOLE Question Assign the appropriate term (sampling distribution, standard error, etc) to the letters A-D in the figure. A.1.3 What form do sampling distributions take? A data file containing a variable sampled from three different populations (labelled A, B, and C) is available in SKEWED_POPULATIONS.CSV. Download the SKEWED_POPULATIONS.CSV file from MOLE and place it in your working directory. Read SKEWED_POPULATIONS.CSV into an R data frame called all_pops. Examine the data set—both visually and in terms of its descriptive statistics: Inspection. Use the View function and dplyr function glimpse (or str) to inspect the ‘data’. Which variables are in the data frame? What kind of variables are they (numeric, categorical, etc)? Descriptive statistics. Use the appropriate dplyr functions (group_by and summarise) to calculate the mean and standard deviation of the Values variable in each population. HINT: You will need the mean and sdfunctions to help you do this. Graphs. Use ggplot2 to construct three histograms to summarise the distribution of the variables. HINT: You will need to use geom_histogram and the facet_wrap functions to do this. Make sure that you use the original data (all_pops)—not the summarised data. MOLE Question How does the distribution of the variable differ across populations in terms of its central tendency, dispersion and skewness? In which population is the variable Values the most skewed? Now that you understand a bit about the distribution of the variable in each population we can move to the next step. You’re going to explore how the shape of a variable’s distribution influences the sampling distribution of its mean. If you’re not sure what that last sentence means, skim over the Sampling error chapter or ask a TA for help before proceeding. One way to tackle this problem is to work with each population in turn, using the bootstrapping trick to construct the sampling distribution of the mean. This involves three steps. First we have to extract the subset of values we require and store these in a numeric vector (step 1). Then we use a bit of R trickery to calculate 1000 bootstrapped means (step 2), and finally, parcel up the result into a data frame (step 3). Here’s how this works for variable A: # 1. extract the values of the variable x &lt;- filter(all_pops, Population == &quot;A&quot;)$Values # 2. carry out the bootstrapping (you don&#39;t need to understand this) boot_means &lt;- replicate(1000, mean(sample(x, size = 25, replace = TRUE))) # 3. wrap up the result as a data frame plot_df &lt;- data.frame(boot_means) Note that this code creates bootstrapped samples of 25 observations to construct the sampling distributions in the exercise above (size = 25). Once we have the bootstrapped sampling distribution of the mean we need to visualise this as a histogram. You should be able to work out how to do this using ggplot2. Construct this histogram for each of the variables. Just look at each histogram in turn. There’s no need to try to make one plot containing all three histograms. Look at each one carefully, paying close attention to the form of the original sample and the bootstrapped sampling distribution of their mean. MOLE Question Are the sampling distributions of the means more, or less, skewed than the distribution of the corresponding variables? Which variable (A, B, or C) has the most skewed sampling distribution associated with its mean? You used bootstrapped samples of 25 observations to construct the sampling distributions in the exercise above (size = 25). You can change this number by altering the size argument of the sample function. Use this fact to explore how the shape of the sampling distribution changes as you increase sample size of the ‘C’ variable. Start by using only 10 individuals in each bootstrapped sample, and gradually increase this to 100. MOLE Question What happens to the shape of the sampling distribution of the mean of the ‘C’ variable as you change the bootstrapped sample size? A.1.4 How does sample size influence the standard error? Think back to the plant colour morph example. We used a simulation in R to calculate the approximate sampling distribution of purple morph frequency estimates. We used this to examine how the amount of sampling variation changes with sample size. We noted that, in general, it seems to decline with sample size. The bigger our sample, the more precise our estimate. That might seem obvious, but what form does this relationship take? We’ve written an R function to allow you to explore how the size of samples influence the standard error of purple morph frequency estimates. You can read this into R by running the following line of R code (just copy and paste it into the Console): sample_plants &lt;- function(samp_sizes, prob) { sapply(samp_sizes, function (size) { raw_samples &lt;- rbinom(n = 10000, size = size, prob = prob) sd(100 * raw_samples / size) }) } (You are not expected to understand how this works!) This will create a function called sample_plants that’s ready for you to use. Here’s how it works: sample_plants(samp_sizes = c(10, 20, 40, 100), prob = 0.4) ## [1] 15.608569 10.946439 7.746370 4.902385 The first argument, samp_sizes = c(10, 20, 40, 100), provides the set of sample sizes we want the standard errors for, the second argument, prob = 0.4, is the frequency of purple plants (expressed as a probability) in the population. The function returns a vector of numbers that are the standard errors at each sample size. The easiest way to explore the relationship between sample size and standard error is to simply plot it. Since we use ggplot2, we need to collect together the inputs and outputs of these simulations into a data frame. Here’s one way to do this: sim_data &lt;- data.frame(sample_size = c(10, 20, 40, 100)) %&gt;% mutate(se = sample_plants(sample_size, prob = 0.4)) sim_data ## sample_size se ## 1 10 15.669804 ## 2 20 10.908217 ## 3 40 7.772273 ## 4 100 4.912963 Use the above code to vary the sample size from around 20 to 500 (the exact numbers don’t matter too much), assuming that the purple morph frequency is 0.4 (prob = 0.4). You only need to vary the values assigned to sample.size to do this. Make a plot to investigate how the standard error changes as the sample size increases. MOLE Question Does the standard error halve when you double the sample size, or is the relationship more complicated? If you think the relationship is more complicated, what form does it take? Now repeat the exercise with assuming that the purple morph frequency is 0.1 (prob = 0.1). MOLE Question Does the standard error depend on purple morph frequency? Does it get smaller or larger when we move from a frequency of 0.4 to 0.1? A.2 Week 3 You should work through the exercises step-by-step, following the instructions carefully. At various points we will interrupt the flow of instructions with a question. Make a note of your answer so that you can complete the associated MOLE quiz, which is called ‘Statistical Principles (II)’. A.2.1 Sample size and statistical power The TWO_POPS_1.CSV file contains values of a numeric variable in two different populations (labelled A and B). The file contains a large sample from each of the two populations. For the purpose of this exercise you’ll treat each these as though they are the ‘whole population’ (even though they are really just limited samples). Download the TWO_POPS_1.CSV file from MOLE and place it in your working directory, then read this into an R data frame called pop_info_1. Examine the populations—both in terms of their descriptive statistics, and visually: Inspection. Use the View function and dplyr function glimpse (or str) to inspect the ‘data’. Which variables are in the data frame? What kind of variables are they (numeric, categorical, etc)? Descriptive statistics. Use the appropriate dplyr functions (group_by and summarise) to calculate the mean, standard deviation and sample size of Values in each population. Graphs. Use ggplot2 to construct a pair of histograms to summarise the distribution of the variable in each population. This is most easily done using facet_wrap. MOLE Question Note down the key features of the distribution of the variable in each population. Does the variable seem to be normally distributed? How do the distributions differ in terms of their central tendency and dispersion among the two populations? Now that you understand the two populations a little bit we can start to experiment with them. We want you to explore what happens when you draw different sized samples from these two populations. Specifically, you’re going to explore how the sample size influences your ability to detect a difference in the population means, using a permutation test. To start with, you’ll use dplyr to simulate the process of drawing an equal sized sample from each population: # take a sample from each population use_data &lt;- pop_info_1 %&gt;% group_by(Population) %&gt;% sample_n(10) %&gt;% ungroup Copy this first chunk of dplyr code into your script and run it. After doing this, use_data will contain a sample of 10 observations from each population (use View to verify this). You haven’t seen it before, but the sample_n function just takes a sample from a data frame, i.e. sample_n(10) takes a sample of 10 observations. Using it with group_by just takes a sample from each population. The ungroup bit at the end removes the grouping information from the output. The next bit won’t work properly if you forget this. Now that you have a sample to work with, you need to use a statistical test to assess the evidence for whether or not the population means are different. You should already know the answer to this question from your initial explorations (go back to these again if you’re not sure). You’ll use a permutation test to do this. You should skim back over the Comparing populations chapter if you’re not sure how this works, or ask a TA to remind you. Here is some not-at-all-simple R code that performs the permutation test: # permutation test (difficult R code!) plt_info &lt;- replicate(1000, simplify = TRUE, { use_data %&gt;% mutate(Values = sample(Values)) %&gt;% group_by(Population) %&gt;% summarise(X = mean(Values)) %&gt;% `$`(X) %&gt;% diff }) %&gt;% data.frame(diff_means = .) There are quite a few tricks used in that R code. And yes, you are not expected to understand how it all works. Ask a TA for an explanation if you’re curious though. You just need to use it, so copy this next chunk of code into your script. Finally, here is some R code that plots the resulting null distribution of the difference between means, along with the difference actually observed in the sample (red line): # compute the difference between (more tricky code) mean_diff &lt;- use_data %&gt;% group_by(Population) %&gt;% summarise(X = mean(Values)) %&gt;% `$`(X) %&gt;% diff # plot everything (this bit should make sense to you) ggplot(plt_info, aes(x = diff_means)) + geom_histogram(bins = 18) + geom_vline(xintercept = mean_diff, colour = &quot;red&quot;) Copy this last chunk of code into your script. You should now have a script that contains all three chunks of code, in the correct order. If everything is working you should end up with a picture like this one: Figure A.2: Example null distribution from the permutation test Yours won’t be the same, as you will have used a different sample. Here’s what we want you to do with this… Using a sample size of 10 (i.e. leave sample_n(10) as it is), run all three chunks several times, checking the final plot each time before you run them again. About 10-20 runs should be enough to answer the first question… MOLE Question Is a sample size of 10 sufficient to detect a difference between the population means? Make sure you can explain your answer. Now repeat this exercise, using successively larger sample sizes, e.g. 10, 20, 40, 80, and 160. To use a sample size of 20 you would change sample_n(10) to sample_n(20). That’s all—there’s no need to make a new copy of all the code (it will end up as a big mess if you do this). Just change the sample_n part and run the new version of everything several times. You might need to experiment a bit with the sample sizes, but don;t make them much bigger than about 200. MOLE Question Which sample size seems to be sufficient to detect a difference between the population means? A.2.2 A bit more about statistical power That last exercise was all about statistical power. The statistical power of a test relates to its ability to detect an effect when it is present. You just explored how sample size affects the power of a test. In this next exercise you are going to investigate how other features of samples affect the statistical power of a test. You’ll do this by repeating the last exercise using two new pairs of populations. The information about the two new population pairs are contained in the TWO_POPS_2.CSV and TWO_POPS_3.CSV files (these have the same structure as TWO_POPS_1.CSV). Download the two files from MOLE and place them in your working directory, then read them into an R data frames called pop_info_2 and pop_info_3, respectively. Make sure you place the R code that does this near the top of your script so that it occurs before all the code that performs the permutation test. Repeat the Descriptive statistics and Graphs steps from the previous exercise to make sure you understand these new population pairs. Again, make sure you place the R code that does this before all the code that performs the permutation test (but after the reading-in-data step). MOLE Question Note down the key features of the distributions of each pair of populations. Note their central tendency and dispersion. How do the new pairs differ from the population pair used in the previous exercise? Now, for each population pair in turn, repeat the previous exercise where you varied the sample size. The only part of the permutation test and plotting code you need to alter is the first chunk. For example, to use sample sizes of 40 from the second population pair (in pop_info_2) you would use: # take a sample from each population use_data &lt;- pop_info_2 %&gt;% group_by(Population) %&gt;% sample_n(40) %&gt;% ungroup The aim of this exercise is to see how big the samples have to get before you think you can reliably detect a difference in the means using the permutation test. The ultimate goal is to understand how the distributions of the population pairs influence your ability to detect the difference in their means. Question to consider Think about the differences between three population pairs. Which aspect (or aspects) of their distributions do you think best explains the change in the statistical power of the permutation test you’ve been using? Ask an instructor or TA for the answer if you’re not sure A.3 Week 4 You should work through the exercises step-by-step, following the instructions carefully. At various points we will interrupt the flow of instructions with a question. Make a note of your answer so that you can complete the associated MOLE quiz, which is called ‘Simple Test (t-tests)’. A.3.1 Eagle owls and Norway rats A data set containing information about the sizes of Norway rat skulls in the pellets of Scandinavian eagle-owls is available in the RATSKULL.CSV file (you may have come across this before). The data comprise a column of rat skull sizes (measured in grams) and a column of codes indicating the season when a particular skull sample was taken. These data were collected in order to evaluate whether there is a difference between sizes of rats eaten in summer and winter. That is, we want to know if there is a statistically significant difference between the mean rat skull sizes in the winter and summer samples. Download the RATSKULL.CSV file from MOLE and place it in your working directory (this is the location you set at the beginning of this practical). Read the data in RATSKULL.CSV into an R data frame, remembering to assign the data frame a name. As always, we should always start by looking at the data — both visually and in terms of its descriptive statistics: Inspection. Use the View function and dplyr function glimpse to visually inspect the raw data. What are the names given to rat skull size variable and the season indicator variable? What values does the season indicator variable take? Descriptive statistics. Use the appropriate dplyr functions (group_by and summarise) to calculate the sample size, sample mean and standard deviation of each sample. HINT: you will need to use the mean, sd and length functions to help you do this. Graphs. Use ggplot2 to construct a pair of dot plots, one above the other, to summarise the winter and summer skull size distributions. HINT: you will need to use geom_dotplot and the facet_wrap functions to do this; look over the plant morph example from the beginning of this practical to see how to use these. Using the dot plots, and the descriptive statistics, conduct an informal evaluation of the assumptions of the t-test. You should re-read the relevant section above if you can’t remember what these are. MOLE question Do you feel the data conform acceptably to the assumptions? If not, explain why. Let’s carry on, assuming that we are confident that it is OK to use a two sample t-test to compare the sample means. Use the R t.test function to carry out this evaluation now. MOLE question Write a concise but complete conclusion summarising the results of the test. Is this what you expected from looking at the distributions of data in the two samples? Suggest two possible biological reasons for the result you observe. A.3.2 The power of pairing The paired-sample t-test is a very useful technique, for the simple reason that it can improve the power of simple experiments. You can get an idea of the value of a paired-sample t-test by seeing what happens when you ignore the pairing structure of a paired-design data set. We want you to do this with the glycolip data introduced in the Paired-sample t-tests chapter. The pairing was with respect to patients in this example. Let’s see what happens if you ignore the pairing. If you don’t already have it in your working directory, download the GLYCOLIPID.CSV file, and then reanalyse the data using an ordinary unpaired two-sample t-test (N.B.—This analysis is wrong!). MOLE question What result do you get and how does this compare with the paired-sample test? A.3.3 Fungal infection in French beans A plant pathologist noticed that fungal infection in roots of French beans (Phaseolus vulgaris) was rather variable among crops and hypothesized that infection might be affected by the soil type: in particular whether the beans were grown on clayey or sandy soils. Root samples were taken from beans growing in each soil type and fungal infection was measured indirectly by measuring the amount of glucosamine in the roots. Glucosamine is a fungal sugar which is polymerised into chitin which forms the cell walls of most fungi. The glucosamine concentrations (\\(\\mu\\)g g\\(^{-1}\\) root dry weight) recorded from the samples were: Sandy soil 2.3 2.4 2.5 2.6 2.8 2.7 3.1 2.3 2.5 Clay soil 2.3 2.5 2.8 3.2 2.9 3.1 3.2 Download the FRENCH_BEANS.CSV file from MOLE and place it in your working directory. Read the data into an R data frame (remember to give this a name!), inspect the data, generate some summary statistics (means and SDs) and then plot the data, just as you did in the last exercise. This should be quick to achieve–just copy and paste the code you produced, and edit this where required. Use an appropriate t-test to determine whether there is a significant difference between the amount of infection of bean roots in the two different soils. MOLE question Make a note of the results: Mean for plants on clay soil = ? Mean for plants on sandy soil = ? t = ? d.f. = ? p = ? MOLE question Write a statement of the result of the test suitable for inclusion in the results section of the plant pathologist’s report. A.3.4 Sheep, grass and nature reserves The management committee of a nature reserve wants to manage some large grassland areas of the reserve using low density sheep grazing to prevent the grass becoming too long and making the habitat unsuitable for some of the low-growing herbaceous plants for which the reserve is important. Before implementing the plan they conduct a pilot experiment using some fenced plots on the reserve, to test whether low density sheep grazing affects various species of plants. One problem is that the area is very variable—some parts are wetter than others, and the plants of interest are not particularly evenly distributed. There is also a limit to the number of plots (and sheep) they can use in the experiment. In order to make the maximum use of the resources and, take some account of the variability in the habitat the experiment is set up by randomly placing eight fenced plots around the reserve, with each plot being divided in half by a fence down the middle. Sheep are introduced to one half of each plot (the half being randomly selected in each case), and allowed to graze for the appropriate period of the year. The other half is left ungrazed. MOLE question Why is this a better design than just having separate grazed and ungrazed plots positioned at random? Over the next 2 years, the abundances of various plants in the in the plots are surveyed. The data below give the numbers of gentians from each of the eight half-plots with sheep, and the corresponding ungrazed halves after one year of the experiment. Treatment Plot 1 Plot 2 Plot 3 Plot 4 Plot 5 Plot 6 Plot 7 Plot 8 Grazed 27 1 16 8 10 19 30 9 Ungrazed 14 6 17 5 0 11 21 6 These data are stored in GENTIANS_GRAZING.CSV. Test whether there is any evidence for an effect of sheep grazing on the numbers of gentians. MOLE question What is your conclusion? MOLE question What other comparison would it be useful to be able to make in order to reach a satisfactory conclusion about the effects of grazing? What test would you do for this? A.4 Week 5 A.4.1 Oviposition behaviour in bean weevils The bean weevil, Callosobruchus maculatus, lays its eggs on the surface of legume seeds such as black-eyed beans and aduki beans. In an experiment to test whether female Callosobruchus are selective in the type of beans on which they oviposit, choice tests were carried out. In each, a single newly mated female was introduced into an experimental arena containing one each of 5 bean types. The bean on which the first egg was laid was recorded. The test was repeated 45 times with a different beetle each time. Bean type Aduki Black-eyed Kidney Haricot Pinto Number of times chosen 6 16 10 8 5 Construct a data frame containing these data. It needs to have two columns: Bean should contain the bean type, and Frequency should contain the number of times the bean was chosen. Make the following bar plot to summarise them: Carry out an appropriate test to determine whether Callosobruchus are selective in the type of beans on which they oviposit. MOLE question Summarise the results from the test. A.4.2 Determining expected values What might be an appropriate way to obtain a set of expected frequencies for an ornithologist who wants to analyse data on observations of great tits foraging to see if they prefer to forage in particular tree species? Imagine that along a transect through a wood a record was made every time a great tit was seen and of the tree species in which it was foraging… MOLE question What, if any, additional data would this require the ornithologist to collect? A.4.3 Sex and eye colour Let’s return to the initial example used to illustrate a contingency table, we might want to ask if there is an association between eye colour and sex. Data in the file CLASS.CSV show a number of attributes measured from a sample of APS students from a few years ago. Each row contains observations from one student. The two variables we are interested in here are Sex (values: ‘Female’ and ‘Male’) and Eye (values: ‘Blue’, ‘Brown’ and ‘Green’). Carry out a \\(\\chi^{2}\\) test to determine whether eye colour and sex are associated in this sample. MOLE question What do you conclude about the association between sex and eye colour? A.4.4 Eagle owls and prey choice There are many situations which may produce tables larger than a 2 x 2 contingency table. For example, we could have used the same experimental procedure as in the Callosobruchus experiment given earlier but looked at bean choice for 5 sets of females each of which had themselves been reared on a different bean type to see if selection of oviposition site is influenced by the bean type the female developed on. In fact we are going to look at prey choice between male and female eagle owls. You will recall that the prey of eagle owls can be established by examination of the pellets containing the undigested remains of their prey. In the eagle owl study the diets of the male and female of a pair were studied by examination of the pellets collected from beneath their roosts (fortunately, an individual tends to use the same roosting site, and individuals tend not to roost together). The numbers of all prey types found in the pellets were recorded. These data are in the file EAGLES.CSV. Read these data into R and inspect them to ensure you understand how they are organised. Once you understand the data, see if you can make the following bar plot to summarise them: Analyse whether there is any evidence of differences in the diets of the male and female eagle owls. MOLE question What do you conclude? MOLE question If there is an effect, what might account for the result? A.5 Week 6 A.5.1 Hedgerows and partridges Hedgerows are the main nesting habitat of the grey partridge (Perdix perdix). A survey was carried out to establish whether the abundance of hedgerows in agricultural land had an effect on the abundance of grey partridge. From an area of agricultural land covering several farms, twelve plots were selected which had land uses as similar as possible but differed, as evident from preliminary inspection, in the density of hedgerows (km hedgerow per km2). Plots were deliberately selected to cover a wide range of hedgerow densities. The total hedgerow lengths, and exact plot areas, were measured by use of large scale maps. The density of partridges was established by visiting all fields in a study plot once immediately after dawn and once just before dusk, when partridges are feeding and therefore most likely to be seen. Counts of birds observed were made on each visit and the dawn and dusk data were averaged to give a value for partridge abundance for each study plot. The data are stored in a CSV file PARTRIDG.CSV. Take note: this is a different data set than the one used in the Regression diagnostics chapter. The density of hedgerows (km per km2) is in the Hedgerow variable and the density of partridges (no. per km2) is in the Partridge variable. Read in the data and take a look at it using the View function. MOLE question Which way round should the variables be? Independent (x): Dependent (y): Make a scatter plot, fit a regression model for the relationship between hedgerow density and partridge density, plot the regression diagnostics, and carry out a statistical significance test to evaluate the relationship (use an F-test for the test). MOLE question If there is a relationship does it look linear? Ratio or interval data? Independent variable y likely to be normally distributed for each x? Variance increases or decreases markedly with increasing x? MOLE question Summarise the results of your analysis in words. Finish up by preparing a figure that summarises the data and the best fit line estimated from the regression: A.5.2 Analysing associations Consider the following scenarios amenable to analysis by either simple linear regression, Pearson’s product-moment correlation (“parametric correlation”), or Spearman’s rank correlation (“non-parametric correlation”): Shoe size and height of students in a class. We want to know if shoe size is associated with the height of male students in a class. We measure the shoe size and height from 120 students. Heart rate and age in the crustacean Daphnia reared in the lab. We want to know how heart rate changes with age. Six groups of 10 Daphnia are reared to ages 2, 4, 6, 8, 10, 12 days old. The heart rate of each individual is then assayed at their target age. Number of plant species and number of herbivorous insect species. We want to know if the diversity of herbivorous insect species is associated with plant diversity. 22 randomly located 5x5m study plots were surveyed in a field. Order of arrival of dung beetle species on elephant dung and body size. We want to know if larger dung beetle species are better at locating dung. We locate a fresh piece of dung and observe beetles arriving over a period of 3 hours, recording the weight of each new arrival. Percentage bud damage to pear trees by bullfinches. We want to know how bud damage changes with respect to the distance of trees from thick vegetation at the edge of the orchard (in which the bullfinches gather). Trees are arranged in rows parallel to the orchard edge and a single tree, selected at random from every row, is sampled for bud damage. Density of nesting stork pairs and number of human births. A science reporter from the Daily Mail is struggling to come up with a story. They think storks might deliver babies, so they collect local hospital records and amateur ornithology records covering a 20 year period in the Sheffield area. Colour of the waterbug Sigara falleni from a series of ponds and colour of the sediment from each pond. We want to know if the colour of the waterbug Sigara falleni (ranked from dark to light) is related to the colour of the sediment (ranked on same scale) from each pond. Number of copulations achieved by male elephant seals and dominance. We want to know if higher-ranked males achieve more matings. Rank is established by observing the outcome of aggressive interactions between males and the number of matings is observed over a two week period in the breeding season. MOLE question Suggest the type of analysis—and the dependent and independent variables where relevant—likely to be appropriate in each case. You may assume that if there is a relationship it will be roughly linear over the range considered. A.5.3 Diagnosing problems We have constructed four artificial data sets to practice interpreting regression diagnostic plots. Each case study corresponds to a hypothetical study where we wish to understand the dependence of numeric variable \\(y\\) on numeric variable \\(x\\). This is a simple linear regression setting where \\(y\\) is the dependent variable and \\(x\\) is the independent variable. All four data sets are stored in one file called SIMPLEREG_EGS.CSV—the dependent variable is stored in y, and the independent variable is stored in x. Each case study is identified by the Case variable (values: ‘A’, ‘B’, ‘C’ and ‘D’). You should analyse each case separately. To do this, you will need to extract a required subset of data using the filter function. Fit a linear regression model to each case in turn, and then examine the residuals vs. fitted plot, the normal probability plot, and the scale-location plot. MOLE question For each of the four case studies, make a note of your answer to the following questions: Is the linearity assumption satisfied? Is the normality assumption satisfied? Is the constant variance assumption satisfied? A.6 Week 7 A.6.1 Lead pollution in lakes The United States Water Quality and Purity Board have been monitoring the concentrations of lead in trout in lakes in New York State because of concern about toxic concentrations of lead in the food-chain, especially in trout and the birds feeding on them. Two of the lakes (Kelvin and Beaver) are surrounded by agricultural land. The other three lakes (Allsopp, Anglers and Rocky) are adjacent to built-up and industrial areas. Data for lead in trout in the lakes are in a CSV file LAKEPB.CSV. The measurements are the concentration of lead (mg kg-1) from 6 randomly sampled trout in each lake. The concentration measurements are stored in the Lead variable and the lake codes are in the Lake variable. Read this data file into R, explore the data visually, and then proceed to fit a one-way ANOVA model. Use your plot and fitted model to evaluate the assumptions of the model: MOLE question Do you feel the data conform acceptably to the assumptions on ANOVA? If the data are appropriate carry out the global significance test and write a concise summary of the results from the ANOVA: MOLE question Summary: MOLE question Do you have any comments on the possible biological reasons for the patterns you see in the data? Produce a ‘publication ready’ plot similar to the one below: A.6.2 The effect of molluscicide on rates of movement in slugs Garden slugs of the same species and approximately the same size were given either a full dose, or a half strength dose, or a one tenth strength dose of Slugit (a molluscicide) dissolved in water. A control group were treated with just water. The rate of movement of the slugs on a moist wooden board after treatment was recorded (cm per min). There were 8 replicate slugs for each treatment. The data are stored in a CSV file SLUGIT.CSV. The rate of movement is stored in the Distance variable and the treatment codes are in the SlugitTreat variable (levels: ‘full dose’, ‘half dose’, ‘one tenth dose’, ‘control’). Read in the data, explore the data — using a box and whiskers plot or a multi-panel dot plot — and then fit a one-way ANOVA model. Use the exploratory plot and fitted model to evaluate the assumptions of ANOVA: MOLE question Do you feel the data conform acceptably to the assumptions on ANOVA? Data type? Normally distributed? Similar variances in each treatment? If the data are appropriate carry out the global significance test and write a concise summary of the results from the ANOVA: MOLE question What are the results from the ANOVA? MOLE question Write a summary of the results from the analysis. Produce a ‘publication ready’ plot similar to the one below: A.6.3 Multiple comparisons — slug movement and molluscicide Return to the slug molluscicide exercise and carry out the Tukey HSD test on the movement rates of slugs following different doses of Slugit. MOLE question Prepare a summary of differences between means. MOLE question Comment on any features of biological interest in the results. A.7 Week 8 A.7.1 Limpets and salinity Limpets live on rocky shores, often in rock pools and on exposed rock surfaces. Those limpets which live in rock pools can experience a very wide range of salinity conditions during the daily tidal cycle, especially if fresh water from inshore flows into the pools when the tide is out. A researcher was interested in whether two different limpet species (Acmaea scabra and Acmaea digitalis) differ in their internal concentration of sodium ions when exposed to low salinity conditions. Eight limpets of each species were placed in tanks of water containing (100% sea water; 75% seawater diluted with distilled water, and 50% sea water diluted with distilled water. The data are stored in a file called ACMAEA.CSV. The layout of the data should be fairly obvious: sodium ion concentration is stored in the Sodium variable, species codes are in Species (levels: ‘scabra’ = A. scabra, ‘digitalis’ = A. digitalis), and salinity is in Salinity (levels: 50, 75 and 100). Use glimpse to understand the data set. What kind of variable is Salinity? It is not a factor. When R reads in the data it assumes Salinity is meant to be treated as a numeric vector because it contains numeric values. We need to convert it to a factor before proceeding so that lm fits the right kind of model (i.e. an ANOVA model). Use the mutate function with the factor function to convert Salinity to a factor variable. These data can be used to answer the following questions: Does salinity affect the sodium concentration in the limpets? Do the two species differ in their internal sodium concentrations? Do the species differ in their response to changes in salinity? Prepare an interaction plot to allow you to visually assess the likely answers to these questions: Once you are satisfied you understand the relationships proceed with a statistical analysis to address the three questions. MOLE question What effects does the analysis indicate? Term F d.f. p Salinity effect: ? ? ? Species effect: ? ? ? Interaction: ? ? ? Use your knowledge of the data and the fitted model object to check the assumptions of your analysis. MOLE question What type of data are they? MOLE question Do the data satisfy the assumption of normality? MOLE question Are the variances similar among treatment combinations? Finally, think about how to best summarise the results in written form. MOLE question Summarise the results from the ANOVA in words. If you find any the global significance tests to be significant, it may be appropriate to carry out one, or more, multiple comparison tests. You should not carry out tests for any effects that are not significant (there’s no point in testing means you already know not to be different!). MOLE question If there were significant effects in the ANOVA summarise the differences between the means using a table of means with appropriate letter codes, and in words. MOLE question What does the analysis suggest about the osmoregulation of Acmaea. Finally, see if you can prepare the following “publication quality” plot to summarise the results: A.7.2 Sheep, grass and nature reserves (again) Take another look at the Sheep, grass and nature reserves question from the week 4 (the data are in GENTIANS_GRAZING.CSV). This was an example of a paired design, where the variable of interest was species diversity in pairs of grazed and ungrazed plots. The data were originally analysed using a paired-sample t-test. Repeat the analysis, but now do it in two ways: 1) use a paired-sample t-test, 2) use an appropriate ANOVA. Warning: Check the data carefully after you read it in. Pay close attention to the type of variable that R creates for the Plot column. You may need to prepare the data for analysis first… MOLE question How similar are the results relating to grazing impacts from the two analyses? MOLE question What is the mathematical relationship between the t-statistic from the paired-sample t-test and the F-ratio (for the treatment effect) from the ANOVA? A.8 Week 9 A.8.1 Fungal pathogen infection on leaves (No actual data are provided for this exercise—we just want you to think about the hypothetical data) Spores of a particular fungal pathogen infect leaves of a tree wherever the spores happen to land and the subsequent development of the fungus causes a single distinct ‘pustule’ on the leaf at each infection site (typically &lt;20 pustules are found on each leaf). Imagine you have data from a study comparing the intensity of infection between canopy and sub-canopy leaves. MOLE question What sort of transformation might be appropriate for these data? A.8.2 Pollution sensitive stoneflies - what’s going into the river? The data for this exercise are in STONEFLY.CSV. Counts of the abundances of stonefly nymphs (which are generally intolerant of organic pollution) at three sites are stored in the Stonefly variable. The Site variable has three values (‘Above’, ‘Adjacent’ and ‘Downstream’) which index the three study site: immediately above (‘Above’), adjacent to (‘Adjacent’), and 0.5 km downstream (‘Downstream’) of a discharge point for a storm drain. Read these data into R and examine them to evaluate whether they are suitable for using one way ANOVA to test for differences in abundance at the three sites. Hint: fit the model and construct the regression diagnostic plots. Suggest a transformation that may help. Carry out the transformation to see whether it has the desired effect. MOLE question What do you recommend and why? MOLE question What do you learn from the diagnostics derived from the ANOVA with the transformed data? A.8.3 Ants again The data for this exercise are in ANTS2.CSV. These data describe ant foraging on sycamores and oaks. The number of lepidopteran caterpillars observed as prey items in ants foraging are in the Caterpillars variable. The total number of prey items being carried during the observation period (1h) are in the Total variable. The The Tree variable has two values (‘Sycamore’ and ‘Oak’) that index the tree type. Express the number of lepidopteran larvae taken as a proportion of all prey (use mutate), and test whether caterpillars constitute a significantly higher proportion of the diet in oak than sycamore. MOLE question Is a transformation appropriate? If so what? MOLE question What test is required? MOLE question What do you conclude from the test? A.8.4 Bryophyte diversity in a woodland The data for this exercise are in BRYOPHYTE.CSV. As part of a survey of bryophyte communities in two areas of woodland with differing canopy species data of species diversity was recorded by randomly positioning quadrats (1m by 1m) and recording the species found in each quadrat. As part of the analysis, of the data, the surveyor wants to determine whether the species diversity (no. of spp. per quadrat) is different between the two sites. The Site variable indexes the site (1 or 2) and the Bryophyte variable contains the species diversity. Examine these data using the View function and prepare a plot to visualise these data. MOLE question Are the data suitable for analysis with a t-test? If not why not? MOLE question Is there a transformation that would help? A.8.5 Reporting the results of non-parametric tests MOLE question You might sometimes see a statements such as: The means were significantly different (Mann-Whitney U-test: U=43, n1=14, n2=14, p&lt;0.05). What is wrong with this? A.8.6 Copper tolerance in Agrostis A study was carried out to examine how quickly copper tolerance is acquired in the grass Agrostis stolonifera growing on copper contaminated soil. Plants from two lawns, planted 8 years and 14 years ago, around the buildings of a copper refinery, were tested for copper tolerance by growing them in a standard liquid culture medium with elevated levels of copper. Root extension (in mm) was measured for each plant over a 14 day period. The data are in the file LAWNS.CSV. There are two variables: Roots contains the measured root extension and Lawn identifies the two groups (years of exposure). Read these data into R, calling the data frame copperlawn. Examine the data with View. Have a look at the distributions of the data (using histograms, dot plots, or whatever method you think best). With 10 and 15 values it is, as always, hard to tell whether or not the data are drawn from a normally distributed population, although they don’t look particularly normal. However, consideration of the nature of the data might also lead us to be cautious. Copper contamination may be patchy in the lawn, so there may be a mixture of more and less tolerant individuals, and depending on the nature of the genetic control of tolerance, it may have a distribution that is not clearly unimodal. In this case, it doesn’t look as though a transformation is obviously going to help, and although we might be prepared to risk a parametric test, a non-parametric test is safer. Use an approrpiate non-parametric test to evaluate whether root growth, in culture solution, differs between plants from the two lawns. MOLE question Summarise the conclusion from the test and think about what the results suggest. A.8.7 Measuring seed dispersal An investigator was interested in the dispersal abilities of a number of plant species which reinvade disturbed ground by means of windborne seed. To try and measure the seed influx they put out a tray of sterilised potting soil at each of 10 locations around a newly disturbed site. Each week for 11 weeks they remove the trays and replace them with new ones. The collected trays are covered and brought into a glasshouse where any seeds they contain are allowed to germinate. From this procedure they know for each plant species the week (1-11) when it first appeared at each location - a value of 12 is given to any species that didn’t arrive at a location by the end of the experiment. You can use these data to test whether, for the four plant species studied, there is any significant difference in dispersal rates between species. The data are in the file DISPERSAL.CSV. There are two variables: Week contains the arrival week and Species identifies the four species (‘A’ - ‘D’). Read these data into R, examine them with View, and make an informative plot. Once you understand the data, use an appropriate non-parametric test to evaluate whether the four species differ significantly in dispersal ability (at least as measured by speed of colonisation). MOLE question Write a statistically supported conclusion from the test: "],
["choosing-models-and-tests.html", "B Choosing models and tests B.1 Introduction B.2 Getting started… B.3 A key to choosing statistical models and tests B.4 Four main types of question B.5 Question 1 –- Comparison of group means or medians B.6 Question 2 – Associations between two variables? B.7 Question 3 -– Frequencies of categorical data B.8 Variables or categories?", " B Choosing models and tests B.1 Introduction One of the more difficult skills in data analysis is deciding which statistical models and tests to use in a particular situation. This book has introduced a range of different approaches, and has demonstrated a variety of biological questions that can be addressed with these tools. Here we draw together the statistical tools we’ve encountered and explain how to match them to particular kinds of question or hypothesis. However, before diving into an analysis, there are a few things to consider… B.1.1 Do we need to carry out a statistical analysis? This may seem like an odd question to ask, having just spent a considerable amount of time learning statistics. But it is an important one. There are many situations in which we don’t need, or can’t use, statistical tools. Here are two common ones: There are no statistical procedures that will allow us to analyse our data correctly56. This happens sometimes. Even with careful planning, things don’t always work out as anticipated and we end up with data that cannot be analysed with a technique we know about. If the data can’t be analysed in a sensible way there is no point doing any old analysis just because we feel we have to produce a p-value. Instead, it’s time to seek some advice. We could quite correctly apply a statistical test to your data, but it would be entirely superfluous. We don’t always need statistics to tell us what is going on. An effect may be exceptionally strong and clear, or it may be that the importance of the result is not something captured by applying a particular statistical model or test57. This caveat is particularly relevant to exploratory studies, where the goal is to use the data to generate new hypotheses, rather than test a priori hypotheses. Also in the category of superfluous statistics are situations where we are using statistics in a technically correct way, but we’re evaluating effects that simply are not interesting or relevant to the question in hand. This often arises from a misplaced worry that unless we have lots of statistics in our study it somehow isn’t ‘scientific’, so we apply them to everything in the hope that a reader will be impressed. Resist the temptation! This strategy will have the opposite effect—a competent reader will just assume you don’t know what you’re doing if they see a load of pointless analyses. B.2 Getting started… If there is a need for statistical analysis then the first thing to do is read the data into R and then… resist the temptation to start generating p-values! There are a number of things to run through before leaping into a statistical analysis: Be sure to carefully review the data after importing it into R (functions like View and glimpse is good for this). There are a number of things to look out for… Understand how the data is organised Most importantly, is the data set ‘tidy’? Each variable should be one column and each row should correspond to one observation. The majority of statistical modelling tools in R expect the data to be organised in this format, as do dplyr and ggplot2. If it isn’t already tidy, a data set will need to be reorganised first. We can always do this by hand, but it is almost always quicker to do it with R (the tidyr package is really good at this). Understand how R has encoded the variables. Examine the data using functions like glimpse or str. Pay close attention to the variable types—is it numeric, a character, or a factor? If a variable is not appropriate for the planned analysis, make any necessary changes. For example, if we plan to treat a variable as a factor because we’re going to carry out ANOVA, but it has been read in as a number, we’d need to convert it to a factor before preceding. Check whether or not there are any missing values. These appear as NA in R. If they are present, were they expected? If not, check the original data source to determine what has happened. If we’re absolutely certain the missing values represent an error in the way the data were coded then it might be sensible to fix the source data. However, if there is any doubt about how they arose, it is better to leave the source data alone and deal with the miscoded NAs in the R script. Ensure there are no miscoded values. In an ideal world we should leave the source data alone and fix these miscoded values in the R script. Why? Because changing the source data is slow and runs the risk of introducing new errors. It’s easy to edit an R script and rerun it. Editing source data is time consuming and error prone. If this is too difficult then fix these in the source data. Either way, there’s no point starting an analysis until the data are error free. Spend some time thinking about the variables in the data set. Which ones are relevant to the question in hand? If appropriate, decide which variable is the dependent variable (the ‘y’ variable) and which variable(s) is (are) the independent variable(s)? What kind of variables are we dealing with—ratio or interval scale numeric variables, ordinal or nominal categorical variables? It is much easier to determine which analysis options are available once these details are straightened out. Make at least one figure to visualise the data. We have done this throughout this book. This wasn’t to fill the time—it is a crucial step in any data analysis exercise. Informative figures allow us spot potential problems with the data and they give us a way to evaluate our question before diving into an analysis. If we can’t see an appropriate way to visualise the data, we probably aren’t ready to start doing the statistics! Steps 1-3 are all critical components of ‘real world’ data analysis. It may be tempting to skip these and just get on with the statistics, especially when pressed for time. Don’t do this! The ‘skip to the stats’ strategy always leads to a lot of time being wasted, either because we fail to spot potential problems or because we end up carrying out an inappropriate analysis. B.3 A key to choosing statistical models and tests The choice of statistical model/test is affected by two things: The kind of question we are asking. The nature of data we have: what type of variables: ratio, interval, ordinal or nominal? are the assumptions of a particular model or test satisfied by the data? The schematic key (below) provides a overview of the statistical models and tests we’ve covered in this book, structured in the form of a key. The different choices in the key are determined by a combination of the type of question being asked, and the nature of the data under consideration. Figure B.1: A key for choosing statistical models and tests. The notes that follow the key expand on some of the issues it summarises and explain some of the trickier elements of deciding what to do. The key is quite large, so it is hard to read easily a web browser (it also doesn’t render very well in Firefox for some reason). Either download a PDF copy of the key or right click on the figure to open it on its own in a new tab and then zoom in. B.4 Four main types of question Question 1: Are there significant differences between the means or medians (‘central tendency’) of a variable in two or more groups, or between a single mean or median and a theoretical value? This first question is relevant when we have measurements of one variable (e.g. plant height) on each experimental unit (e.g. individual plants) and experimental units are in different groups. If there is more than one group, one or more variables (factors) are used to encode group membership (given by the factor levels). Keep in mind that these grouping factors are distinct from the variable being analysed—they essentially describe the study design. This type of question includes anything where a comparison is being made between the variable in one group and… a single theoretical value another group whose values are independent of those in the first group (independent design) more than one other group a second group which has values that form logical pairs with those in the first group (paired design)58 The measurement scale of the variable in these situations may be ratio, interval, or ordinal. The only variable for which the statistical tools described here would not be suitable are categorical variables Question 2: Is there a association between two variables? What is the equation that describes the dependence of one variable on the other? Where Question 1 is concerned with comparing distinct groups, where we have measurements of one variable on each experimental units, Question 2 occurs where we’ve taken two different measurements for each experimental unit (e.g., plant size and number of seeds produced). Here we are interested in asking whether, and perhaps how, the two variables are associated with each other. Here, again, the measurement scale of the variable in these situations may be ratio, interval, or ordinal. Question 3: Are there significant differences between the observed and expected number of objects or events classified by one or more categorical variables? Question 3 is the only question which is focused on the analysis of categorical variables. Here we have situations where the ‘measurements’ on objects can be things like colour, species, sex, etc. In these situations we analyse the data by counting up how many of the objects fall into each category, or combination of categories. The frequency of counts across the categories can then be tested against the against some predicted pattern. B.5 Question 1 –- Comparison of group means or medians B.5.1 Question 1 How many groups? Within the set of situations covered by Question 1, there are some further subdivisions: we need to decide whether: we have one group only (and a theoretical or expected value to compare it against), we have two groups, or we have more than two groups. Usually this should be a fairly straightforward decision. Confusion sometimes arises in situations similar to Festuca experiment, where the observations are classified by two factors (pH and and Calluna). What exactly do we mean by ‘a group’ in this situation? The answer is that if we have more than one factor, then we must think of each group as being the set of observations defined by each combination of factor levels. So in the case of the Festuca experiment, there were four different combinations of the two factors (with and without Calluna, at either pH level). The simplest thing to remember is that if we have more than one factor (regardless of how many levels the factors have) then we must be dealing with more than two groups. B.5.2 [Question 1] Single group When we have a single group (1a) the only thing that remains to be done is check the type and distribution of the variable. If the data are approximately normally distributed then the obvious test is a one-sample t-test. If the data are not suitable for a t-test, even after transformation, then we could use a Wilcoxon test (we studied this in terms of a paired design, but remember, a paired experimental design is ultimately reduced to a one-sample test). B.5.3 [Question 1] Two groups If we have two groups then there is a further choice to be made: whether there is a logical pairing between variable measured in the two groups, or whether the data can be regarded as independent. This sometimes causes problems, particularly where the pairing is not of the obvious sort. One useful rule-of-thumb is to ask whether there is more than one way the data could be ‘paired’. If there is any uncertainty about how the pairing should be done, that is probably an indication that it isn’t a paired situation. The most common problem, however, is failing to recognise pairing when it exists. When faced with paired design the test involves first deriving a new variable from the differences among pairs, and then using this variable in a one-sample test. Either a one-sample t-test or Wilcoxon paired-sample test is required, depending on whether the new variable (the differences) is approximately normally distributed or not. If the data are independent then a two-sample t-test or Mann-Whitney -test will be the best approach. B.5.4 [Question 1] More than two groups The first decision here is about the structure of the data. This sometimes causes problems. There are a variety of different situations in which we may be interested in testing for differences between several means (or perhaps medians). Most of the time these will involve either a one-way comparison in which each observation can be classified as coming from one set of treatments (one factor), or a two-way comparison in which each value comes from a combination of two different sets of treatments (two factors). It is easy to mix these situations up if we’re not paying attention. One way to try and establish the structure of the data is to draw up a table… If the data fit neatly into the sort of table below, where there is one factor (e.g. factor ‘A’) which has two or more levels (e.g. level 1, 2, 3) then we have a one-way design. The treatments designated by the levels of the factor in this situation are typically related in some way (e.g. concentrations of pesticide, temperatures). The only question it makes sense to address with these data is whether there are differences among the means of the three treatments. FACTOR A Treatment 1 1,4,6,2,9 Treatment 2 7,3,8,9,4 Treatment 3 5,3,7,6,4 If there are two different types of treatment factor (factors ‘A’ and ‘B’) and within each factor there are two or more levels then we have a two-way design. The treatments designated by the levels of a particular factor are typically related in some way, but the set of treatments associated with each factors are typically not related to one another. We could not draw up a table like the first one (above) and fit these data into it, because each observation occurs simultaneously in one treatment level from each of the two factors (below). Treatment B1 Treatment B2 Treatment B3 Treatment A1 1,4,6 3,9,1 2,2,7 Treatment A2 7,3,8 2,3,6 9,3,4 Treatment A3 5,3,7 1,8,6 2,2,6 From the data in this design, we can ask more than one question (i.e. there are several different tests associated with this design). It clearly makes sense to ask questions about the main effects and the interaction: main effect of factor A: difference among row means main effect of factor B: difference among column means interaction: differences among means for each different combination of treatments of factors A and B (individual cells of the table). When a data set can be described by the second table, but there is only one value in each cell of the table, then we still have a two-way design, but now without replication. It is possible to analyse this design, but we there is no way to assess the interaction between treatments—we can only test main effects with this kind of design. This design is most often associated with a Randomized Complete Block Design with one treatment factor (the thing we care about) and one blocking factor (a nuisance factor). Having established whether we have a one-way or two-way design we need to determine whether the data are likely to satisfy the assumptions of the model we’re presented with. We can start to make this evaluation by plotting the raw data (e.g. using a series dot plots). Occasionally it will be obvious at this stage that the data are not suitable for ANOVA. However, things are often not so clear so it’s better to fit the ANOVA model and use regression diagnostics to check whether the assumptions are satisfied. In the case of a one-way design we have the option of a non-parametric Kruskal-Wallis test when the data are not suitable for ANOVA. Otherwise we need to find a suitable transformation and then use a one-way ANOVA. Remember, before turning to a Kruskal-Wallis test it is a good idea to see if the data can be made suitable for ANOVA by transformation. In the case of the two-way design, we don’t have the option of a non-parametric test. If the data are suitable (or can be made suitable by transformation), then use a two-way ANOVA, otherwise there is no choice but to break the analysis down into its component parts, which can be analysed as one-way designs using non-parametric methods. This is far from ideal though because we lose all information about the interactions by doing this (which somewhat defeats to purpose of designing a two-way experiment). (N.B. In the special case of a two-way design without replication, then there is a non-parametric test—Friedman’s test—that can be used instead of normal two-way ANOVA.) Finally, we should consider the option of multiple comparison tests. These should only be used if the global significance test from an ANOVA is significant. Additionally, in the two-way ANOVA, there is a further consideration, which is whether the main effects (one or both of them) or the interaction are significant. If the interaction is significant then the multiple comparison should be done for the interaction means (i.e. the means in each treatment combination). If the interaction is not significant, then the significance of the differences between the main effect means (whichever are significant) should be evaluated. B.6 Question 2 – Associations between two variables? Assuming it is an association we’re after—not a difference between groups (see box below)—then the main decision we need to make is whether to use a correlation or regression analysis. B.6.1 [Question 2] Testing \\(y\\) as a function of \\(x\\), or an association between \\(x\\) and \\(y\\)? The choice between regression and correlation techniques, for analysing the relationship between two variables, depends on the nature of the data and the purpose of the analysis. If the purpose of the analysis is to find (and describe mathematically) the relationship that describes the dependence of one variable (\\(y\\)) on another (\\(x\\)), then this points to a regression being the most appropriate technique. If we just want to know whether there is an association between two variables, then this would suggest correlation. However, it is not just the goal of an analysis that matters, unfortunately! The two techniques also make different assumptions about the data. For example, regression makes the assumption that the \\(x\\)-variable is measured with little error relative to the \\(y\\)-variable, but doesn’t require the \\(x\\)-variable to be normally distributed. Pearson’s correlation assumes that both \\(x\\) and \\(y\\) are normally distributed. So the final decision about which method to use may depend on trying to match up both the question and the nature of the data. It sometimes happens that we want to ask a question that requires a regression approach (e.g. we need an equation that describes a relationship) but the data are not suitable. In this situation it can be worth proceeding with regression, bearing in mind that the answer we get may be less accurate than it should be (though careful use of transformations may improve things). Once we decide a correlation is appropriate, then the choice of parametric or non-parametric test should be based on the extent to which the data match the assumptions of the parametric test. A final point here that can cause difficulties is the issue of dependence of one variable on another. In biological terms we are often interested in the relationship between two variables, one of which we know is biologically dependent on the other. However, designating one variable the dependent variable (\\(y\\)) and the other the independent variable (\\(x\\)) does not imply that we think ‘y depends on x’, or that ‘x causes y’. For example, tooth wear in a mammal is dependent on age. However, imagine that we have collected a number of samples of mammalian teeth from individuals of a species which have died for various reasons, and for which we also have an estimate of age at death. We may want to find the an equation for the relationship between age (\\(y\\)) and tooth wear (\\(x\\)). Why? Well for example, in a population study you might often recover remains of individuals that have died, from which you can obtain the teeth (even if not much else remains). It could be useful to be able to use the measurement of tooth wear to estimate the age of the individuals, and to do this you want the equation that describes the ‘dependence’ of age on tooth wear. So here the direction of dependence in the analysis is not the same as the causal dependence of the two variables. The point is that the choice of analysis does not determine the direction of biological dependence—it is up to us to do the analysis in a way that makes sense for the purpose of the study. Qustion 1 or Question 2? Although it seems straightforward to choose between Question 1 and Question 2, it does sometimes cause a problem in the situation there are two groups, the data are paired, and the same variable has been measured in each ‘group’. Because two sets of measurements on the same objects (say individual organisms) fit the structure a paired-sample t-test or a regression (or correlation), it is very important to identify clearly the effect we want to test for. A concrete example will help make this clearer… The situation where confusion most easily arises is when the same variable has been measured in both groups. Imagine we’ve collected data on bone strength from males and females in twenty families, where the males and females are siblings—a brother and sister from each family. The pairing clearly makes sense because the siblings are genetically related and likely to have grown up in similar environments. Consider the following two situations… If our goal is to test whether males and females tend to have different bone strengths, then a paired-sample t-test makes sense: it compares males with females controlling for differences due to relatedness and environment. If our goal is to test whether bone strength runs in families then the t-test is no use. In this case it makes sense to evaluate whether there is a correlation in the bone strength of sibling pairs (i.e. if one sibling has high bone strength then does the other as well?). So while the data can be analysed in either way, it is the question we’re asking that is the critical thing to consider. Just because data can be analysed in a particular way, doesn’t mean that analysis will tell us what we want to know. One way to do tackle this sort of situation is to imagine what the result of each test using those data might look like and think about how one might interpret that result. Does that interpretation answer the right question? B.7 Question 3 -– Frequencies of categorical data This kind of question relates to categorical or qualitative data, (e.g. the number of male versus female offspring; the number of black versus red ladybirds, the number of plants of different species). The data are frequencies (counts, not means) of the number of objects or events belonging to each category. The principle of testing such data is that the observed frequencies in each category are compared with expected (predicted) frequencies in each category. Deciding between goodness of fit tests and contingency tables is generally fairly straightforward once we’ve determined whether counts are classified by a single categorical variable, or more than one categorical variable. If there is more than one categorical variable, then it should be possible to classify each observation (organism, event, habitat, location, etc.) into one category of each categorical variable, and that allocation should be unique. Each observation should fit in only one combination of categories. There is a further difference which may also affect our choice. In the case of a single factor, the question we ask is whether the numbers of objects in each category differ from some expected value. The expected values might be that there are equal numbers in each category, but could be something more complicated—it is entirely dependent on the question we’re asking. In the case of the two-variable classification, the test addresses one specific question: is there an association between the two factors? The expected numbers are generated automatically based on what would be expected if the frequencies of objects in the different categories of one factor were unaffected by their classification by the other factor. B.8 Variables or categories? One final issue that sometimes causes questions is when a variable is treated as categorical when it is really a continuous measure or when a continuous variable is made into categories. There is indeed some blurring of the boundaries here. Two situations are discussed below. B.8.1 ANOVA vs. regression There are many situations in which data may be suitable for analysis by regression or one-way ANOVA, even though they are different kinds of models. For example, if a farmer wishes to determine the optimal amount of fertiliser to add to fields to achieve maximum crop yield, he might set up a trial with 5 control plots and 5 replicate plots for each of 4 levels of fertiliser treatment: 10, 20, 40 and 80 kg ha NPK (nitrogen, phosphorus and potassium fertiliser) and measure the crop yield in each replicate plot at the end of the growing season (kg ha year). If we are simply interested in determining whether there is a significant difference in yields from different fertiliser treatments, and if so which dose from the levels we have used is best, then ANOVA (and multiple comparisons) is probably the best technique. On the other hand we might be interested in working out the general relationship between fertiliser dose and yield, perhaps in order to be able to make predictions about the yield at other doses than those we have tested. If the relationship between fertiliser and yield was linear, or could be made so by transformation, then we could use a regression to determine whether there was a significant relationship between the two, and describe it mathematically59. B.8.2 Making categories out of continuous measures Sometimes you will have data which are, at least in principle, continuous measurements (e.g. the abundance of an organism at different sites), but have been grouped into categories (e.g. abundance categories such as 1-100, 101-200, 201-300, etc. ). One question is whether these count as categories – and hence whether for example you could look at the association between abundance categories and habitat by taking the frequencies of samples in each abundance category across two different habitats and examining the association using a contingency table test. The answer is that this would indeed be a perfectly legitimate procedure (though it may not be the most powerful). It is a good idea not to group numeric data into categories if it can be avoided because this throws away information. However, it isn’t always possible to make a proper measurement, though we can at least assign observations to ordinal categories. For example, when observing flocks of birds we might find that it’s impossible to count them properly, but we can reliably place the numbers of birds in a flock into abundance categories (1-100, 101-200, etc. ). Many variables that we treat as ordinal categories could in principle be measured in a more continuous form: ‘unpigmented’, ‘lightly pigmented’, ‘heavily pigmented’; ‘yellow’, ‘orange’, ‘red’; ‘large’, ‘small’… These are all convenient categories, but in one sense they are fairly arbitrary. This doesn’t mean that we can’t construct an analysis using these categories. However, one thing to bear in mind is that when we divide a continuous variable into categories, decisions where to draw the boundaries will affect the pattern of the results. And, of course, there are many ‘true’ categorical data: on the whole things like male/female, species, dead/alive, can be regarded as fairly unambiguous categories. Alternatively, an appropriate technique exists, but we don’t know about it!↩ As the old joke goes: What does a statistician call it when ten mice have their heads cut off and one survives? Not significant.↩ In a paired design the two groups aren’t really separate, independent entities, in the sense that pairs of measurements have been taken from the same physical ‘thing’ (site, animal, tree, etc).↩ One additional potential advantage of regression in this kind of situation is that it might result in more a powerful statistical test of fertiliser effects than ANOVA. This is because a regression model only ‘uses up’ two degrees of freedom—one for each of the intercept and slope—while ANOVA uses four (n-1). A regression makes stronger assumptions about the data though, because it assumes a linear relationship between crop yield and fertiliser.↩ "],
["writing-a-scientific-report.html", "C Writing a scientific report C.1 Introduction C.2 The structure of a scientific report C.3 Presenting species names C.4 Approaches to writing C.5 Further reading", " C Writing a scientific report “The question is,” said Alice, “whether you can make words mean so many different things.” “The question is,” said Humpty Dumpty, “which is to be master – that’s all.” Lewis Carroll, Through the Looking Glass (1871) C.1 Introduction Scientific information is communicated in a variety of ways, through talks and seminars, through posters at meetings, but mainly through scientific papers. Papers, published in books or journals provide the main route by which the substance of scientific findings are made available to others, for evaluation and subsequent use. Over time the scientific paper has developed into a fairly formal method of communication, with certain structures, styles and conventions. These mean that information is presented in a standardised way, meaning particular bits of information can be extracted more easily by a knowledgeable reader. In this chapter, we will examine the structure and conventions of a biological paper, using an example of a field study of the territorial behaviour of a damselfly. Our aim is to illustrate the typical form and content of a scientific paper. Of course papers vary in their exact requirements, and no one example can cover all the possibilities. Read recent papers in a relevant subject area and analyse what styles and structures they use, and which work best. The structures and conventions discussed below are not rules and should be flexibly interpreted, under the guiding principle that the aim is to present information as clearly, concisely and unambiguously as possible. Although taking the scientific paper as a model, the principles here apply equally to other, less formal project write-ups and reports. C.2 The structure of a scientific report The normal scientific report has a standard structure (parts in parentheses are optional): Title Abstract / Summary Introduction Methods Results Discussion (Acknowledgements) Literature cited (Appendix) C.2.1 Title Although not really a section of the paper, it is worth giving the title some thought. Aim for something that gives a fairly specific description of the topic of the paper, and possibly the essential result, but without being too long: Diurnal changes in the depth distribution of copepods in lakes with and without planktivorous fish: evidence of a predator avoidance mechanism? An experimental study of the effect of food supply on laying date in the coot. The distribution and altitudinal limits of bracken (Pteridium aquilinum) in the North York Moors National Park. Reverse transcription-PCR detection of LaCrosse virus in mosquitoes and comparison with enzyme immunoassay and virus isolation. The important thing to note is that the titles contain a good deal of specific information—we have a pretty good idea what the paper is about before we read it. Avoid vague titles such as… A study of damselfly behaviour …when in fact we have looked at is the mating and oviposition behaviour of damselflies of a particular species in relation to the current speed in different areas of the river and what we want to say is: The influence of river flow rate on mating behaviour and oviposition in the damselfly Calopteryx splendens Don’t put irrelevant specific information in the title. It might be irrelevant to say that we carried out a study in a particular river—that detail is probably not important for the question we’re asking . The reference to the North York Moors above, however, is relevant because the study is of an area-specific problem (the study is primarily of use to people who want to know about bracken in that area). C.2.2 Abstract or Summary The purpose of an abstract is to present a factual summary of the main purpose, results and conclusions of the report which is short and makes sense on its own. Often it is best (and some journals require it) to do this as 3-6 numbered points comprising some, or all, the following: The scope and purpose of the study Methods (not always necessary) Result 1 Result 2… Conclusion For example: The territorial behaviour, mating frequency and oviposition of Calopteryx splendens (Charpentier) (Odonata: Calopterygidae) were studied in relation to the water flow rate in the territories (weed patches) of individual males. Weed patches with faster flow rates appeared to be preferentially selected by males, and more vigorously defended. Weed patches in slow or still water were often unoccupied. Experimental reduction of flow rate in individual patches caused males to desert previously defended territories. Males had greater mating success on territories with higher flow rates and more ovipositions were observed in these patches. It is not known why weed patches with faster flows seem to be better quality sites for Calopteryx oviposition, but possible reasons include higher oxygen levels for developing eggs and better protection from egg parasitoids. C.2.3 Introduction The introduction should: set the background to the question, using the literature (Why is it interesting? Why is it important?). state the question, hypotheses and predictions. (What is it that we’re actually investigating?) briefly state what the study does (What is in this paper?) Start with brief general statements to put the study into its broader context: Oviposition site selection by female insects can be a critical factor in offspring survival, and hence fitness (Smith 1981). In some insects, notably many of the Odonata, males occupy or defend oviposition sites and mate with arriving females before allowing them to oviposit at that site. Males in such systems benefit in two ways from defending high quality sites: mating with all females ovipositing at the site ensures their offspring will have higher survival, and by occupying high quality sites, they will have access to more females (Jones 1976). Then move on to more specific detail about the type of system: In calopterygid damselflies females oviposit in the submerged stems of aquatic plants in streams and small rivers (Hines 1956, Norman 1968). Males defend patches of weed… Then develop the question: It has been repeatedly observed that many weed patches are always occupied and are the subject of much territorial dispute amongst males, whilst others remain unoccupied or uncontested (Gateman and Nunn 1978, Speake 1982, Mollison 1987). This suggests substantial differences in patch quality, but the basis of this difference is not known. Since the larvae may disperse after hatching, the underwater environment of a weed patch seems most likely to be important for survival and development of the eggs. One important physical factor which could influence the environment in a weed patch, and which may vary considerably in different parts of the river channel, is flow rate. We therefore hypothesised that flow rates could be an important determinant of patch quality. Say what the study actually does: In this study we investigated the physico-chemical differences between ‘good’ and ‘poor’ quality patches of weed as defined by the behaviour of the damselfly Calopteryx virgo Linnaeus. We also tested the assumption that males on more vigorously defended patches have greater mating success. Don’t separate out the question, hypothesis and predictions as special statements in bold or whatever, or put them under separate headings. Although they provide vitally important context to a study they should simply appear where necessary as part of the normal text. C.2.4 Methods The Methods section should provide enough information about how the study was carried out to enable the reader to evaluate the validity of the results. What was done? Where (usually necessary for field work) ? When (may be necessary for seasonally dependent studies) ? Why (may be necessary to justify the use of a particular approach) ? It is often said that we should write the methods so that someone could repeat what we’ve have done exactly. This is OK in principle, but often takes an excessive amount of space and shouldn’t be the overriding principle. The emphasis should be on giving the reader sufficient information to evaluate the results of a study. Focus on the important detail: i.e., it doesn’t matter that we sorted our sample into Petri dishes, or which make of microscope we used, but it does matter that we worked at 20x magnification, because that may determine how likely it is that we missed very small items. The main exception to this is if we’re reporting a novel technique that other people are likely to want to use. In that case more detail than normal will be required. Be concise. There is no need to explain the details of standard procedures. If we’re using a procedure described by someone else then summarise the essential features and just cite the reference for the method. In the Methods it is not necessary to state which statistical tests were used unless they are non-standard or require particular discussion (e.g. it is useful to state that the data were transformed before analysis). Similarly, we don’t need to state what statistics package were used for standard statistical procedures. Avoid ‘padding’ sentences that just waste space, such as: The data were analysed statistically and by plotting graphs to see what the results were. The standard style in scientific reports is to write in the third person (“Experimental plots were marked out” rather than “We marked out experimental plots”). This an area where the accepted conventions vary between different areas of biology. In some the use of the first person, where it enhances readability of the text, is permitted and even encouraged. Judicious use of ‘I’ or ‘we’ can improve the clarity and readability of text and should be used where appropriate. Just keep in mind that the use of the first person is not accepted in some disciplines. If in doubt it is safest to stick with the third person approach. Also try to use the active voice, i.e., avoid this style… It was found that males always defended single weed patches. … and and use this style instead: Males always defended single weed patches. A final point is that when a study is made up of several experiments, or sets of observations, it is a good idea to use appropriate subheadings to make it easier for the reader to follow, both within a particular section (such as the Methods) but then also using the equivalent subheadings to organise the Results and possibly the Discussion. e.g., MATERIALS AND METHODS Study site Territory occupancy by males Oviposition behaviour Experimental manipulations of flow rate C.2.5 Results The central goal of a results section is to provide a clear account of the material factual findings of the investigation, using a combination of text, summarised data, and figures. If different parts of the study are covered under different subheadings in the Methods, then use the same subheadings (where relevant) to organize the Results. The Results section should focus on explaining clearly what the results are, but should not contain discussion of the biological implications of the results. Results are presented in a variety of different ways: Text. The text part is important. This must include clear statements of the results. No result should just be presented just as a figure or a table with no corresponding statement in the text. It is important to lead the reader through the information, bringing out the important features. This does not mean that we should duplicate information in text and figures, or tables, but if a figure is used then there should be a reference to that figure in the text, which summarizes the result. Data. Numerical data are normally presented in tables (see below), but sometimes ‘stand-alone’ simple numerical results can be given in the text. In either case the data is normally be presented in summarised form only (e.g. means and standard deviations). Presenting raw data may be appropriate if there are very few data, or there is a legitimate need to discuss the values of specific data points, but this is rarely the case. Don’t include big tables of raw data. If we want include the raw data (usually only the case if the data set may be of use to others as a basis for further analyses), they should go in as an appendix. Alternatively, if our goal is to share our data with the rest of the world we might consider using an online data repository like Figshare. Statistical summaries. The results should be where most or all of the statistical results appear. There are three places to include summaries of statistics: In the text… In figure legends… In tables. If there are large numbers of tests to present which would clutter the text, e.g. the analysis involves 10 regressions of the same kinds of variables, for a number of different taxa, then it may be convenient to summarise the slopes, intercepts and significance of the relationships in a table rather than trying to put all ten in the text (although a figure would be even better). Be sure to report statistical results in full: include the test statistic, degrees of freedom (both of them, when reporting an F-test), and the p-value. Figures and Tables. Any type of graphical presentation is a figure. A table has just text and numbers. All figures and tables must be referred to in the text of the Results (or elsewhere). Tables… contain just text and numbers have the legend at the top use just horizontal lines as separators are labelled: Table 1, Table 2, etc. Table 2. The flow rates of the manipulated patches, and mean simultaneous number and turnover (number of different males per day) of territorial males on experimental patches. (Values in parentheses are standard errors for each mean). Experimental treatment Mean flow rate (m s-1) Mean number of males per patch Mean turnover of males per patch Increased flow 0.45 (0.11) 1.20 (0.22) 1.2 (0.4) Increased flow 0.18 (0.09) 0.18 (0.09) 3.1 (0.6) Increased flow 0.02 (0.01) 0.10 (0.03) 5.9 (0.9) Figures… have a legend at the bottom should be labelled: Figure 1, Figure 2, etc. can comprise a single graph or diagram, but a single figure can also comprise several graphs – in which case each should be labelled: a, b, c, etc. Preparing a manuscript for publication Word processing software makes it easy to incorporate tables and figures directly into a document. When writing a report which is not going to be typeset, then this allows the production of very professional looking documents. However, if the material is for publication, then the printers will usually require the text and figures (and sometimes tables) on separate pages. If sending a manuscript for publication, then the figures and tables should be put at the end of the document (after all the text and references), but in a normal document if a table or figure occurs on a separate page then the page should immediately follow the first point at which the figure is referred to in the text. C.2.6 Discussion The function of the discussion is to consider the meaning of the results and the light they throw on the original question; to assess the results in the context of other studies; and, if appropriate, to consider the limitations of the work and future directions for study. The exact structure and content of the Discussion will vary somewhat depending on the particular study and what the results show, but usually the following components should be included. It is usually helpful to start the discussion with a short paragraph, or so, summarizing the key results. e.g., Calopteryx females exhibit a distinct preference for weed beds in faster flowing water as oviposition sites. Males recognise such good quality sites and occupy and defend them against other males, ignoring weed beds in slower water. This results in more copulations for males which occupy the fast flowing sites. The assessment and response of males to flow rate changes can occur within a few hours Next, we usually consider the whether the results support the hypothesis or suggest it requires modification or rejection. e.g., The male damselflies’ preferential occupancy and vigorous defence of weed patches with faster flow rates, combined with a clear positive relationship between flow rate and oviposition frequency, provides strong support for the view that the underwater environment is an important determinant of oviposition site quality. It may be important to discuss the limitations of the study and the appropriate direction for further work, but these are not always required. Don’t pad out the discussion with endless text considering every possible wrinkle in the study. When appropriate, a discussion of the limitations should be brief and to the point. Although the results do implicate flow rate as a determinant of oviposition site, it is not clear whether females are responding directly to flow rate, or whether males are assessing flow rate and females are selecting the higher quality males (presumably those that occupy the best patches) assessed in some other way. This would require a separate experiment where females were allowed to select oviposition sites in the absence of males. Don’t just grumble and don’t make stock criticisms without good reason (e.g., don’t automatically say it would have been better to have a larger sample size—this may be true, but it may not—large sample sizes don’t solve everything). There may be unresolved, or unsolvable, problems. Be honest about these, but also be positive: if the author don’t seem to be sure that a study is worth reporting, how will anyone else be convinced? A report does not require a section headed ‘Experimental Error’. Similarly don’t attribute any problems that can’t be explained to experimental error; everyone knows measurements aren’t perfect so it doesn’t explain anything. Finally, bring out the wider implications (but be realistic about the significance of the work) and future directions, e.g. These results indicate selection of oviposition sites, by females, on the basis of flow rate, but the reasons for such selectivity are not known. Flow rate has been implicated in other studies of aquatic insects as being of importance for preventing low oxygen conditions developing (a stress to which developing eggs may be particularly sensitive) (Armherst 1989). High flow may also reduce the ability of egg parasitoids to search the plants (Girton and Jenner 1976). A critical part of assessing the basis of site choice, and evaluating the role of the underwater environment will be measurement of egg and larval survival in weed beds of different flow rates. It seems likely that the patterns observed in Calopteryx in a single section of the river may also be important in determining choice of habitat between different river sections or even different rivers with high or flow rates. This also raises the unwelcome possibility that quite subtle changes in flow caused by water abstraction and river regulation (a problem on a neighbouring stream to the study site) could cause marked interference with Calopteryx breeding and even loss of the species from a river system. The Discussion should not contain new results (except occasionally for small additional analyses of the data that have arisen as a direct consequence of interpretation of the main results - and that shed light upon the questions in the paper). Also avoid over-extending the implications of what was found. A slight trend in the results from one particular experiment may not be an entirely sound basis from which to challenge the fundamental tenets of evolutionary biology. (On the other hand it just could be; the skill is in spotting the few occasions when it is!). Overall, keep the focus of the discussion firmly on the results, don’t wander off into ten pages of philosophical discourse on the state of the field in general. And keep the volume and depth of the discussion in proportion to the rest of the paper, and to the significance (biological rather than statistical) of the results. C.2.7 Acknowledgements This is the place to acknowledge persons or organizations who have made significant contributions to the execution of the work. For example: funding bodies, people who have contributed ideas or assisted with some of the actual work, landowners giving access to sites, specialists who have made identifications and people who have read and commented on the manuscript. Don’t get carried away—there’s no need to thank every friend, relation and loved one for general help through life’s little crises. C.2.8 Literature cited / References This section should provide a complete listing of all, and only, references cited in the text of the report. There are three things to consider here: What to cite How to cite it in the text How to construct a reference list C.2.8.1 What to cite We should cite appropriate references wherever a point of substance (fact, or opinion) is made that is not our own or may not be regarded as common knowledge. e.g. Facts: Several species in the genus Calopteryx perform a complex `wing floating’ display as part of the courtship behaviour (Malmquist 1956) Opinion of others: This behaviour is generally considered to be a display of male quality (Fredenholm 1978, Summers 1991). Absence of citation is taken to indicate either the author’s own view or a result generated by the present study: The function of this behaviour may be to signal the flow rate, and hence quality of a patch, to a female. …or something sufficiently well known to be regarded as common knowledge: Damselflies are predatory both in the larval and adult stages. C.2.8.2 Styles of citation If writing a manuscript for publication in a scientific journal, obviously use the style of the journal in question (exactly—including punctuation). When writing any other type of report, we can choose our own style, but if in doubt the easiest approach is probably to follow the style of a major journal in the appropriate subject area. There are two main styles in widespread use: The most common (and most straightforward) cites references in the text using names and dates, and lists all references alphabetically in the reference list. In the text, e.g. Wide fluctuations in temperature reduce egg viability (Smith 1987). OR Smith (1987) found that wide variations in temperature reduced egg viability. And in the reference list: Smith, A. J. (1987) The effect of temperature on egg development and survival in the damselfly Calopteryx virgo. J. Zool. (Lond.) 47: 231-243. Note that the necessary information about the journal is the: journal title, the volume number (47) and the pages of the article (231-243). Journals often also have a part number, e.g., volume 47(2). There is no to include this in the citation; the page numbers should be sufficient. The list should be in alphabetical order by first author. If there is more than one reference by the same author then order them by date. If there are papers with the same first author but different second/third authors then these come after the single author papers by the first author, and in alphabetical order by second, third, etc. authors, e.g., Smith A J (1987)… Smith A J (1989)… Smith A J, Girton S and Mackay R H (1984)… Smith A J and Wallis K C (1983)… Smith A J and Wallis K C (1985)… If several citations by the same author in the same year are in the list, then denote them with letters e.g. In the text: Smith (1987a), Smith (1987b) In the list: Smith, A. J. (1987a) The effect of temperature on egg development and survival in the damselfly Calopteryx virgo. J. Zool. (Lond.) 47: 231-243 Smith, A. J. (1987b) The oviposition behaviour of Calopteryx virgo (Odonata: Zygoptera). Anim. Behav. 27: 197-209 The other main style is to use numerical superscripts (or equivalent) in the text, numbering the references in the order in which they are mentioned in the text, and ordering the final reference list in the same way, e.g. In the text: Wide fluctuations in temperature reduce egg viability23. Smith23 found that wide variations in temperature reduced egg viability. In the reference list: Wilcove H, Papapangiotou L A and Lu, X (1978) Mating strategies in a calopterygid damselfly. Anim. Behav. 16:21-30 Smith A J (1987) The effect of temperature on egg development and survival in the damselfly Calopteryx virgo. J. Zool. (Lond.) 47:231-243 Morris L L (1991) A model of territory switching behaviour. Am. Nat. 230:390-395 In many journals using this system, the titles of the references in the list are also omitted e.g. Smith A J (1987) 47: 231-243 This is done purely to save space, so unless we are specifically asked to do this it is best to include the complete reference. Although such numerical systems usually require the reference list to be ordered by number, it is possible (and much more convenient) to use an alphabetical listing even if numbers are used in the text (alphabetically ordered references are numbered in order and then the numbers used in the text instead of names). The advantage to a numbering system is that is saves space in the text, the disadvantage is that the numbers don’t tell a reader which paper is being referred to as they read—they have to keep looking them up in the list. Some final points to bear in mind about references and their citation: 1) every reference cited in the text must appear in the reference list, and every reference in the list must appear in the text; 2) we should never cite something we’ve not seen. If we need to cite something we have seen discussed or cited somewhere else, but haven’t seen our self (and cannot get hold of) we should make it clear that we’re citing someone else’s interpretation of the original reference, e.g., …Jones (1928—cited in Smith 1987) In the list we should then give the full citation for Smith (1987), not Jones (1928). There are standard abbreviations for journal names. These are often given in the journal itself, and are available on a list in the Library, or can be found by looking up the journal on Biological Abstracts. If we don’t know what the standard abbreviation is, and it is not obvious, then it’s best to use the full name rather than making up a new abbreviation. Use a reference manager! Managing citations and generating reference lists can be a painful process, especially when everything is done ‘manually’ (cut and paste, cut and paste, cut and paste…). These days a number of reasonably good reference managers (software packages) are available to help with the process of managing references, inserting citations, formatting, and generating reference lists. Endnote, Mendeley and Papers are probably the most popular, but there are many different options. Choose one, and learn how to use it. This will save a huge amount of effort in the long run. C.2.9 Appendices Use appendices for large amounts of raw data, long species lists, detailed mathematical or laboratory working, of a non-standard method, or (short) program listings, but only where the inclusion of such information markedly enhances the usefulness of the paper. Normally such appendices are not required. Avoid using them just to show how much work went into a study! C.3 Presenting species names A final thing worth noting, is the correct way to present species names (there is an example of it in the passage above). This causes a great deal of confusion, largely because it is not always appreciated that specific meaning attaches to the conventions used for presenting species names. The name above, Calopteryx virgo Linnaeus, has several distinct elements in its presentation (italics, upper case initial letter(s) etc.). These elements matter. The full meanings of each of the various elements one might find in a scientific name are too extensive to cover here, but the following guidelines should cover most situations. Presentation of common names is less fixed by convention than of scientific binomials but, in general, common names can be written with lower case initial letters unless the name itself contains a proper name (e.g. Norway spruce). Common names are written in the same typeface as the normal text. Common names can be used in reports, but the scientific binomial is a unique identifier that provides a standard, internationally recognized, label for a species. A report should always include the scientific name of the species we’re dealing with. Obviously it is important ensure the scientific name is spelled correctly. Fortunately systematists’ scientific latin is fairly simple phonetically, but nonetheless it is best to check the name from a reliable source when writing it for the first time (try searching for ‘calopterix’ on the web!). So now the spelling is right let’s look at the parts of the name and how to present them. In typeset documents we usually see: Calopteryx virgo The first name (the genus) should begin with an upper case letter, and the second name (the species) should begin with a lower case letter. This is not a style choice, it is a rule! Both names are usually written in italics, but sometimes they’re written in normal type and underlined. Underlining is a less-used alternative that derives from the fact that single underlining was the printers’ instruction to a typesetter to set the text in italic. This convention was widely used in the days before word-processors, as italic text was tricky to produce on a typewriter. Sometimes there will be more than just the genus and species names: Calopteryx virgo (Odonata: Calopterygidae) The names on the right (though they could equally well be on the left) are the higher taxonomic classification (order and family in this case) and are sometimes presented to enable a reader to easily place the organism. Just having a species name is not always very informative unless the reader already knows what sort of organisms are being discussed. These are written in normal text, but with an upper case initial letter. Just to confuse things though, when we write the informal derivative version of such names such as ‘odonate’ or ‘calopterygid’, e.g. …calopterygids, unlike other odonates… …then they have a lower case initial letter. If, as is occasionally the case, we’re describing the subspecies of an organism (e.g. Calopteryx splendens xanthostoma) then the sub-species name (xanthostoma) is formatted the same way as the species name. In the passage above the name of the damselfly is followed by a name: Linnaeus. This is the authority, the name of the taxonomist responsible for naming the species (in this case the famous Swedish systematist Carolus Linnaeus). Sometimes a year is also shown after the authority. Taxonomy changes as groups are revised and new classifications developed, and so species names are often not the same now as the ones they were originally given. This results in a complicated system of having more than one authority, dates, and authorities appearing in different sorts of brackets and parentheses, sometimes abbreviated, sometimes not, e.g., Calopteryx virgo Linnaeus 1758 Calopteryx splendens xanthostoma (Charpentier) Althea rosea (L.) Cavanille To present things correctly in a report there is no need to know exactly what all these different arrangements mean. The important thing to remember is that things like the arrangements of parentheses mean something specific – don’t just stick them in to make it look tidy. And when authorities are abbreviated (e.g. Linnaeus to L.) these abbreviations are fixed. We can’t just decide to abbreviate an authority to something that looks sensible. If these esoteric details are really needed then copy them carefully from a reliable source. When should we include the authority? In a scientific paper it is conventional to include the authority when the species is first mentioned (in the main text, not the abstract), and leave it out thereafter. However, for most other purposes there is no need to include the authority. Finally, abbreviation of names. Once we’ve given the full name of a species it is often convenient to refer to it in an abbreviated form later in the report: Females of C. virgo were regularly observed… There is only one correct way of abbreviating the name: shorten the genus to its initial letter (plus full stop) and keep the full specific name; never do the reverse (Calopteryx v.). If there is a subspecies name then it’s fine to abbreviate both generic and specific names, e.g., C. s. xanthostoma. C.4 Approaches to writing Everyone will actually tackle the task of writing a report in different ways, dictated by a combination of personal preference and practical constraints. However, it is worth making some general suggestions. Separate the process of writing, editing and revising—these are not the same thing. Begin with a clear written plan of what to say before starting, but expect to go through several cycles of writing, editing and revising a report before it is satisfactory (it is impossible to get everything right first time): When writing, focus on writing! Follow the plan and get the key ideas down, even if the text ends up a little ugly in places, and keep going. Avoid the urge to double back and polish up text. Editing is essentially a sentence-level process. Editing is about the minutiae. It addresses problems with spelling, grammar or word choice. This step doesn’t involve large, structural changes to a document. Revising concerns higher level criticism. Are the questions clearly elaborated? Is there a logical flow of ideas? Are the Results and Discussion connected. This step often involves moving (or removing) paragraphs, extending or narrowing text, or rewriting confusing text. Word processors greatly ease the task of editing and revising manuscripts but it is better to get a draft down and then work on it on paper, rather than agonising too long in front of the screen. It is much easier to get an overview of the structure, and to spot errors, from printed copy rather than trying to do it entirely on the computer, where we’re always peering at a fragment of the manuscript though a little window. It is generally suggested that starting with the Results and/or Methods sections is the easiest approach—these require straightforward reporting of factual information, and the pattern and presentation of results will be clearly established before it’s time to move onto the Discussion and Introduction. If writing the Discussion and Introduction is still a struggle, try writing the Abstract first—this will crystallize the key messages. Keep the writing simple, clear and concise. Science writing is about clear communication, not verbal acrobatics. Explain things to an appropriate level for the intended audience. This is harder than it sounds. By the time we reach the writing stage we have usually spent a good deal of time thinking about the problem we’re discussing. Things that seem ‘obvious’ may not be so obvious to a naive reader. Find a critical friend to read a draft when it’s ready (and be prepared to accept their honest comments!). It is of value to have the manuscript read by people who know the field and those who don’t; they will pick up different things. If possible, it is worth putting the report away somewhere for a couple of weeks and doing something else, then going back and rereading it. A report’s faults are much easier to spot after a break from working on it. C.4.1 A last piece of advice… No discussion can cover all the subtly different sorts of report that a scientist may be required to write at one time or another, but the ideas above provide a guide to one of the commonest. It is only a guide, and some circumstances will require a different approach, or structure. Writing isn’t easy. Like any difficult skill, it takes a great deal of time and effort to develop. People learn to write in different ways, and what works for one person may not work for another. Nonetheless, two pieces of advice from experienced writers crop up time and time again: Read actively. A good way to develop your scientific writing is to think critically about the papers we read, not just in terms of the science they present, but also the effectiveness of that presentation. Decide what works well and what doesn’t, then adopt the good ideas. Of course, this only works if we spend time reading the scientific literature in the first place! It’s very difficult (impossible perhaps) to become a competent writer without first becoming a regular and active reader. Write often. Many good writers will say that although they don’t always write a lot, they write often (every day!). Spread writing tasks out and set aside time to get them right. Rather than waiting until just before a deadline is looming to begin a project report or essay, start early and spend a little time every day working on it. Write regularly, but also write with effort—be prepared to critically evaluate, edit and revise written work. C.5 Further reading Barnard C, Gilbert F and McGregor P (1993) Asking questions in biology. Longman. Booth V (1985) Communicating in science: writing and speaking. Cambridge University Press. Lindsay D (1990) Scientific writing. Longman "]
]
